%                                         -*- bibtex -*-
% Time-stamp: <Thu Oct 30 2003 18:32:42 Stardate: [-29]1128.65 hwloidl>
%
% Copyright (C) 1995 -   Hans-Wolfgang Loidl
%
% Author: Hans-Wolfgang Loidl <hwloidl$%dcs.glasgow.ac.uk>
% Maintainer: Hans-Wolfgang Loidl <hwloidl$%cee.hw.ac.uk>
% Keywords: bibliography, info, hypertext
% Created: March 1995
% Status: Alpha version
%
% $Id: stat-an.bib,v 1.2 1998/04/04 21:33:10 hwloidl Exp hwloidl $
%
% This file contains references to literature about functional programing,
% (parallel) implementation of functional languages, static analysis and
% other more or less relate issues. I used this bib file for my PhD thesis,
% which is on-line:
%  http://www.dcs.gla.ac.uk/~hwloidl/publications/PhD.ps.gz  
%
% Note: This file uses emacs fakeinfo mode to structure the file. Alas,
%       the use of an at sign in structuring commands in that mode
%       confuses bibtex, even though the whole construct is under comment. 
%       As a result, bibtex genreates a lot of warnings,
%       but it still produces the right .bbl file. I plan to modify 
%       fakeinfo mode to use another operator instead of the at sign. For
%       now, you can get rid of the warnings by replacing all at signs by 
%       a magic sequence, if you don't plan to edit the file.
% Fakeinfo mode is available from:
% ftp://ftp.dcs.gla.ac.uk:/pub/glasgow-fp/authors/Hans_Loidl/Elisp/fakeinfo.el
%
% Changelog:
%  $Changelog$
% ----------------------------------------------------------------------------

% Table of contents for Springer LNCS:
%  http://link.springer.de/link/service/series/0558/tocs/t<no>.htm
% where <no> is the LNCS number. Abstracts are readable, full papers
% need subscription

% Maybe get copies of the following papers: 
%   [Rabh92], [Tick93] ([Tick88]?), [GoGa88], [Waug91], [DLH90], [Kasa80]
%   [Shul85], [Dietz88], [Nela91], [BHS78], ([Moni79]?)[BHA86]
%   [Vree87], [HoVr92]
% Sharing/Update Analysis: [Gold87], [JoLe89], [SaCl94]

%$%menu
%* Constants::			
%* Program Analyses::		
%* Time analysis::		
%* Granularity Improvement Mechanisms::	 
%* Personal and departmental references::  
%* Parallel Lisp::		
%* Theoretical work::		
%* Simulators::			
%* Functional Algorithms::	
%* Parallel Fctal Pging::	
%* Implementation of Functional Languages::  
%* Parallel Garbage Collection::  
%* Applications::		
%* Symbolic Computation::	
%* Misc::			
%* Our Own Stuff::		
%* References To Check::	
%* Index::			
%$%end menu

%$%node Top, Constants, (dir), (dir)
%$%top

% ++++++++++++++++++++++++++++++++++++++++
% Other bibs to check:
%  * Sharing analysis: shar-an.bib
%  * Bibs from DIKU:   
%  * From Berkeley: TAM.bib
%  * WS's Par Fctal Prging bib: ws-pfp.bib 
%  * Bib from Manchester: man.bib
%  * Bib of Glasgow Fp Workshops: GlaFp.bib
%  * Several personal bibs
% ++++++++++++++++++++++++++++++++++++++++

%$%node Constants, Program Analyses, Top, Top
%$%section Constants

% Taken from one of the diku-bib files; should be in an own file, really

@String{AI =     "Artificial Intelligence"}
@String{Acta =   "Acta Informatica"}
@String{CACM =   "Communications of the {ACM}"}
@String{CJ =     "Computer Journal"}
@String{EATCS =  "Bulletin of the European Association for Theoretical
                 Computer Science"}
@String{ESOP =   "European Symposium on Programming"}
@String{FPCA =   "International Conference on Functional Programming
                 Languages and Computer Architecture"}
@String{IFIP =   "IFIP World Congress Proceedings"}
@String{IPL =    "Information Processing Letters"}
@String{JACM =   "Journal of the {ACM}"}
@String{JCSS =   "Journal of Computer and System Sciences"}
@String{JFP =    "Journal of Functional Programming"}
@String{JFLP =   "Journal of Functional and Logic Programming"}
@String{JPDC =   "Journal of Parallel and Distributed Computing"}
@String{JSC =    "Journal of Symbolic Computation"}
@String{LSC =    "Lisp and Symbolic Computation"}
@String{LFP =    "{ACM} Conference on Lisp and Functional Programming"}
@String{LICS =   "{IEEE} Symposium on Logic in Computer Science"}
@String{LNCS =   "Lecture Notes in Computer Science"}
@String{MFCS =   "Mathematical Foundations of Computer Science"}
@String{MFPLS =  "Mathematical Foundations of Programming Language
                 Semantics"}
@String{NGC =    "New Generation Computing"}
@String{PEMC =   "Partial Evaluation and Mixed Computation"}
@String{PEPM =   "Partial Evaluation and Semantics-Based Program
                 Manipulation, New Haven, Connecticut. (Sigplan Notices,
                 vol. 26, no. 9, September 1991)"}
@String{PDO =    "Programs as Data Objects, Copenhagen, Denmark.
                 (Lecture Notes in Computer Science, vol. 217)"}
@String{POPL =   "ACM Symposium on Principles of Programming Languages"}
@String{PLDI =   "Programming Language Design and Implementation"}
@String{SCP =    "Science of Computer Programming"}
@String{SIGPLAN = "Sigplan Notices"}
@String{SMD =    "Soviet Mathematics Doklady"}
@String{CPE =    "Concurrency -- Practice and Experience"}
@String{SPE =    "Software -- Practice and Experience"}
@String{TCS =    "Theoretical Computer Science"}
@String{TOPLAS = "ACM Transactions on Programming Languages and
                 Systems"}
@String{TSE =    "IEEE Transactions on Software Engineering"}
% Publishers
@String{A-W =    "Addison-Wesley"}
@String{AP =     "Academic Press"}
@String{CSP =    "Computer Science Press"}
@String{CUP =    "Cambridge University Press"}
@String{JWS =    "John Wiley \& Sons"}
@String{MIT =    "MIT Press"}
@String{MKP =    "Morgan Kaufmann Publishers"}
@String{N-H =    "North-Holland"}
@String{OUP =    "Oxford University Press"}
@String{P-H =    "Prentice-Hall"}
@String{S-V =    "Springer-Verlag"}
@String{SL =     "Studentlitteratur, Lund, Sweden"}
@String{WHF =    "W.H. Freeman"}

% Institutions and people
@String{GLA =    "Department of Computing Science, University of Glasgow"}
@String{RISC =   "Research Institute for Symbolic Computation (RISC)"}
@String{BEJ =    "D. BjÅ¯rner and A.P. Ershov and N.D. Jones"}
@String{CCN =    "Computing Center, NoÅ≠voÅ≠siÅ≠birsk, USSR"}
@String{CTH =    "Chalmers University of Technology"}
@String{DIKU =   "DIKU, University of CoÅ≠penÅ≠haÅ≠gen, DenÅ≠mark"}
@String{ISI =    "Informatics Systems Institute, Novosibirsk, USSR"}
@String{PMG =    "Programming Methodology Group, Chalmers University of
                 Technology"}
@String{UPMAIL = "UPMAIL, Uppsala University, Sweden"}

% +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

% ###########################################################################
%$%node Program Analyses, Time analysis, Constants, Top
%$%section Program Analyses
% Concrete analyses and general background
% ###########################################################################

%$%menu
%* Standard references on abstract interpretation::  
%* Inference based analysis techniques::  
%* Strictness analysis::	
%* Sharing analysis::		
%* Subtyping::			
%* On-line publications::	
%* Misc analysis::		
%* Solving recursive finite-difference equations::  
%* Tmp::			
%$%end menu

%$%node  Standard references on abstract interpretation, Inference based analysis techniques, Program Analyses, Program Analyses
%$%subsection Standard references on abstract interpretation
% ---------------------------------------------------------------------------

@book{AbHa87,
  editor =       {Abramsky, S. and Hankin, C.},
  title =        {{Abstract Interpretation of Declarative Languages}},
  publisher =    {Ellis Horwood}, 
  year =         {1987},
  descr =        {plfun, pllog},
  annote =       {main early compendium of abstract interpretation},
}

@InProceedings{CoCo77,
  author = 	 {Cousot, P. and Cousot, R.},
  title = 	 {{Abstract Interpretation: A Unified Lattice Model for
                   Static Analysis of Programs by Construction or 
                   Approximation of Fixpoints}},
  pages =	 {238--252},
  booktitle =	 {{POPL'77 --- Symposium on Principles of Programming
                   Languages}},
  year =	 {1977},
  address =	 {Los Angeles, California},
  month =	 {January},
  annote =       {starting point of abstract interpretation},
}

@InProceedings{CoCo78,
  author = 	 {Cousot, P. and Cousot, R.},
  title = 	 {{Static Determination of Dynamic Properties of Recursive
		  Procedures}}, 
  editor =	 {Neuhold, E.J.},
  booktitle =    {{Formal Description of Programming Concepts}},
  year =	 {1978},
  publisher =	 {North-Holland},
  annote =       {lazy fixpoint iteration},
}

@InCollection{PeCl87,
  author = 	 {{Peyton Jones}, S.L. and Clack, C.},
  title = 	 {{Finding Fixpoints in Abstract Interpretation}},
  booktitle =    {{Abstract Interpretation of Declarative Languages}},
  publisher =	 {Ellis Horwood},
  year =	 {1987},
  editor =	 {Abramsky, S. and Hankin, C.},
  chapter =	 {11},
  pages =	 {246--265}
}

@InProceedings{JoMy86,
  author = 	 {Jones, N.D. and Mycroft, A.},
  title = 	 {{Dataflow Analysis of Applicative Programs using Minimal
		  Function Graphs}},
  pages =	 {296--306},
  booktitle =	 {{POPL'86 --- Symposium on Principles of Programming
                   Languages}},
  year =	 {1986},
  address =	 {St.\ Petersburg Beach, Florida},
  month =	 {January},
  annote =       {abstract interpretation, lazy fixpointing, mfgs},
}

@book{Burn91,
author = {Burn, G.L.},
title = {{Lazy Functional Languages: Abstract Interpretation and Compilation}},
year = {1991},
publisher = {Pitman},
series = {Research Monographs in Parallel and Distributed Computing},
owner = {risc},
descr = {plfun},
}

@inproceedings{Burn93a,
 author      ={Burn, G.L.},
 title       ={{The Abstract Interpretation of Functional Languages}},
 address     ={Isle of Thorns Conference Centre, Chelwood Gate, 
               Sussex, UK},
 publisher   =S-V,
 editor      ={Burn, G.L. and Gay, S.J. and Ryan, M.D.},
 booktitle   ={{Theory and Formal Methods 1993: Proceedings of 
               the First Imperial College, Department of 
               Computing, Workshop on Theory and Formal Methods}},
 series      ={Workshops in Computer Science},
 month       ={March},
 year        ={1993},
 scope       ={static},
 url =        {http://theory.doc.ic.ac.uk/guests/Burn/SectionWorkshop93.ps.gz},
 abstract =   "Abstract interpretation is a methodology for developing provably correct
program analyses. Uses of such analyses include optimising compilers and
program verification. In this introductory paper, we introduce key issues
in the abstract interpretation of higher-order languages, including some
open problems. Intuitions, examples and open problems are given instead of
a list of theorems."
}

@InProceedings{Burn87,
  author =       {Burn, G.L.},
  title =        {{Evaluation Transformers --- A Model for the Parallel
                 Evaluation of Functional Languages (Extended
                 Abstract)}},
  booktitle =    {{FPCA'87 --- Conference on Functional Programming Languages and Computer
                 Architecture}},
  OPTeditor =       {Kahn, G.},
  address =      {Portland, Oregon, USA, September 14--16},
  year =         {1987},
  pages =        {446--470},
  series =       {Lecture Notes in Computer Science},
  volume =       {274},
  publisher =    {Springer},
  abstract =     "If we are not careful, a parallel machine may become
                 swamped with the computation of the expressions whose
                 values are not needed in order to produce the result
                 from a program. We give a semantic criterion which
                 ensures that this does not happen. An abstract
                 interpretation can be developed which gives the
                 definedness of a function in terms of the definedness
                 of its arguments. Traditionally this has been used to
                 give a {\em strictness analysis\/} which has been
                 interpreted to say how much the argument in an
                 application can be evaluated in parallel with the
                 application while still satisfying the semantic
                 criterion. Strictness analysis however only takes into
                 account local information. By taking into account
                 contextual information of an application, we are able
                 to find that in many cases more evaluation of the
                 argument is allowed. This information is available from
                 the same abstract interpretation that is used for
                 strictness analysis. Given how much evaluation is
                 allowed of a function application, an {\em evaluation
                 transformer\/} says how much evaluation of the argument
                 in the application is allowed. This leads to a natural
                 model of the parallel evaluation of functional
                 languages.",
  keywords =     "Strictness Analysis, Abstract Interpretation,
                 Evaluation Transformers",
}

% -----------------------------------------------------------------------------
%$%node Inference based analysis techniques, Strictness analysis, Standard references on abstract interpretation, Program Analyses
%$%subsection Inference based analysis techniques
% -----------------------------------------------------------------------------

@Article{Miln78,
  author = 	 {Milner, R.},
  title = 	 {{A Theory of Type Polymorphism in Programming Languages}},
  journal = 	 {Journal of Computer and System Sciences},
  year = 	 1978,
  volume =	 17,
  pages =	 {348--375},
  annote =	 {std reference to Hindley-Milner type system}
}

@InProceedings{DaMi82,
  author = 	 {Damas, L. and Milner, R.},
  title = 	 {{Principal Type Schemes for Functional Programming}},
  booktitle =	 {{POPL'82 --- Symposium on Principles of Programming
                   Languages}},
  address =      {Albequerque, New Mexico},
  month =        jan,
  year = 	 {1982},
  pages = 	 {207--212},
  annote = 	 {std reference to Milner Damas alg}
}

@InProceedings{Mycr84,
  author = 	 {Mycroft, A.},
  title = 	 {{Polymorphic Type Schemes and Recursive Definitions}},
  booktitle = 	 {International Conference on Programming},
  year = 	 {1984},
  annote = 	 {std ref to Milner Mycroft inference}
}


@Article{Heng93,
  author = 	 {Henglein, F.},
  title = 	 {{Type Inference with Polymorphic Recursion}},
  journal = 	 TOPLAS,
  year = 	 1993,
  volume =	 15
}

@article{Wand87,
 author      ={Wand, M.},
 title       ={{A Simple Algorithm and Proof for Type Inference}},
 pages       ={115--122},
 volume      ={10},
 journal     ={Fundamenta Infomaticae},
 year        ={1987},
 scope       ={type},
 abstractURL ={http://www.ccs.neu.edu/home/wand/pubs.html},
 documentURL ={ftp://ftp.ccs.neu.edu/pub/people/wand/papers/fundamenta-87.dvi},
 documentSize={30.2 kbytes},
 annote      ={proof of Hindley's Theorem: that it is decidable 
               whether a term of the untyped lambda calculus is 
               the image under type-erasing of a term of the 
               simply typed lambda calculus},
 abstract    = "We present a simple proof of Hindley's Theorem: that it is decidable
whether a term of the untyped lambda calculus is the image under type-erasing
of a term of the simply typed lambda calculus. The proof proceeds by a direct
reduction to the unification problem for simple terms. This arrangement of the
proof allows for easy extensibility to other type inference problems."
}


@Article{PWO97,
  author = 	 {Palsberg, J. and Wand, M. and O'Keefe, P.},
  title = 	 {{Type Inference with Non-Structural Subtyping}},
  journal = 	 {Formal Aspects of Computer Science},
  year = 	 {1997},
  OPTkey = 	 {},
  OPTvolume = 	 {9},
  OPTnumber = 	 {},
  OPTmonth = 	 {},
  OPTpages = 	 {49--67},
  OPTnote = 	 {},
  OPTannote = 	 {},
  url =          {ftp://ftp.ccs.neu.edu/pub/people/wand/papers/palsberg-wand-okeefe95.ps},
  abstractURL =  {http://www.ccs.neu.edu/home/wand/pubs.html},
  abstract =     "We present an O($n^3$) time type inference algorithm for a type system with
a largest type $\top$, a smallest type $\perp$, and the usual ordering
between function types. The algorithm infers type annotations of minimal
size, and it works equally well for recursive types. For the problem of
typability, our algorithm is simpler than the one of Kozen, Palsberg, and
Schwartzbach for type inference without $\perp$. This may be surprising,
especially because the system with $\perp$ is strictly more powerful."
}

@InProceedings{KuMi89,
  author = 	 {Kuo, T-M. and Mishra, P.},
  title = 	 {{Strictness Analysis: a New Perspective Based on Type
                   Inference}},
  booktitle =    {{FPCA'89 --- Conference on Functional Programming Languages and
                   Computer Architecture}}, 
  address =      {Imperial College, London, UK, September 11--13},
  year =	 {1989},
  publisher =	 {ACM Press},
  pages =        {260--272},
  annote =       {starting point of using type inference schemes for
                  general static analyses; strictness analysis, type inference}
}

@InProceedings{HaMe94,
  author = 	 {Hankin, C. and {Le M\'{e}tayer}, D.},
  title = 	 {{Lazy Type Inference for the Strictness Analysis of Lists}},
  series =	 LNCS,
  booktitle =    {{ESOP'94 --- European Symposium on Programming}},
  year =	 {1994},
  pages =        {257--271},
  publisher =	 S-V,
  address =	 {Edinburgh, UK, April 11--13},
  annote =       {strictness analysis, type inference}
}

@InProceedings{HaMe94a,
  author = 	 {Hankin, C. and {Le M\'{e}tayer}, D.},
  title = 	 {{A Type-based Framework for Program Analysis}} ,
  editor = 	 {{Le Charlier}, B.},
  volume =	 {864},
  series =	 LNCS,
  pages =	 {380--394},
  booktitle =	 {SAS'94 --- Static Analysis Symposium},
  year =	 {1994},
  publisher =	 S-V,
  address =	 {Namur, Belgium},
  month =	 {September},
  annote =       {type inference}
}

@InProceedings{BaSm95,
  author = 	 {Barendsen, E.and Smetsers, S.},
  title = 	 {{Uniqueness Type Inference}},
  booktitle = 	 {{PLILP'95 --- Programming Languages: Implementations,
                  Logics and Programs}},
  volume =	 982,
  series =	 LNCS,
  year =	 1995,
  publisher =	 S-V,
  pages =	 {189--207},
  url =          {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/bare95-unitypeinference.ps.gz},
  abstractURL =  {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/bare95-unitypeinference.abs},
  abstract =     "In this paper we extend the Uniqueness Type System with uniqueness
polymorphism. Using a notion of `principal uniqueness variants' the
type system is shown to be effective in the sense that a uniqueness
variant of a given conventional type can be determined automatically.
The presented algorithm serves as a basis for type checking in the
language Clean. We illustrate the system by some concrete examples."
}

@InProceedings{BaSm95a,
  author = 	 {Barendsen, E. and Smetsers, S.},
  title = 	 {{A Derivation System for Uniqueness Typing}},
  booktitle = 	 {{SEGRAGRA'95 --- Joint COMPUGRAPH/SEMAGRAPH Workshop on
                  Graph Rewriting and Computation}},
  series =	 {Electronic Notes in Theoretical Computer Science},
  year =	 1995,
  publisher =	 {Elsevier},
  address =	 {Volterra, Pisa, Italy},
  annote =	 {Formal presentation of uniqueness typing in a natural
                  deduction  style.},
  url =          {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/bare95-unitypederiv.ps.gz},
  abstractURL =  {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/bare95-unitypederiv.abs},
  abstract =     "We present two systems for first-order typing of graph expressions, in
natural deduction style: conventional typing and (polymorphic)
uniqueness typing. In both type systems, typing is preserved during
evaluation, and types can be determined effectively."
}

@PhdThesis{Toft88,
  author = 	 {Tofte, M.},
  title = 	 {{Operational Semantics and Polymorphic Type Inference}},
  school = 	 {University of Edinburgh},
  year = 	 1988,
  annote =	 {also uses operational style description of type inference},
  url =          {http://www.diku.dk/users/tofte/publ/thesis-part1and2.ps},
  abstract =     "Three languages with polymorphic type disciplines are discussed, namely the
lambda-calculus with Milner's polymorphic type discipline; a language with
imperative features (polymorphic references) and a skeletal module language
with structures, signatures and functors. In each of the two first cases we
show that the type inference system is consistent with an operational
dynamic semantics.

On the module level, polymorphic types correspond to signatures. There is a
notion of principal signature. So-called signature checking is the module
level equivalent of type checking. In particular, there exists an algorithm
which either fails or produces a principal signature.",
}

@Article{Toft90,
  author = 	 {Tofte, M.},
  title = 	 {{Type Inference for Polymorphic References}},
  journal = 	 {Information and Computation},
  year = 	 1990,
  volume =	 89,
  pages =	 {1--34},
  annote =	 {Std reference for using Plotkin style operational
                  semantics for inference}
}

@TechReport{Plot81,
  author = 	 {Plotkin, G.},
  title = 	 {{A Structural Approach to Operational Semantics}},
  institution =  {Computer Science Department},
  year = 	 1981,
  number =	 {DAIMI-FN-19},
  address =	 {Aarhus University},
  annote =	 {defines structural operational semantics; reference in \cite{Toft90}}
}

@InProceedings{NiNi95,
  author = 	 {Nielson, H.R. and Nielson, F.},
  title = 	 {{Static and Dynamic Processor Allocation for Higher-order Concurrent Languages}},
  booktitle = 	 {{TAPSOFT'95}},
  volume =	 915,
  series =	 LNCS,
  year =	 1995,
  publisher =	 S-V,
  annote =	 {found via BIDS},
  abstract =     "Starting from the process algebra for Concurrent ML we develop two 
program analyses that facilitate the intelligent placement of 
processes on processors. Both analyses are obtained by augmenting an 
inference system for counting the number of channels created, the 
number of input and output operations performed, and the number of 
processes spawned by the execution of a Concurrent ML program. One 
analysis provides information useful for making a static decision 
about processor allocation; to this end it accumulates the 
communication cost for all processes with the same label. The other 
analysis provides information useful for making a dynamic decision 
about processor allocation; to this end it determines the maximum 
communication cost among processes with the same label. We prove tile
soundness of the inference system and the two analyses and 
demonstrate how to implement them; the latter amounts to transforming
the syntax-directed inference problems to instances of syntax-free 
equation solving problems."
}

%% BIDS entry
%TI- STATIC AND DYNAMIC PROCESSOR ALLOCATION FOR HIGHER-ORDER CONCURRENT 
%    LANGUAGES
%AU- NIELSON, HR;NIELSON, F
%NA- AARHUS UNIV,DEPT COMP SCI,AARHUS,DENMARK
%JN- LECTURE NOTES IN COMPUTER SCIENCE
%PY- 1995
%VO- 915
%PG- 590-604
%IS- 0302-9743
%AB- Starting from the process algebra for Concurrent ML we develop two 
%    program analyses that facilitate the intelligent placement of 
%    processes on processors. Both analyses are obtained by augmenting an 
%    inference system for counting the number of channels created, the 
%    number of input and output operations performed, and the number of 
%    processes spawned by the execution of a Concurrent ML program. One 
%    analysis provides information useful for making a static decision 
%    about processor allocation; to this end it accumulates the 
%    communication cost for all processes with the same label. The other 
%    analysis provides information useful for making a dynamic decision 
%    about processor allocation; to this end it determines the maximum 
%    communication cost among processes with the same label. We prove tile
%    soundness of the inference system and the two analyses and 
%    demonstrate how to implement them; the latter amounts to transforming
%    the syntax-directed inference problems to instances of syntax-free 
%    equation solving problems.
%CR- AMTOFT_T, UNPUB TYPE BEHAVIOUR
%    CAI_J, 1989 Vol.11 p.197, SCI COMPUT PROGRAM
%    CRIDLIG_R, 1993 Vol.724, SPRINGER LECTURE NOT
%    GIACALONE_A, 1990 Vol.443, SPRINGER LECTURE COM
%    HECHT_MS, 1977, FLOW ANAL COMPUTER P
%    HUNG_YC, 1993 Vol.23, SOFTWARE PRACTICE EX
%    JAGANNATHAN_S, 1994, P L FP
%    JOURDAN_M, 1990 Vol.432 p.240, LECT NOTES COMPUT SC
%    MCDOWELL_CE, 1989 Vol.6, J PARALLEL DISTRIBUT
%    MERCOUROFF_N, 1992 Vol.598, SPRINGER LECTURE NOT
%    NIELSON_F, 1993 Vol.715, SPRINGER LECTURE NOT
%    NIELSON_F, 1994 Vol.845, SPRINGER LECTURE NOT
%    NIELSON_HR, 1994 p.84, P POPL 94
%    REIF_JH, 1990 Vol.19, INT J PARALEL PROGRA
%    REPPY_JR, 1993 Vol.693 p.165, SPRINGER LECT NOTES
%    TARJAN_R, 1976 p.91, ALGORITHMS COMPLEXIT
%    THOMSEN_B, 1994, COMMUNICATION    MAY
%RF- 0379_94 1 LOGIC PROGRAMS; INFORMATION-SYSTEMS DESIGN; PARALLEL 
%    EXECUTION


@article{DAF97,
      author={Debbabi, M. and A\"{\i}doud, Z. and Faour, A.},
      title={{On the Inference of Structured Recursive Effects with Subtyping}},
      journal=JFLP,
      volume={1997},
      number={5},
      publisher=MIT,
      month={June},
      year={1997},
      annote = {presents inference on effects, an effect reconstruction alg
                  and proof of soundness and completeness for an ML like
                  h.o. lang},
      url = {http://www.cs.tu-berlin.de/journal/jflp/articles/1997/A97-05/JFLP-A97-05.ps.gz},
      abstractURL = {http://www.cs.tu-berlin.de/journal/jflp/articles/1997/A97-05/A97-05.html},
      abstract = "We address the type and effect inference in higher-order
     concurrent functional programming languages · la Concurrent ML. We
     present three extensions of the type and effect discipline. First,
     the discipline is extended to deal with infinite but recursive
     effects. Second, the inferred effects are structured, i.e., we keep
     track of the structure of effects (sequencing, choice, parallel
     composition, and recursion) instead of using an AC1I (associative,
     commutative, unitary, and idempotent) effect cumulation
     operator. Third, for the sake of flexibility, a subtyping relation
     is considered on the type and effect algebras. This is much more
     powerful than the classical subeffecting technique. This is meant to
     avoid type mismatches that may arise in some typing contexts between
     subexpressions that have similar type structure but different effect
     annotations. We present the language syntax together with its static
     semantics. The latter consists of the typing rules and an inference
     algorithm that is proved to be sound with respect to the typing
     rules."
}


% -----------------------------------------------------------------------------
%$%node  Strictness analysis, Sharing analysis, Inference based analysis techniques, Program Analyses
%$%subsection Strictness analysis
% -----------------------------------------------------------------------------

@Article{BHA86,
  author = 	 {Burn, G.L. and Hankin, C. and Abramsky, S.},
  title = 	 {{Strictness Analysis for Higher Order Functions}},
  journal =	 {Science of Computer Programming},
  year =	 {1986},
  volume =	 {7},
  pages =	 {249--278},
  month =	 {November},
  annote =	 {Simon says: Excellent paper, good intro too.}
}

@InCollection{Hugh87,
  author = 	 {Hughes, R.J.M.},
  title = 	 {{Analysing Strictness by Abstract Interpretation of
                   Continuations}}, 
  booktitle = 	 {{Abstract Interpretation of Declarative Languages}}, 
  publisher =	 {Ellis Horwood},
  year =	 {1987},
  pages =	 {63--102},
  editor =	 {Abramsky, S. and Hankin, C.},
}


@InProceedings{Hugh85,
  author = 	 {Hughes, R.J.M.},
  title = 	 {{Strictness Detection in Non-Flat Domains}},
  editor =	 {Ganzinger, H. and Jones, N.D.},
  volume =	 {217},
  series =	 LNCS,
  pages =	 {112--135},
  booktitle =    {{Workshop on Programs as Data Objects}},
  year =	 {1985},
  publisher =	 S-V,
  address =	 {Copenhagen, Denmark, October 17--19}
}

@PhdThesis{Ferg95,
  author = 	 {Ferguson, A.},
  title = 	 {{Higher Order Strictness Analysis by Abstract
                   Interpretation over Finite Domains}},
  school = 	 {Department of Computing Science},
  year = 	 {1995},
  address =	 {University of Glasgow},
  month =	 {May}
}

@PhdThesis{Davi94,
  author = 	 {Davis, K.},
  title = 	 {{Projection-based Program Analysis}},
  school = 	 {Department of Computing Science},
  year = 	 {1994},
  address =	 {University of Glasgow},
  month =	 {July}
}

@TechReport{Hugh87a,
  author = 	 {Hughes, R.J.M.},
  title = 	 {{Backwards Analysis of Functional Programs}},
  institution =  {Department of Computing Science},
  year = 	 {1987},
  address =	 {University of Glasgow},
  number =	 {CSC/87/R3},
  month =	 {March}
}

@TechReport{HuLa??,
  author = 	 {Hughes, R.J.M. and Launchbury, J.},
  title = 	 {{Towards Relating Forwards and Backwards Analyses}},
  institution =  {Department of Computing Science},
  year = 	 {19??},
  address =	 {University of Glasgow},
  annote = 	 {Part of Semantique project; related to \cite{KHL92}}
}

@InProceedings{WaHu87,
  author = 	 {Wadler, P. and Hughes, R.J.M.},
  title = 	 {{Projections for Strictness Analysis}},
  editor =	 {Kahn, G.},
  volume =	 {274},
  series =	 LNCS,
  pages =	 {385--407},
  booktitle =    {{FPCA'87 --- Conference on Functional Programming Languages
		  and Computer Architecture}}, 
  year =	 {1987},
  publisher =	 S-V,
  address =	 {Portland, Oregon, September 14--16},
  url =  {http://www.dcs.glasgow.ac.uk/\~{}wadler/papers/strictproject/context.ps},
}


@InCollection{Wadl87,
  author = 	 {Wadler, P.},
  title = 	 {{Strictness Analysis on Non-Flat Domains}},
  editor =       {Abramsky, S. and Hankin, C.},
  booktitle =    {{Abstract Interpretation of Declarative Languages}},
  publisher =    {Ellis Horwood},
  year =         {1987}
}

@InProceedings{DaWa89,
  author = 	 {Davis, K. and Wadler, P.},
  title = 	 {{Backwards Strictness Analysis: Proved and Improved}},
  editor =	 {Davis, K. and Hughes, R.J.M.},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  year =	 {1989},
  publisher =	 S-V,
  address =	 {Fraserburgh, Scotland, August 21--23}
}

@PhdThesis{Bara93,
  author = 	 {Baraki, G.},
  title = 	 {{Abstract Interpretation of Polymorphic Higher-Order
		  Functions}}, 
  school = 	 {Department of Computing Science},
  year = 	 {1993},
  address =	 {University of Glasgow},
  month =	 {August}
}

@InProceedings{FeHu92,
  author = 	 {Ferguson, A. and Hughes, R.J.M.},
  title = 	 {{Abstract Interpretation of Higher-Order Functions Using
		  Concrete Data Structures}},
  series =	 {Workshops in Computing},
  pages =	 {57--61},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  year =	 {1992},
  publisher =	 S-V,
  address =	 {Ayr, Scotland, July 6--8},
  editor = 	 {Launchbury, J. and Sansom, P.}
}

@InProceedings{PePa93,
  author = 	 {{Peyton Jones}, S.L. and Partain, W.D.},
  title = 	 {{Measuring the Effectiveness of a Simple Strictness
		  Analyser}}, 
  series =	 {Workshops in Computing},
  pages =	 {201--221},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  year =	 {1993},
  publisher =	 S-V,
  address =	 {Ayr, Scotland, July 5--7},
  editor = 	 {{O'Donnell}, John T. and Hammond, K.}
}

@InProceedings{JHR94,
  author = 	 {Jensen, K.D. and HjÅÊresen, P. and Rosendahl, M.},
  title = 	 {{Efficient Strictness Analysis of Haskell}},
  number =	 {864},
  series =	 LNCS,
  pages =	 {346--362},
  booktitle =	 {SAS'94 --- Static Analysis Symposium},
  year =	 {1994},
  publisher =	 S-V,
  address =	 {Namur, Belgium},
  month =	 {September}
}

@PhdThesis{Sewa95,
  author = 	 {Seward, J.R.},
  title = 	 {{Abstract Interpretation of Functional Languages: A
		  Quantitative Assessment}},
  school = 	 {Department of Computer Science},
  year = 	 {1995},
  address =	 {University of Manchester},
}

@TechReport{Cons91,
  author = 	 {Consel, C.},
  title = 	 {{Fast Strictness Analysis via Symbolic Fixpoint Iteration}},
  institution =  {Department of Computer Science},
  year = 	 {1991},
  type =         {Research Report}, 
  number = 	 {{YALEU/DCS/RR-867}},
  address =	 {Yale University},
  month =	 {September},
  notes =        {A later version appeared in: SAS'94, pp.\ 423--431},
  url =  {http://www.irisa.fr/EXTERNE/projet/lande/consel/papers/sas-strictness.ps.gz}
}

@Article{Brya86,
  author = 	 {Bryant, R.E.},
  title = 	 {{Graph-Based Algorithms for Boolean Function Manipulation}},
  journal =	 {IEEE Transactions on Computers},
  year =	 {1986},
  volume =	 {35},
  pages =	 {677--691}
}

@PhdThesis{Howe96,
  author = 	 {Howe, D.B.},
  title = 	 {{Using Evaluation Transformers to Optimise a Real Lazy
                   Functional Language}},
  school = 	 {{Imperial College of Science, Technology and Medicine}},
  year = 	 {1996},
  address =	 {University of London},
  month =	 {March},
  annote =	 {Submitted: Dec 95; Examined: Mar 96}
}

@InProceedings{Nock93,
  author = 	 {NÅˆcker, E.},
  title = 	 {{Strictness Analysis Using Abstract Reduction}},
  booktitle =    {{FPCA'93 --- Conference on Functional Programming
		  Languages and Computer Architecture}}, 
  year =	 {1993},
  publisher =	 {ACM Press},
  pages =        {255--265},
  address =      {Copenhagen, Denmark, June 9--11},
  annote =       {Explains the efficient and powerful strictness analysis
                  method incorporated in Clean.},
}

@phdthesis{Bent92,
 author      = {Benton, N/},
 title       = {{Strictness Analysis of Lazy Functional Programs}},
 school      = {Computer Laboratory},
 address     = {University of Cambridge},
 month       = {December},
 year        = {1992},
 scope       = {static},
 url         = {http://www.cl.cam.ac.uk/users/pnb/thesis.dvi.Z},
}

% Check the following references

@InProceedings{Burn90c,
  author =       {Burn, G.L.},
  title =        {{Using Projection Analysis in Compiling Lazy Functional
                   Programs}},
  pages =	 {227--241},
  booktitle =    {{LFP'90 --- Conference on Lisp and Functional Programming}},
  year = 	 {1990},
  publisher =	 {ACM Press},
  address =	 {Nice, France, June 27--29}
}

@Article{Burn91a,
  title =        {{Implementing the Evaluation Transformer Model of
                  Reduction on Parallel Machines}},
  author =       {Burn, G.L.},
  pages =        {329--366},
  journal =      JFP,
  month =        jul,
  year =         1991,
  volume =       1,
  number =       3,
  descr =        {plfun},
  url =          {http://theory.doc.ic.ac.uk/tfm/papers/BurnGL/JnlFP\_Implementation.ps.gz},
  abstract =     "The  evaluation transformer model  of reduction generalises lazy evaluation
in two ways: it can start the evaluation of  expressions before their first
use,  and it  can    evaluate expressions further  than  weak   head normal
form.   Moreover, the amount  of  evaluation required of   an argument to a
function may depend  on the amount of  evaluation required of the  function
application.   It is  a suitable   candidate   model for  implementing lazy
functional languages  on parallel machines.  In this  paper we  explore the
implementation of lazy   functional languages  on parallel  machines,  both
shared and  distributed   memory   architectures,   using   the  evaluation
transformer  model  of reduction. We  will  see that the  same code  can be
produced  for  both styles   of architecture,  and   the definition of  the
instruction set is virtually  the   same  for  each style. The    essential
difference  is  that a distributed  memory architecture  has one extra node
type for  non-local pointers, and  instructions which involve the  value of
such  nodes  need  their definitions extended  to  cover  this new type  of
node.  To make our  presentation accessible, we base   our description on a
variant of the well-known G-machine, an abstract machine for executing lazy
functional programs."
}

@InProceedings{FiBu93,
  author = 	 {Finne, S. and Burn, G.},
  title = 	 {{Assessing the Evaluation Transformer Model of Reduction
                   on the Spineless Tagless G-machine}},
  booktitle =    {{FPCA'93 --- Conference on Functional Programming
		  Languages and Computer Architecture}}, 
  year =	 {1993},
  publisher =	 {ACM Press},
  address =      {Copenhagen, Denmark, June 9--11},
  annote = 	 {}
}


% ---------------------------------------------------------------------------

@PhdThesis{Nela91,
  author = 	 {Nelan, G.C.},
  title = 	 {{Firstification}},
  school = 	 {Arizona State University},
  year = 	 {1991}
}

@InProceedings{Hunt89,
  author = 	 {Hunt, S.},
  title = 	 {{Frontiers and Open Sets in Abstract Interpretation}},
  pages =	 {1--11},
  booktitle =    {{FPCA'89 --- Conference on Functional Programming
                   Languages and Computer Architecture}}, 
  address =      {Imperial College, London, September 11--13},
  year =	 {1989},
  publisher =	 {ACM Press}
}

@PhdThesis{Hunt91,
  author = 	 {Hunt, S.},
  title = 	 {{Abstract Interpretation of Functional Languages: From
		  Theory to Practice}},
  school = 	 {Imperial College},
  year = 	 {1991},
  address =	 {University of London}
}

@PhdThesis{Mart92,
  author = 	 {Martin, C.C.},
  title = 	 {{Algorithms for Finding Fixpoints in Abstract
		  Interpretation}}, 
  school = 	 {Imperial College},
  year = 	 {1992},
  address =	 {University of London},
  month =	 {June},
}

@InProceedings{MaHa87,
  author = 	 {Martin, C.C. and Hankin, C.},
  title = 	 {{Finding Fixed Points in Finite Lattices}},
  editor =	 {Kahn, G.},
  address =      {Portland, Oregon, USA, September 14--16},
  number =	 {274},
  series =	 LNCS,
  pages =	 {426--445},
  booktitle =    {{FPCA'87 --- Conference on Functional Programming
		  Languages and Computer Architecture}}, 
  year =	 {1987},
  publisher =	 S-V,
  month =	 {September},
}

@TechReport{KHL92,
  author = 	 {Kubiak, R. and Hughes, R.J.M. and Launchbury, J.},
  title = 	 {{A Projection-Based Strictness Analyser for a Haskell
                   Compiler}}, 
  institution =  {Department of Computing Science},
  year = 	 {1992},
  address =	 {University of Glasgow},
  month =	 {May},
  note =	 {Part of the documentation for the Glasgow Haskell Compiler},
}

%$%xref{kubiak:projection,Paper on implementation,,hughes.bib,}

@inproceedings{KHL91,
	AUTHOR = {Kubiak, R. and Hughes, R.J.M. and  Launchbury, J.},
	TITLE = {{Implementing Projection Based Strictness Analysis}},
	BOOKTITLE = {{Functional Programming}},
	PUBLISHER = S-V,
	SERIES = {Workshops in Computing},
	ADDRESS = {Glasgow},
	YEAR = {1991},
        url = {http://www.cse.ogi.edu/\~{}jl/Papers/implementing.ps},
        abstract = "Projection-based backwards strictness analysis has been understood for some
years. Surprisingly, even though the method is fairly simple and quite
general, no reports of its implementation have appeared. This paper
describes ideas underlying our prototype implementation of the analysis for
a simple programming language. The implementation serves as a case study
before applying the method in the Glasgow Haskell compiler."
}

% ---------------------------------------------------------------------------
%$%node Sharing analysis, Subtyping, Strictness analysis, Program Analyses
%$%subsection Sharing analysis
% See also shar-an.bib (xref{shar-an.bib})
% ---------------------------------------------------------------------------

@Unpublished{Hugh92,
  author = 	 {Hughes, J.},
  title = 	 {{Analysing Sharing in Parallel Functional Programs}},
  note = 	 {Draft},
  year =	 {1992},
  month =	 {March}
}

@inproceedings{Gold87,
  author =       {Goldberg,B.},
  title =        {{Detecting Sharing of Partial Applications in Functional
                   Programs}}, 
  pages =        {408--425},
  booktitle =    {{FPCA'87 --- Conference on Functional Programming Languages
		  and Computer Architecture}}, 
  year =	 {1987},
  publisher =	 S-V,
  address =	 {Portland, Oregon, September 14--16}
}

@inproceedings{JoLe89,
  author =       {Jones, S.B. and {Le M\'{e}tayer}, D.},
  title =        {{Compile-Time Garbage Collection by Sharing Analysis}},
  pages =        {54--74},
  booktitle =    {{FPCA'89 --- Conference on Functional Programming Languages and
                   Computer Architecture}}, 
  address =      {Imperial College, London, UK, September 11--13},
  year =	 {1989},
  publisher =	 {ACM Press},
}

@inproceedings{SaCl94,
  author =       {Sastry, A.V.S and Clinger, W.},
  title =        {{Parallel destructive updating in strict functional 
                   languages}},
  pages =        {263--272},
  booktitle =	 {LFP'94 --- Conference on Lisp and Functional Programming},
  year =	 {1994},
  address =      {Orlando, Florida, June 27--29},
  publisher =	 {ACM Press},
  month =	 {June}
}

% -----------------------------------------------------------------------------

@InProceedings{Wadl88,
  author = 	 {Wadler, P.},
  title = 	 {{Strictness analysis aids time analysis}},
  booktitle =    {POPL'88 --- Symposium on Principles of Programming Languages},
  year =	 {1988},
  month =	 {January},
  url =          {ftp://ftp.dcs.gla.ac.uk/pub/glasgow-fp/authors/Philip\_Wadler/stricttime.dvi},
}

%  booktitle =    ({Symposium on Principles of Programming Languages}}, 


@InProceedings{Hugh82,
  author = 	 {Hughes, R.J.M.},
  title = 	 {{Super-Combinators: a New Implementation Method for
                   Applicative Languages}},
  pages =	 {1--10},
  booktitle =    {{LFP'82 --- Conference on Lisp and Functional Programming}},
  year =	 {1982},
  publisher =	 {ACM Press},
  month =	 {August}
}

%InProceedings{LeMe85,
%  author = 	 {{Le M\'{e}tayer}, D.},
%  title = 	 {{Mechanical Analysis of Program Complexity}},
%  volume =	 {20(7)},
%  OPTnumber =	 {7},
%  series =	 {ACM SIGPLAN Notices},
%  pages =	 {69--73},
%  booktitle =	 {SIGPLAN'85 Conference},
%  year =	 {1985},
%  month =	 {July}
%}

@Article{KrLe88,
  author = 	 {Kruatrachue, B. and Lewis, T.},
  title = 	 {{Grain Size Determination for Parallel Processing}},
  journal =	 i3es,
  year =	 {1988},
  volume =	 {5},
  number =	 {1},
  pages =	 {23--31},
  month =	 {January}
}

%$%node Subtyping, On-line publications, Sharing analysis, Program Analyses
%$%subsection Subtyping

@Article{Mitc91,
  title={Type Inference with Simple Subtypes},
  author={Mitchell, J.C.},
  pages={245--285},
  journal=jfp,
  month=jul,
  year=1991,
  volume=1,
  number=3
}

@InProceedings{LiMi92,
  author = 	 {Lincoln, P. and Mitchell, J.C.},
  title = 	 {{Algorithmic Aspects of Type Inference with Subtypes}},
  booktitle = 	 {POPL'92 --- Symposium on Principles of Programming Languages},
  year =	 1992,
  annote =       {good general part; emphasis algorithmic apsects of
                  subtype inference},
  month =	 {January},
  publisher =	 {ACM Press},
  address =      {Albuquerque, New Mexico},
}

@Book{Mitc96,
  author = 	 {Mitchell, J.C.},
  title = 	 {{Foundations for Programming Languages}},
  publisher = 	 MIT,
  year = 	 1996,
  series =	 {Foundations of Computing series},
  annote =	 {ISBN 0-262-13321-0}
}

@Article{FuMi90,
  author = 	 {Fuh, Y. and Misra. P.},
  title = 	 {{Type Inference with Subtypes}},
  journal = 	 TCS,
  year = 	 1990,
  volume =	 73,
  annote =	 {gives a type inference alg for structural subtyping; cited in \cite{LiMi92}}
}

@InProceedings{MaWa97,
  author = 	 {Marlow, S. and Wadler, P.},
  title = 	 {{A Practical Subtyping System for Erlang}},
  booktitle = 	 {{ICFP'97 --- International Conference on Functional Programming}},
  year =	 1997,
  publisher =	 {ACM Press},
  address =	 {June 9--11, Amsterdam, The Netherlands},
  pages =	 {136--149},
  annote =	 {std reference for the Erlang subtyping system},
  url =  {ftp://ftp.dcs.gla.ac.uk/pub/glasgow-fp/authors/Simon\_Marlow/erltc.ps},
  abstract =     "We present a type system for the programming language Erlang. The type
system supports    subtyping and declaration-free   recursive types,  using
subtyping constraints. Our system is  similar to one  explored by Aiken and
Wimmers, though it sacrifices expressive  power in favour of simplicity. We
cover our techniques for type inference,  type simplification, and checking
when  an  inferred  type conforms to   a user-supplied  type signature, and
report on early experience with our prototype."
}

@InProceedings{AWL94,
  author = 	 {Aiken, A. and Wimmers, E.L. and Lakshman, T.K.},
  title = 	 {{Soft Typing with Conditional Types}},
  booktitle =    {POPL'94 --- Symposium on Principles of Programming Languages},
  year =	 1994,
  address = 	 {Orlando, Florida},
  annote =	 {gives a subtype inference algorithm for Lisp; uses
                  conditional types to encode runtime info; goal: omit
                  runtime checks},
  url =          {http://http.cs.berkeley.edu/\~{}aiken/ftp/popl94.ps},
  abstractURL =  {http://http.cs.berkeley.edu/\~{}aiken/pubs.html},
  abstract = "We present a simple and powerful type inference method for dynamically typed
languages where no type information is supplied by the user. Type inference is
reduced to the problem of solvability of a system of type inclusion
constraints over a type language that includes function types, constructor
types, union, intersection, and recursive types, and conditional
types. Conditional types enable us to analyze control flow using type
inference, thus facilitating computation of accurate types. We demonstrate the
power and practicality of the method with examples and performance results
from an implementation."
}

@InProceedings{Pott96,
  author=        {Pottier, F.},
  title=         {{Simplifying Subtyping Constraints}},
  pages=         {122--133},
  booktitle = 	 {{ICFP'96 --- International Conference on Functional Programming}},
  year =	 1996,
  publisher =	 {ACM Press},
  address =	 {Philadelphia, Pennsylvania, May 24--26},
  keywords =     {subtyping},
  annote =	 {algorithm for checking entailment of constraint sets},
}

@InProceedings{MaSe82,
  author = 	 {MacQueen, D. and Sethi, R},
  title = 	 {{A Semantic Model of Types for Applicative Languages}},
  booktitle =    {{LFP'82 --- Conference on Lisp and Functional Programming}},
  year =	 {1982},
  publisher =	 {ACM Press},
  month =	 {August},
  pages = 	 {243--252}
}

@InProceedings{MPS84,
  author = 	 {MacQueen, D. and Plotkin, G. and Sethi, R.},
  title = 	 {{An Ideal Model of Recursive Polymorphic Types}},
  booktitle = 	 {POPL'84 --- Symposium on Principles of Programming Languages},
  year =	 1984,
  month =	 {January},
  pages =	 {165--174},
  annote =	 {std semantic model for types; also in Information and Control}
}

@Article{MPS86,
  author = 	 {MacQueen, D. and Plotkin, G. and Sethi, R.},
  title = 	 {{An Ideal Model of Recursive Polymorphic Types}},
  journal = 	 {Information and Control},
  year = 	 1986,
  volume =	 71,
  pages =	 {95--130},
  annote =	 {expanded version of the POPL paper},
}

@InProceedings{SeYo94,
  author = 	 {Sekiguchi, T. and Yonezawa, A.},
  title = 	 {{A Complete Type Inference System for Subtyped Recursive Types}},
  booktitle = 	 {{TACS'94 --- Theoretical Aspects of Computer Software}},
  volume =	 789,
  series =	 LNCS,
  year =	 1994,
  publisher =	 S-V,
  month =	 {April},
  pages =	 {667--686},
  annote =	 {gives a subtype inference algorithm for an ML like
       language with records, recursive type, and parametric polymorphism},
  url =  {ftp://ftp.yl.is.s.u-tokyo.ac.jp/pub/papers/tacs94-oo-type-a4.ps.gz},
  abstract =     "Since record polymorphism is one of essential factors for object-oriented
languages, various approaches to incorporate record polymorphism into type
systems have been proposed to lay the foundation for object-oriented
languages. Recursive types, which are essentially types of lists or trees,
are major programming tools. In object-oriented languages, a pseudo
variable self has a recursive type, which requires that type systems be
able to treat recursive types. The purp ose of this paper is to provide a
type system and its type inference algorithm which can handle subtyping,
recursive types and parametric polymorphism without any kind of type
declaration or unnatural restrictions. We prove soundness and completeness
of the type inference algorithm. Our system integrates subtyping and
recursive types into Damas and Milner's type system and preserves important
properties such as existence of principal typing. The basic idea is that we
consider a type as a regular tree."
}

@article{6042,
 author = {Luca Cardelli and Peter Wegner},
 title = {On understanding types, data abstraction, and polymorphism},
 journal = {ACM Computing Surveys (CSUR)},
 volume = {17},
 number = {4},
 year = {1985},
 issn = {0360-0300},
 pages = {471--523},
 doi = {http://doi.acm.org/10.1145/6041.6042},
 publisher = {ACM Press},
}

% ---------------------------------------------------------------------------
%$%node On-line publications, Misc analysis, Subtyping, Program Analyses
%$%subsection On-line publications
% Mainly links to lists of papers or other indices
% ---------------------------------------------------------------------------

%$%menu
%* Web pages::			
%* On-line Bibliographies::	
%$%end menu

%$%node Web pages, On-line Bibliographies, On-line publications, On-line publications
%$%subsubsection Web pages

@Misc{WWW-GHC,
  key =		 {GHC},
  title =	 {{The Glasgow Haskell Compiler}},
  address =      {Department of Computing Science, University of Glasgow},
  addressURL =   {http://www.dcs.gla.ac.uk/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  description =  "The Glasgow Haskell Compiler compiles code written in the
                  functional programming language Haskell",
  url =          {http://www.dcs.gla.ac.uk/fp/software/ghc/},
}

@Misc{WWW-GPH,
  key =		 {WWW-GpH},
  title =	 {{GpH Web Pages}},
  OPThowpublished = {WWW page},
  month =	 {May},
  year =	 2001,
  note =	 {$<$URL:{\tt http://www.cee.hw.ac.uk/\~{}dsg/gph}$>$},
  annote =	 {date fields mean ``valid at''}
}

@Misc{WWW-GranSim,
  key =		 {GranSim},
  title =	 {{GranSim Home Page}},
  address =      {Department of Computing Science, University of Glasgow},
  addressURL =   {http://www.dcs.gla.ac.uk/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.dcs.glasgow.ac.uk/fp/software/gransim/},
}

@Misc{WWW-nofib,
  key =		 {NoFib},
  title =	 {{The NoFib Benchmark Suite}},
  address =      {Department of Computing Science, University of Glasgow},
  addressURL =   {http://www.dcs.gla.ac.uk/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {March},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.dcs.gla.ac.uk/fp/software/ghc/nofib.html},
}

@Misc{WWW-pHluid,
  key =		 {pHluid},
  author =	 {Nikhil, R.S.},
  title =	 {{The pHluid System Home Page}},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.research.digital.com/CRL/personal/nikhil/pHluid/home.html},
}

@Misc{WWW-Cid,
  key =		 {Cid},
  author =	 {Rishiyur S. Nikhil},
  title =	 {{The Cid System Home Page}},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.research.digital.com/CRL/personal/nikhil/pHluid/home.html}
}

@Misc{WWW-pH,
  key =		 {pH},
  title =	 {{pH Home Page}},
  address =      {MIT Laboratory for Computer Science},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.csg.lcs.mit.edu:8001/pH/},
}

@Misc{WWW-StarT,
  key =		 {StarT},
  title =	 {{StarT-the Next Generation}},
  address =      {Laboratory for Computer Science, M.I.T.},
  addressURL =   {http://www.lcs.mit.edu/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.csg.lcs.mit.edu:8001/StarT-NG/},
}

@Misc{WWW-Concert,
  key =		 {Concert},
  title =	 {{Concert System Papers}},
  address =      {Department of Computer Science, University of Illinois at
                  Urbana-Champaign.},
  addressURL =   {http://www.cs.uiuc.edu/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www-csag.cs.uiuc.edu/papers/}
}

@Misc{WWW-Cilk,
  key =		 {Cilk},
  title =	 {{The Cilk Project --- Papers}},
  address =      {Laboratory for Computer Science, M.I.T.},
  addressURL =   {http://www.lcs.mit.edu/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://supertech.lcs.mit.edu/cilk},
  OLDurl =          {http://theory.lcs.mit.edu/\~{}cilk/papers.html},
}

@Misc{WWW-Charm,
  key =		 {Charm},
  title =	 {{The Charm Project --- Papers}},
  address =      {Department of Computer Science, University of Illinois},
  addressURL =   {http://charm.cs.uiuc.edu/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://charm.cs.uiuc.edu/version2/papers.html},
}

@Misc{WWW-Linda,
  key =		 {Linda},
  title =	 {{Yale Linda Group}},
  address =      {Department of Computer Science, Yale University},
  addressURL =   {http://www.cs.yale.edu/},
  howpublished = {WWW page},
  year =	 2000,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.cs.yale.edu/Linda/linda.html},
}

@Misc{WWW-Impala,
  key =		 {Impala},
  author =	 {Shaw, A.},
  title =	 {{Impala Application Suite}},
  address =      {Laboratory for Computer Science, M.I.T.},
  addressURL =   {http://www.lcs.mit.edu/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.csg.lcs.mit.edu:8001/impala/},
}

@Misc{WWW-SISAL,
  key =		 {SISAL},
  title =	 {{Sisal Language Project}},
  address =      {Lawrence Livermore National Laboratory},
  addressURL =   {http://www.llnl.gov/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.llnl.gov/sisal/SisalHomePage.html}
}

@Misc{WWW-Pebbles,
  key =		 {Pebbles},
  title =	 {{Pebbles}},
  address =      {Colorado State University Computer Science Department},
  addressURL =   {http://www.cs.colostate.edu/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.cs.colostate.edu/\~{}dataflow/},
}

@Misc{WWW-Clean,
  key =		 {Clean},
  title =	 {{Clean's Home Page}},
  address =      {Computing Science Department, University of Nijmegen},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.cs.kun.nl/\~{}clean/Clean.Home.html},
}

@Misc{WWW-Orca,
  key =		 {Orca},
  title =	 {{The Orca Parallel Programming Language }},
  address =      {Division of Mathematics and Computer Science, Vrije Universiteit Amsterdam},
  howpublished = {WWW page},
  year =	 2000,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.cs.vu.nl/orca/},
  abstract =     "Orca is a language for parallel programming on distributed systems, based on the shared data-object model. This model
is a simple and portable form of object-based distributed shared memory. "
}

@Misc{WWW-Jade,
  key =		 {Jade},
  title =	 {{Jade}},
  address =      {Laboratory for Computer Science, M.I.T.},
  howpublished = {WWW page},
  year =	 2000,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.cag.lcs.mit.edu/~rinard/jade/},
  abstract =     "For my Ph.D. thesis I designed and implemented Jade, an implicitly parallel
language for coarse-grain parallel computing. Jade has been implemented on
shared-memory multiprocessors, message-passing machines, and heterogeneous
networks of workstations. Jade programs port without modification between all
of these platforms. An implementation of Jade is available.",
}


@Misc{WWW-SEL-HPC,
  key =		 {SEL-HPC},
  author =	 {Hill, J. and Tsaptsinos, D. and Smallbone, A. and McEwan, A.},
  title =	 {{SEL-HPC Article Archive}},
  address =      {London Parallel Applications Centre},
  addressURL =   {http://www.lpac.ac.uk/lpac/},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {January},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.lpac.ac.uk/SEL-HPC/Articles/index.html},
}

@Misc{WWW-LANL,
  key =		 {LANL},
  title =	 {{Sisal Performance Data}},
  address =      {Lawrence Livermore National Laboratories},
  howpublished = {WWW page},
  year =	 1998,
  month =	 {June},
  annote =	 {date fields mean ``valid at''},
  url =          {http://www.llnl.gov/sisal/PerformanceData.html},
}


%$%node On-line Bibliographies,  , Web pages, On-line publications
%$%subsubsection On-line Bibliographies


@Manual{WWW-TFM-BIB,
  title = 	 {{A Database of Bibliographies in the Theoretical Computer Science Research Archive}},
  key =		 {TFM-BIB},
  address =	 {Department of Computing, Imperial College, London},
  year =	 1998,
  month =	 {February},
  annote =	 {date fields mean ``valid at''},
  url =          {http://theory.doc.ic.ac.uk/bibliography/},
}

% ---------------------------------------------------------------------------
%$%node Misc analysis, Solving recursive finite-difference equations, On-line publications, Program Analyses
%$%subsection Misc analysis
% ---------------------------------------------------------------------------

@PhdThesis{Blos89,
  author = 	 {Bloss, A.G.},
  title = 	 {{Path Analysis and the Optimization of Non-strict
                   Functional Languages}},
  school = 	 {Department of Computer Science},
  year = 	 {1989},
  address =	 {Yale University},
  month =	 {May},
  type =         {Research Report}, 
  note =	 {{YALEU/DCS/RR-704}},
}

@InProceedings{PaGo92,
  author = 	 {Park, Y.G. and Goldberg, B.},
  title = 	 {{Order-of-Demand Analysis for Lazy Languages}},
  editor = 	 {Billaud, M. and CastÅÈran, P. and Corsini, M.M. and
		  Musumbu, K. and Rauzy, A.},
  pages =	 {91--101},
  booktitle =    {{WSA'92 --- Analyse Statique}},
  year =	 {1992},
  address =	 {Bordeaux, France, September 23--25},
}

@InProceedings{SCA93,
  author = 	 {Sastry, A.V.S. and Clinger, W. and Ariola, Z.},
  title = 	 {{Order-of-Evaluation Analysis for Destructive Updates in
                   Strict Functional Languages with Flat Aggregates}},
  booktitle =    {{FPCA'93 ---  Conference on Functional Programming
		  Languages and Computer Architecture}}, 
  year =	 {1993},
  publisher =	 {ACM Press},
  address =      {Copenhagen, Denmark, June 9--11}
}

@InProceedings{BlHu87,
  author = 	 {Bloss, A. and Hudak, P.},
  title = 	 {{Path Semantics}},
  number =	 298,
  series =	 LNCS,
  pages =	 {476--489},
  booktitle =    {{Mathematical Foundations of Programming Language
                   Semantics}}, 
  year =	 {1987},
  publisher =	 S-V
}

@InProceedings{DrPu90,
  author = 	 {Draghicescu, M. and Purushothaman, S.},
  title = 	 {{A Compositional Analysis of Evaluation Order and its
                   Applications}} ,
  pages =	 {242--250},
  booktitle =    {{LFP'90 --- Conference on Lisp and Functional Programming}},
  address =      {Nice, France, June 27--29},
  year =	 {1990},
  publisher =	 {ACM Press}
}

@Article{HuHa91,
  author = 	 {Hunt, S. and Hankin, C.},
  title = 	 {{Fixed Points and Frontiers: a New Perspective}},
  journal =	 JFP,
  year =	 {1991},
  volume =	 {1},
  number =	 {1},
  pages =	 {91--120},
  month =	 {January}
}

@InProceedings{HuntSands:PEPM91,
  author = 	{Hunt, S.  and Sands, D.},
  title = 	{{Binding Time Analysis: A New PERspective}},
  booktitle = 	{{Symposium on Partial Evaluation and Semantics-Based
                 Program Manipulation}}, 
  year = 	{1991},
  month = 	{September},
  note = 	{ACM SIGPLAN Notices 26(9)}
}

@InProceedings{Lest89,
  author = 	 {Lester, D.R.},
  title = 	 {{Stacklessness: Compiling Recursion for a Distributed
                   Architecture}}, 
  booktitle =    {{FPCA'89 --- Conference on Functional Programming Languages and
                   Computer Architecture}}, 
  address =      {Imperial College, London, September 11--13},
  year =	 1989,
  publisher =	 {ACM Press},
  pages =	 {116--128},
  keywords =     {Static analysis},
  annote =	 {Analysis to determine required size of a packet in HDG
                  machine, thus avoiding a stack}
}

%$%node Solving recursive finite-difference equations, Tmp, Misc analysis, Program Analyses
%$%subsection Solving recursive finite-difference equations
% Most of these are from the bibliography in \cite{Rose89}

 
@Article{CoKa77,
  author = 	 {Cohen, J. and Katcoff, J.},
  title = 	 {{Symbolic Solution of Finite-Difference Equations}},
  journal =	 {Transactions on Mathematical Software},
  year =	 {1977},
  volume =	 {3},
  number =	 {3},
  pages =	 {261--271},
  month =	 {September}
}

@Article{Cohe82,
  author = 	 {Cohen, J.},
  title = 	 {{Computer-Assisted Microanalysis of Programs}},
  journal =	 cacm,
  year =	 {1982},
  volume =	 {25},
  number =	 {10},
  pages =	 {724--733},
  month =	 {October}
}


@Article{HiCo88,
  author = 	 {Hickey, T. and Cohen, J.},
  title = 	 {{Automating Program Analysis}},
  journal =	 jacm,
  year =	 {1988},
  volume =	 {35},
  number =	 {1},
  pages =	 {185--220},
  month =	 {January}
}

@PhdThesis{Rams79,
  author = 	 {Ramshaw, L.H.},
  title = 	 {{Formalizing the Analysis of Algorithms}},
  school = 	 {Stanford University},
  year = 	 {1979}
}


@TechReport{BHS78,
  author = 	 {Bentley, J.L. and Haken, D. and Saxe, J.B.},
  title = 	 {{A General Method for Solving Divide and Conquer
                   Recurrences}}, 
  institution =  {Department of Computer Science},
  year = 	 {1978},
  number =	 {78-154},
  address =	 {Carnegie-Mellon University},
  month =	 {December}
}


@TechReport{Moni79,
  author = 	 {Monier, L.},
  title = 	 {{Combinatorial Solutions of Multidimensional
                   Divide-and-Conquer Recurrences}},
  institution =  {Department of Computer Science},
  year = 	 {1979},
  number =	 {79-126},
  address =	 {Carnegie-Mellon University},
  month =	 {May}
}


@Article{FlSt87,
  author = 	 {Flajolet, P. and Steyaert, J-M.},
  title = 	 {{A Complexity Calculus for Recursive Tree Algorithms}},
  journal =	 {Mathematical Systems Theory},
  year =	 {1987},
  volume =	 {19},
  pages =	 {301--331}
}


@PhdThesis{Petk91,
  author = 	 {Petkov\v{s}ek, M.},
  title = 	 {{Finding Closed-Form Solutions of Difference Equations by
                   Symbolic Methods}},
  school = 	 {School of Computer Science},
  year = 	 {1990},
  address =      {Carnegie Mellon University},
  month =	 {September},
  note =	 {CMU-CS-91-103},
}

@InProceedings{PeSa93,
  author = 	 {Petkov\v{s}ek, M. and Salvy, B.},
  title = 	 {{Finding All Hypergeometric Solutions of Linear
                  Differential Equations}},
  year = 	1993,
  pages =       {27--33},
  address = 	{Kiev, Ukraine, July 6--8},
  booktitle = 	{{ISSAC'93 --- International Symposium on Symbolic and
		  Algebraic Computation}},
  publisher =	 {ACM Press},
  annote =	 {there is a J of Symb Comp paper 92 of similar title}
}

@article {Petk92,
   author = {Petkov{\v{s}}ek, M.},
   title = {{Hypergeometric Solutions of Linear Recurrences with Polynomial
	Coefficents}},
   journal = JSC,
   year = {1992},
   volume = {14},
   number = {2 and 3},
   pages = {243--264},
   month = {August\&September},
   keywords = {}}

@Article{Sing90,
  author = 	 {Singer, M.F.},
  title = 	 {{Formal Solutions of Differential Equations}},
  journal = 	 JSC,
  year = 	 1990,
  volume =	 10,
  pages =	 {59--94},
  annote =	 {bibliography on finding closed form solutions to linear
                  differential equations}
}

%$%node Tmp,  , Solving recursive finite-difference equations, Program Analyses
%$%subsection Tmp

@manual{HPF93,
 author      ={},
 title       ={{High Performance Fortran Language Specification}},
 organization={Rice University},
 address     ={Houston, Texas},
 edition     ={1.1},
 month       ={May},
 year        ={1993},
 scope       ={f90},
 url = {http://www.erc.msstate.edu/hpff/hpf-report-ps/hpf-v11.ps},
 documentSize={415.7 kbytes}
}

@Manual{HPF2,
  title = 	 {{High Performance Fortran Language Specification}},
  author =	 {High Performance Fortran Forum},
  month =	 jan,
  year =	 1997,
  note =	 {Version 2.0},
  url =          {http://wotug.ukc.ac.uk/parallel/standards/hpf/hpf-v20-final.tar.gz},
}

@InProceedings{Wins97,
  key =		 {derive},
  author =	 {Winstanley, N.},
  title =	 {{A Type-Sensitive Preprocessor for Haskell}},
  year =         1997,
  booktitle =	 {Glasgow Workshop on Functional Programming},
  address =	 {Ullapool, Scotland, September 15--17},
  note =         {Submitted for publication},
  url =          {http://www.dcs.gla.ac.uk/\~{}nww/Papers/GlaFP-draft.ps.Z},
  abstract = "This paper examines the use of type-safe linguistic reflection in Haskell, a
functional language. Several uses are identified: providing an extensible
system of instance derivation; extending derivation beyond type classes and
providing a method for polytypic programming. A preprocessor for Haskell is
presented that performs compile-time reflection upon type declarations in
source code.",
}

@InProceedings{JDH97,
  author = 	 {Junaidu, S. and Davie, A. and Hammond, K.},
  title = 	 {{Naira: A Parallel$^{2}$ Haskell Compiler}},
  booktitle = 	 {{IFL'97 --- International Workshop on the Implementation of
                  Functional Languages}},
  year = 	 1997,
  pages =        {215--231},
  series =       LNCS,
  volume =       {1467},
  address = 	 {September 10--12, St.\ Andrews, Scotland},
  annote = 	 {was Sahl97 in draft proceedings},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/naira.ps.gz}
}

@PhdThesis{Juna98,
  author = 	 {Junaidu, S.},
  title = 	 {{A Parallel Functional Language Compiler for Message
                  Passing Multicomputers}},
  school = 	 {School of Mathematical and Computational Sciences},
  year = 	 1998,
  month =        {March},
  address =	 {University of St.\ Andrews},
  annote =	 {earlier version was Juna97},
  url =          {http://www-fp.dcs.st-and.ac.uk/publications/1998/junaidu-thesis.ps.gz}
}

% ---------------------------------------------------------------------------
%$%node Time analysis, Granularity Improvement Mechanisms, Program Analyses, Top
%$%section Time analysis
%
% I use `time analysis' as the most general notion, including theoretical 
% analysis as well as comile-time analysis.
% ---------------------------------------------------------------------------

%$%menu
%* Time analysis for Lazy Languages::  
%* Time analysis for Strict Languages::	 
%* Granularity analysis for logic programs::  
%* Using complexity information::  
%* Theoretical complexity analysis::  
%* Misc related papers::	
%$%end menu

%$%node  Time analysis for Lazy Languages, Time analysis for Strict Languages, Time analysis, Time analysis
%$%subsection Time analysis for Lazy Languages

@InProceedings{BjHo89,
  author = 	 {Bjerner, B. and HolmstrÅˆm, S.},
  title = 	 {{A Compositional Approach to Time Analysis of First Order
		  Lazy Functional Programs}},
  pages =	 {157--165},
  booktitle =    {{FPCA'89 ---  Conference on Functional Programming
		  Languages and Computer Architecture}}, 
  year =	 {1989},
  publisher =	 {ACM Press}
}


@PhdThesis{Bjer89,
  author = 	 {Bjerner, B.},
  title = 	 {{Time Complexity of Programs in Type Theory}},
  school = 	 {Department of Computer Sciences},
  year = 	 {1989},
  address = 	 {University of GÅˆteborg},
  month =	 {January}
}

@inproceedings{Sand90a,
  author = 	{Sands, D.},
  title = 	{{Complexity Analysis for a Lazy Higher-Order Language}},
  booktitle = 	{{ESOP'90 --- European Symposium on Programming}},
  publisher = 	S-V,
  year = 	{1990},
  address =     {Copenhagen, Denmark, May 15--18},
  volume = 	{432},
  pages =       {361--376},
  series = 	LNCS,
  documentURL = {http://www.cs.chalmers.se/\~{}dave/papers/Sands:ESOP90.ps.gz},
  abstract =    "This  paper is  concerned with  the time-analysis  of  functional programs.
Techniques which enable  us to reason formally about  a program's execution
costs  have had  relatively little  attention  in the  study of  functional
programming.  We concentrate here  on the  construction of  equations which
compute the time-complexity of expressions in a lazy higher-order language.

The problem with higher-order functions  is that complexity is dependent on
the cost of applying functional parameters. Structures called cost-closures
are introduced to allow us to model both functional parameters and the cost
of their application.

The   problem  with   laziness   is  that   complexity   is  dependent   on
context.Projections  are  used to  characterise  the  context  in which  an
expression  is  evaluated, and  cost-equations  are  parameterised by  this
context-description to give a compositional time-analysis.  Using this form
of   context  information   we  introduce   two  types   of  time-equation:
sufficient-time  equations  and  necessary-time equations,  which  together
provide bounds on the exact time-complexity."
}

@PhDThesis{Sand90,
  author = 	{Sands, D.},
  title = 	{{Calculi for Time Analysis of Functional Programs}},
  school = 	{{Imperial College}},
  year = 	{1990},
  address = 	{University of London},
  month = 	{September},
  documentURL = {http://www.cs.chalmers.se/~dave/papers/PhDthesis.ps},
  abstract =    "Techniques  for  reasoning   about  extensional  properties  of  functional
programs  are well-understood,  but  methods for  analysing the  underlying
intensional,  or operational  properties  have been  much neglected.   This
thesis presents  the development  of several calculi  for time  analysis of
functional programs.

We focus on two features, higher-order functions and lazy evaluation, which
contribute much to the expressive power and semantic elegance of functional
languages, but serve to make operational properties more opaque.  Analysing
higher-order functions  is problematic because complexity  is dependent not
only  on  the  cost  of  computing,  but  also  on  the  cost  of  applying
function-valued expressions.   Techniques for statically  deriving programs
which compute time-cost in the presence of arbitrary higher-order functions
are developed.  The key  to this process  is the introduction  of syntactic
structures called cost-closures, which  enable intensional properties to be
carried by functions. The approach  is formalised by the construction of an
appropriate cost-model, against which  the correctness of the derivation is
proved. A  specific factorisation  tactic for reasoning  about higher-order
functions out  of context is illustrated.  Reasoning  about lazy evaluation
(ie call-by-name, or more usually, call-by-need) is problematic because the
cost of evaluating an expression cannot be understood simply from the costs
of  its   sub-expressions.   A  direct  calculus  for   reasoning  about  a
call-by-name language with lazy lists  is derived from a simple operational
model.  In  order  to  extend  this  calculus with  a  restricted  form  of
equational  reasoning, a  nonstandard notion  of  operational approximation
called cost-simulation is developed, by analogy with (bi)simulation in CCS.

The  problem  with  calculi  of  the  above  form,  based  directly  on  an
operational model, is that they do not yield a compositional description of
cost, and  cannot model lazy  evaluation (graph-reduction) easily.  We show
how a  description of the context in  which a function is  evaluated can be
used to parameterise two  types of time-equation: sufficient-time equations
and necessary-time  equations, which together  provide bounds on  the exact
time-cost of  lazy evaluation.  This  approach is extended  to higher-order
functions using a modification of the cost-closure technique." 
}


@InProceedings{Sand91,
  author = 	{Sands, D.},
  title = 	{{Time Analysis, Cost Equivalence and Program Refinement}},
  booktitle = 	{Conference on Foundations
		 of Software Technology and Theoretical Computer Science},
  year = 	{1991},
  pages = 	{25--39},
  publisher = 	S-V,
  number = 	{560},
  series = 	LNCS,
  month = 	{December},
}

@InProceedings{Sands:GFPW91,
  author = 	{Sands, D.},
  title = 	{{Operational Theories of Improvement in Functional Languages
		 (Extended Abstract)}},
  booktitle = 	{{Glasgow Workshop on Functional Programming}},
  year = 	{1991},
  publisher = 	{{S}pringer Workshop Series},
  abstract =     "This paper is concerned with the time-analysis of
                 functional programs. Techniques which enable us to
                 reason formally about a program's execution costs have
                 had relatively little attention in the study of
                 functional programming. We concentrate here on the
                 construction of equations which compute the
                 time-complexity of expressions in a lazy higher-order
                 language. The problem with higher-order functions is
                 that complexity is dependent on the cost of applying
                 functional parameters. Structures called {\em cost
                 closures\/} are introduced to allow us to model both
                 functional parameters {\em and\/} the cost of their
                 application. The problem with laziness is that
                 complexity is dependent on {\em context\/}. Projections
                 are used to characterise the context in which an
                 expression is evaluated, and cost-equations are
                 parameterized by this context-description to give a
                 compositional time-analysis. Using this form of context
                 information we introduce two types of time-equation:
                 {\em sufficient-time\/} equations and {\em
                 necessary-time\/} equations, which together provide
                 bounds on the exact time-complexity.",
  keywords =     "Granularity Analysis",
}

@article{Sands95,
  author = {Sands, D},
  title =  {{A Na\"{\i}ve Time Analysis and its
                 Theory of Cost Equivalence}},
  journal = {Journal of Logic and Computation},
  year = {1995},
  volume = {5},
  number = {4},
  optpages = {495--541},
  optpostscript = {},
  abstract = "Techniques for reasoning about extensional properties of
functional programs are well-understood, but methods for analysing the
underlying intensional, or operational properties have been much neglected.
This paper begins with the development of a simple but useful
calculus for time analysis of non-strict functional programs with lazy
lists.  One limitation of this basic calculus is that the ordinary
equational reasoning on functional programs is not valid. In order to buy
back some of these equational properties we develop a non-standard
operational equivalence relation called {\em cost equivalence}, by
considering the number of computation steps as an ``observable'' component
of the evaluation process.  We define this relation by analogy with Park's
definition of bisimulation in {\sc ccs}. This formulation allows us to show
that cost equivalence is a contextual congruence (and thus is substitutive
with respect to the basic calculus) and provides useful proof techniques
for establishing cost-equivalence laws. It is shown that basic evaluation time can be derived
by demonstrating a certain form of cost equivalence, and we give a
axiomatisation of cost equivalence which complete is with respect to
this application. This shows that cost equivalence subsumes the basic
calculus. Finally we show how a new operational interpretation of evaluation
demands can be used to provide a smooth interface between this time
analysis and more compositional approaches, retaining the advantages
of both.",
  documentURL = {http://www.cs.chalmers.se/\~{}dave/papers/JLC-FINAL-VERSION.ps},
}

@InProceedings{HPS96,
 author      ={Hughes, R.J.M. and Pareto, L. and Sabry, A.},
 title       ={{Proving the Correctness of Reactive Systems Using 
               Sized Types}},
 address     ={St.\ Petersburg Beach, Florida},
 publisher   ={ACM},
 OPTeditor   ={{Steele Jr.}, G.L.},
 OPTvolume      ={23},
 booktitle   ={{POPL'96 --- Symposium on Principles of Programming
                   Languages}},
 month       ={January},
 year        ={1996},
 scope       ={static},
 abstractURL ={http://www.cs.chalmers.se/\~{}rjmh/Papers/popl-96.html},
 url         ={http://www.cs.chalmers.se/\~{}rjmh/Papers/popl-96.ps},
 keywords    ={sized types, reactive systems, embedded software, 
               liveness and termination analysis, productivity},
 abstract =   "We have designed and implemented a type-based analysis for proving some
basic properties of reactive systems. The analysis manipulates rich type
expressions that contain information about the sizes of recursively defined
data-structures. Sized types are useful for detecting deadlocks,
non-termination, and other errors in embedded programs. To establish the
soundness of the analysis we have developed an appropriate semantic model
of sized types."
}


@Article{LaBa96,
  author = 	 {Launchbury, J. and Baraki, G.},
  title = 	 {{Representing Demand by Partial Projections}},
  journal = 	 JFP,
  year = 	 1996,
  volume =	 6,
  number =	 4,
  month =	 {July},
  pages =	 {563--585},
  annote =	 {not on time analysis, but modelling demand is needed for
                  a lazy time analysis},
  abstractURL =  {http://www.dcs.glasgow.ac.uk/jfp/bibliography/References/launchburyb1996:563.html},
  abstract =     "The projection-based strictness analysis of Wadler and Hughes is
     elegant and theoretically satisfying except in one respect: the
     need for lifting. The domains and functions over which the
     analysis is performed need to be transformed, leading to a less
     direct correspondence between analysis and program than might be
     hoped for. In this paper we shall see that the projection
     analysis may be reformulated in terms of partial projections, so
     removing this infelicity. There are additional benefits of the
     formulation: the two forms of information captured by the
     projection are distinguished, and the operational significance of
     the range of the projection fits exactly with the theory of
     unboxed types."
}

%$%node  Time analysis for Strict Languages, Granularity analysis for logic programs, Time analysis for Lazy Languages, Time analysis
%$%subsection Time analysis for Strict Languages

@InProceedings{HuLa92,
  author = 	 {Huelsbergen, L. and Larus, J.R.},
  title = 	 {{Dynamic Program Parallelization}},
  pages =	 {311--323},
  booktitle =    {{LFP'92 --- Conference on Lisp and Functional Programming}},
  year =	 {1992},
  publisher =	 {ACM Press},
  address =	 {San Francisco, California, June 22--24}
}

@TechReport{Huel93,
  author = 	 {Huelsbergen, L.},
  title = 	 {{Dynamic Program Parallelization}},
  institution =  {Department of Computer Science},
  year = 	 {1993},
  number =	 {TR-93-1178},
  address =	 {University of Wisconsin/Madison},
  month =	 {September}
}

@InProceedings{HLA94,
  author = 	 {Huelsbergen, L. and Larus, J.R. and Aiken, A.},
  title = 	 {{Using Run-Time List Sizes to Guide Parallel Thread
		  Creation}},
  booktitle =	 {LFP'94 --- Conference on Lisp and Functional Programming},
  pages =	 {79--90},
  year =	 {1994},
  address =      {Orlando, Florida, June 27--29},
  publisher =	 {ACM Press},
  month =	 {June}
}

@InProceedings{ReGi94,
  author = 	 {Reistad, B. and Gifford, D.K.},
  title = 	 {{Static Dependent Costs for Estimating Execution Time}},
  booktitle =    {{LFP'94 --- Conference on Lisp and Functional Programming}},
  pages =	 {65--78},
  year =	 {1994},
  address =      {Orlando, Florida, June 27--29},
  publisher =	 {ACM Press},
  month =	 {June},
  url =  {http://www-psrg.lcs.mit.edu/ftpdir/pub/reistad/lfp94.ps},
  abstractURL=   {http://www-psrg.lcs.mit.edu/\~{}reistad/lfp94-abstract.html},
  abstract =     "We present the first system for estimating and using data-dependent expression
execution times in a language with first-class procedures and imperative
constructs. The presence of first-class procedures and imperative constructs
makes cost estimation a global problem that can benefit from type
information. We estimate expression costs with the aid of an algebraic type
reconstruction system that assigns every procedure a type that includes a
static dependent cost. A static dependent cost describes the execution time of
a procedure in terms of its inputs. In particular, a procedure's static
dependent cost can depend on the size of input data structures and the cost of
input first-class procedures. Our cost system produces symbolic cost
expressions that contain free variables describing the size and cost of the
procedure's inputs. At run-time, a cost estimate is dynamically computed from
the statically determined cost expression and run-time cost and size
information. We present experimental results that validate our cost system on
three compilers and architectures. We experimentally demonstrate the utility
of cost estimates in making dynamic parallelization decisions. In our
experience, dynamic parallelization meets or exceeds the parallel performance
of any fixed number of processors."
}

% -----------------------------------------------------------------------------

@TechReport{Shul85,
  author = 	 {Shultis, J.},
  title = 	 {{On the Complexity of Higher-Order Programs}},
  institution =  {University of Colorado},
  year = 	 {1985},
  type =	 {Technical Report},
  number =	 {CU-CS-288},
  month =	 {February}
}

@Article{LeMe88,
  author = 	 {{Le M\'{e}tayer}, D.},
  title = 	 {{ACE: An Automatic Complexity Evaluator}},
  journal =	 {ACM Transactions on Programming Languages and Systems},
  year =	 {1988},
  volume =	 {10},
  number =	 {2},
  pages =        {248--266},
  month =	 {April},
  abstract =     "There has been a great deal of research done on the
                 evaluation of the complexity of particular algorithms;
                 little effort, however, has been devoted to the
                 mechanization of this evaluation. The ACE (Automatic
                 Complexity Evaluator) system is able to analyze
                 reasonably large programs, like sorting programs, in a
                 fully mechanically way. A time-complexity function is
                 derived from the initial functional program. This
                 function is transformed into its nonrecursive
                 equivalent according to McCarthy's recursion induction
                 principle, using a predefined library of recursive
                 definitions. As the complexity is not a decidable
                 property, this transformation will not be possible in
                 all cases. The richer the predefined library is, the
                 more likely the system is to succeed. The operations
                 performed by ACE are described and the use ofthe system
                 is illustrated with the analysis of a sorting
                 algorithm. Related works and further improvements are
                 discussed in the conclusion.",
  keywords =     "Granularity Analysis",

}

@InProceedings{Meta85,
  author =       {{Le M\'{e}tayer}, D.},
  title =        {{Mechanical Analysis of Program Complexity}},
  booktitle =    {SIGPLAN '85 Symposium},
  series =       {SIGPLAN Notices},
  volume =       {20(7)},
  OPTnumber =       {7},
  pages =        {69--73},
  publisher =    {ACM Press, New York},
  year =         {1985},
  abstract =     "This paper describes the ACE system for automatic
                 generation of complexity functions for a subset of FP.
                 This system is based on program transformation, with a
                 library of more than 1000 transformation rules which
                 are applied to transform the step-counting version of a
                 program to a composition of the complexity function and
                 a size measure.",
  keywords =     "Granularity Analysis",
}

@InProceedings{Rose89,
  author = 	 {Rosendahl, M.},
  title = 	 {{Automatic Complexity Analysis}},
  pages =	 {144--156},
  booktitle =    {{FPCA'89 ---  Conference on Functional Programming
		  Languages and Computer Architecture}}, 
  year =	 {1989},
  publisher =	 {ACM Press},
  url =          {ftp://ftp.diku.dk/diku/semantics/papers/D-36.dvi.Z}, 
  abstract =     "One way to analyze programs is to derive expressions
                 for their computational behavior. A time bound function
                 (or worst-case complexity) gives an upper bound for the
                 computation time as a function of the size of the
                 input. We describe a system to derive such time bounds
                 automatically using abstract interpretation. The
                 semantics-based setting makes it possible to prove the
                 correctness of the time-bound function. The system can
                 analyze programs in a first-order subset of Lisp and we
                 show how the system also can be used to analyze
                 programs in other languages.",
  keywords =     "Complexity Analysis, Abstract Interpretation",
}

@TechReport{Talc89,
  author = 	 {Talcott, C.},
  title = 	 {{Programming and Proving with Function and Control
                   Abstractions}}, 
  institution =  {Department of Computer Science},
  year = 	 {1989},
  number =	 {STAN-CS-89-1288},
  address =	 {Stanford University},
  month =	 {October}
}

@TechReport{SkCa93,
  author = 	 {Skillicorn, D.B. and Cai, W.},
  title = 	 {{A Cost Calculus for Parallel Functional Programming}},
  institution =  {Queens University},
  year = 	 {1993},
  number =	 {ISSN-0836-0227-93-348},
  address =	 {Kingston, Canada},
  url =          {ftp://ftp.qucis.queensu.ca/pub/skill/costcalculusII.ps.Z}
}

@Article{SkTa98,
  author = 	 {Skillicorn, D.B. and Talia, D.},
  title = 	 {{Models and Languages for Parallel Computation}},
  journal = 	 {ACM Computing Surveys},
  year = 	 {1998},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {June},
  OPTannote = 	 {},
  url =          {http://www.cs.queensu.ca/home/skill/models.ps},
  abstract =     "We  survey many  parallel programming  models  and languages.   We use  six
criteria  to  assess  their  suitability for  realistic  portable  parallel
programming, arguing that an ideal  model should be easy to program, should
have     a     software      development     methodology,     should     be
architecture-independent,   should  be  easy   to  understand,   should  be
efficiently  implementable, and should  provide accurate  information about
the cost  of programs.  We consider programming  models in  six categories,
depending on  the level of abstraction  they provide.  Those  that are very
abstract conceal even  the use of parallelism from  the software. They make
software  easy  to  build and  port,  but  efficiency  is usually  hard  to
achieve. At the other end of the spectrum, low-level models make all of the
messy issues  of parallel  programming explicit (how  many threads,  how to
place   them,  how   to  express   communication,  and   how   to  schedule
communication), so  that software is hard  to build and  not very portable,
but is usually  efficient.  Most recent models are near  the center of this
spectrum,  exploring   the  best  trade-offs   between  expressiveness  and
efficiency. However, there are models that are both abstract and able to be
implemented efficiently, opening the prospect of parallelism as part of the
mainstream of computing, rather than a high-performance backwater."
}

@InProceedings{Rang95,
  author = 	 {Rangaswami, R.},
  title = 	 {{HOPP --- A Higher-order Parallel Programming Model}},
  booktitle = 	 {Algorithms and Parallel VLSI Architectures},
  editor =	 {Moonen, M. and Cathoor, F.},
  year =	 1995,
  publisher =	 {Elsevier Science},
  keywords =     {Parallelisation Techniques, Functional Programming, Bird-Meertens Formalism, Higher-Order Functions, Compile-time Analysis, D-M MIMD Machines.},
  url =  {http://www.dcs.ed.ac.uk/home/ror/PAPERS/palgarc.ps.gz},
  abstract =     "The   efficient programming of    parallel computers is   still a difficult
task. This paper focuses on  studying methods for expressing parallelism in
programs without   making   the  programmer   explicitly  responsible   for
parallelism. Parallel programming using a set of useful implicitly-parallel
constructs is  considered. These constructs are   borrowed from FP  and the
Bird-Meertens  approach.  Programs   are  analysed  statically,  using   an
analytical cost model which  selects a cost-effective implementation for  a
chosen architecture. A  brief overview of  the model  is presented  and its
operation is demonstrated with the aid of two case studies."
}

@PhdThesis{Rang96,
  author = 	 {Rangaswami, R.},
  title = 	 {{A Cost Analysis for a Higher-order Parallel Programming
                  Model}},
  school = 	 {Department of Computer Science},
  address =      {University of Edinburgh},
  year = 	 1996,
  month =	 {December},
  url =          {http://www.dcs.ed.ac.uk/home/ror/THESIS/thesis.ps.gz},
}

@InProceedings{Jay95,
  author = 	 {Jay, C.B.},
  title = 	 {{Shape Analysis for Parallel Computing}},
  booktitle = 	 {International Parallel Computing Workshop},
  year =	 1995,
  address =	 {Imperial College London, September 25--26},
  pages =	 {287--298},
  annote =	 {reference from Roopa's thesis}
}

@InProceedings{JCSS97,
  author =       {Jay, C.B. and Cole, M.I. and Sekanina, M. and Steckler, P.},
  title =        {{A Monadic Calculus for Parallel Costing of a Functional 
                   Language of Arrays}},
  booktitle =    {{EuroPar97 --- Parallel Processing}},
  OPTeditor =       {C. Lengauer and M. Griebl and S. Gorlatch},
  volume =       {1300},
  series =       LNCS,
  year =         {1997},
  publisher =    S-V,
  month =        {August},
  pages =        {650--661},
}

% Cost Analysis

@Article{DJG92,
  author = 	 {Dornic, V. and Jouvelot, P. and Gifford, D.K.},
  title = 	 {{Polymorphic Time Systems for Estimating Program
                   Complexity}},
  journal =	 {ACM Letters on Programming Languages and Systems},
  year =	 {1992},
  volume =	 {1},
  number =	 {1},
  pages =	 {33--45},
  month =	 {March},
  url =          {http://www.psrg.lcs.mit.edu/ftpdir/papers/loplas.dvi},
  abstract =     "We present a new approach to static program analysis that permits each
expression in a program to be assigned an execution time estimate. Our
approach uses atime system in conjunction with a conventional type system
to compute both the type and the time of an expression. The time of an
expression is either an integer upper bound on the number of ticks the
expression will execute, or the distinguished element long that indicates
that the expression contains a loop and thus may run for an arbitrary
length of time. Every function type includes a latent time that is used to
communicate its expected execution time from the point of its definition to
the points of its use. Unlike previous approaches a time system works in
the presence of first-class functions and separate compilation. In
addition, time polymorphism allows the time of a function to depend on the
times of any functions that it takes as arguments. Time estimates are
useful when compiling programs for multiprocessors in order to balance the
overhead of initiating a concurrent computation against the expected
execution time of the computation. The correctness of our time system is
proven with respect to a dynamic semantics."
}

@Article{TaJo92,
  author = 	 {Talpin, J-P. and Jouvelot, P.},
  title = 	 {{Polymorphic Type, Region and Effect Inference}},
  journal = 	 JFP,
  year = 	 1992,
  volume =	 2,
  number =	 3,
  month =	 {July},
  pages =	 {245--271},
  annote =	 {mainly talks about effects but most of the work is
                  closely related to mine; espec, they have a type and
                  effect recosntruction algorithm and allow subeffecting;
                  proofs of soundness and completeness are given},
}

@TechReport{Dorn93,
  author = 	 {Dornic, V.},
  title = 	 {{Ordering Times}},
  institution =  {Department of Computer Science},
  year = 	 {1993},
  type =         {Research Report}, 
  number =	 {{YALEU/DCS/RR-956}},
  address =	 {Yale University},
  month =	 {April}
}

@InProceedings{LuGi88,
  author = 	 {Lucassen, J.M. and Gifford, D.K.},
  title = 	 {{Polymorphic Effect Systems}},
  booktitle =	 {{POPL'88 --- Symposium on Principles of Programming
                   Languages}},
  pages =	 {47--57},
  year =	 {1988},
  organization = {ACM},
  address =	 {San Diego, California},
  month =	 {January}
}

@InProceedings{Wadl98,
  author = 	 {Wadler, P.},
  title = 	 {{The Marriage of Effects and Monads}},
  booktitle =	 {{POPL'98 --- Symposium on Principles of Programming
                   Languages}},
  year =	 1998,
  OPTnote =	 {Submitted},
  url =          {http://cm.bell-labs.com/cm/cs/who/wadler/papers/effects/effects.ps.gz},
  abstractURL =  {http://cm.bell-labs.com/cm/cs/who/wadler/topics/recent.html},
  abstract =     "Gifford and others proposed an effect typing discipline to delimit the scope
of computational effects within a program, while Moggi and others proposed
monads for much the same purpose. Here we marry effects to monads, uniting two
previously separate lines of research. In particular, we show that the type,
region, and effect system of Talpin and Jouvelot carries over directly to an
analogous system for monads, including a type and effect reconstruction
algorithm. The same technique should allow one to transpose any effect systems
into a corresponding monad system."
}

@InProceedings{ShGi90,
  author = 	 {Sheldon, M.A. and Gifford, D.K.},
  title = 	 {{Static Dependent Types for First Class Modules}},
  pages =	 {20--29},
  booktitle =    {{LFP'90 --- Conference on Lisp and Functional Programming}},
  address =      {Nice, France, June 27--29},
  year =	 {1990},
  publisher =	 {ACM Press},
  url =          {http://www.psrg.lcs.mit.edu/ftpdir/papers/lfp90.dvi},
  abstract =     "Static dependent types are the basis of a new type system that permits
types and values to be packaged together into first class modules. Unlike
other approaches to modules, static dependent types permit unrestricted
access to the types and values in first class modules without sacrificing
static type checking or data abstraction. Static dependent types are type
safe in the presence of side effects because they rely on an effect system
that can make deductions conventional type systems cannot. Experience with
an implementation, built as an extension to the FX-87 programming language,
shows that static dependent types can be used for building large systems.",
}

@InProceedings{JoGi91,
  author = 	 {Jouvelot, P. and Gifford, D.K.},
  title = 	 {{Algebraic Reconstruction of Types and Effects}},
  pages =	 {303--310},
  booktitle =    {{POPL'91 --- Symposium on Principles of Programming
                   Languages}},
  year =	 {1991},
  publisher =	 {ACM Press},
  url =  {http://www.psrg.lcs.mit.edu/ftpdir/papers/popl91.dvi},
  abstract =     "We present the first algorithm for reconstructing the types and effects of
expressions in the presence of first class procedures in a polymorphic
typed language. Effects are static descriptions of the dynamic behavior of
expressions. Just as a type describes what an expression computes, an
effect describes how an expression computes. Types are more complicated to
reconstruct in the presence of effects because the algebra of effects
induces complex constraints on both effects and types. In this paper we
show how to perform reconstruction in the presence of such constraints with
a new algorithm called algebraic reconstruction, prove that it is sound and
complete, and discuss its practical import. This research was supported by
DARP A under ONR Contract N00014-89-J-1988.
"
}

@InProceedings{OTGi89,
  author = 	 {{O'Toole}, J. and Gifford, D.K.},
  title = 	 {{Type Reconstruction with First-Class Polymorphic Values}},
  booktitle =    {{PLDI'89 --- Programming Languages Design and
                   Implementation}}, 
  series =       {SIGPLAN Notices},
  year =	 {1989},
  month =	 {June},
  publisher =    {ACM Press},
  keywords =     {type systems, effect systems, type inference, type reconstruction, FX-89},
  url =  {http://www.psrg.lcs.mit.edu/ftpdir/papers/pldi89-otoole.dvi},
  abstract =     "We present the first type reconstruction system which combines the implicit
typing of ML with the full power of the explicitly typed second-order
polymorphic lambda calculus. The system will accept ML-style programs,
explicitly typed programs, and programs that use explicit types for all
first-class polymorphic values. We accomplish this flexibility by providing
both generic and explicitly-quantified polymorphic types, as well as
operators which convert between these two forms of polymorphism. This type
reconstruction system is an integral part of the FX-89 programming
language. We present a type reconstruction algorithm for the system. The
type reconstruction algorithm is proven sound and complete with respect to
the formal typing rules."
}

% new 17/06/02

@Article{LiGo01,
  author = 	 {Liu, Y.A. and Gomez, G.},
  title = 	 {{Automatic accurate cost-bound analysis for high-level languages}},
  journal = 	 {IEEE Transactions on Computers},
  year = 	 2001,
  volume =	 50,
  number =	 12,
  pages =	 {1295--1390},
  month =	 dec,
  documentURL = {ftp://ftp.cs.sunysb.edu/pub/liu/CostAnal-TC01.ps.gz},
  abstract = "
This paper  describes a language-based approach for  automatic and accurate
cost-bound analysis. The approach  consists of transformations for building
cost-bound functions  in the presence of partially  known input structures,
symbolic  evaluation  of  the  cost-bound  function  based  on  input  size
parameters,  and optimizations to  make the  overall analysis  efficient as
well as  accurate, all  at the source-language  level. The  calculated cost
bounds  are  expressed  in   terms  of  primitive  cost  parameters.  These
parameters  can be  obtained based  on  the language  implementation or  be
measured conservatively or  approximately, yielding accurate, conservative,
or approximate time or space  bounds. We have implemented this approach and
performed  a  number of  experiments  for  analyzing  Scheme programs.  The
results helped confirm the accuracy of the analysis."
}

@TechReport{USL00,
  author = 	 {Unnikrishnan, L. and Stoller, S.D. and Liu, Y.A.},
  title = 	 {{Automatic Accurate Stack Space and Heap Space Analysis for High-Level Languages}},
  institution =  {Computer Science Dept},
  year = 	 2000,
  type =	 {Technical Report},
  number =	 538,
  address =	 {Indiana University},
  month =	 apr,
  documentURL = {http://www.cs.sunysb.edu/~leena/papers/TR538.ps.gz},
  abstract = "This paper describes a general approach for automatic and accurate space
and space-bound analyses for high-level languages, considering stack space,
heap  allocation and live  heap space  usage of  programs. The  approach is
based on program  analysis and transformations and is  fully automatic. The
analyses produce accurate  upper bounds in the presence  of partially known
input  structures. The  analyses  have been  implemented, and  experimental
results confirm the accuracy."
}

@PhdThesis{Grob01,
  author = 	 {Grobauer, B.},
  title = 	 {{Topics in Semantics-based Program Manipulation}},
  school = 	 {BRICS International PhD School in Computer Science},
  year = 	 2001,
  month =      jul,
  address =	 {University of Aarhus},
  documentURL = {http://www.brics.dk/DS/01/6/BRICS-DS-01-6.ps.gz},
  abstract = "
Programming is at  least as much about manipulating existing  code as it is
about  writing new code.  Existing code  is modified,  for example  to make
inefficient  code run  faster,  or  to accommodate  for  new features  when
reusing  code; existing  code is  analyzed, for  example to  verify certain
program  properties,   or  to  use   the  analysis  information   for  code
modifications.  Semantics-based program  manipulation addresses methods for
program modifications  and program analyses  that are formally  defined and
therefore  can  be  verified   with  respect  to  the  programming-language
semantics.  This  dissertation comprises  four  articles  in  the field  of
semantics-based  techniques for  program manipulation:  three  articles are
about partial  evaluation, a method for program  specialization; the fourth
article treats an approach to automatic cost analysis.

Partial evaluation optimizes programs  by specializing them with respect to
parts of their input that  are already known: Computations that depend only
on  known  input  are   carried  out  during  partial  evaluation,  whereas
computations that depend on unknown  input give rise to residual code.  For
example,  partially evaluating  an interpreter  with respect  to  a program
written  in the  interpreted  language  yields code  that  carries out  the
computations  described by  that  program; partial  evaluation  is used  to
remove interpretive overhead. In effect,  the partial evaluator serves as a
compiler from the interpreted  language into the implementation language of
the interpreter.  Compilation  by partial evaluation is known  as the first
Futamura projection. The second  and third Futamura projection describe the
use of  partial evaluation  for compiler generation  and compiler-generator
generation,  respectively;  both  require  the partial  evaluator  that  is
employed to be self applicable.

The first  article in this  dissertation describes how the  second Futamura
projection can  be achieved for type-directed partial  evaluation (TDPE), a
relatively  recent  approach  to   partial  evaluation:  We  derive  an  ML
implementation  of the  second Futamura  projection  for TDPE.  Due to  the
differences between  `traditional', syntax-directed partial  evaluation and
TDPE,   this   derivation  involves   several   conceptual  and   technical
steps.  These  include  a  suitable  formulation  of  the  second  Futamura
projection and techniques for making TDPE amenable to self-application.

In the  second article, compilation  by partial evaluation plays  a central
role  for  giving  a   unified  approach  to  goal-directed  evaluation,  a
programming-language paradigm that is  built on the notions of backtracking
and of generating successive results.  Formulating the semantics of a small
goal-directed  language  as  a  monadic semantics--a  generic  approach  to
structuring denotational  semantics-- allows us to  relate various possible
semantics to each other both conceptually and formally. We thus are able to
explain goal-directed  evaluation using an  intuitive list-based semantics,
while  using  a  continuation  semantics  for  semantics-based  compilation
through  partial  evaluation. The  resulting  code  is  comparable to  that
produced by an optimized compiler described in the literature.

The  third  article  revisits  one   of  the  success  stories  of  partial
evaluation, the generation of  efficient string matchers from intuitive but
inefficient implementations.  The basic idea  is that specializing  a naive
string matcher with respect to a  pattern string should result in a matcher
that searches a text for this  pattern with running time independent of the
pattern and  linear in  the length of  the text.  In order to  succeed with
basic partial-evaluation  techniques, the naive matcher has  to be modified
in a non-trivial way, carrying out so-called binding-time improvements.  We
present  a  step-by-step  derivation  of a  binding-time  improved  matcher
consisting of one problem- dependent step followed by standard binding-time
improvements. We also consider several variants of matchers that specialize
well, amongst them the first  such matcher presented in the literature, and
we demonstrate how variants can be derived from each other systematically.

The fourth article  is concerned with program analysis  rather than program
transformation.   A challenging  goal for  program analysis  is  to extract
information about time  or space complexity from a  program.  In complexity
analysis, one  often establishes cost recurrences as  an intermediate step,
and  this step  requires an  abstraction  from data  to data  size. We  use
information contained  in dependent types  to achieve such  an abstraction:
Dependent  ML (DML),  a conservative  extension of  ML,  provides dependent
types  that can  be  used to  associate  data with  size information,  thus
describing   a  possible  abstraction.    We  automatically   extract  cost
recurrences  from first-order  DML programs,  guiding the  abstraction from
data to data size with information contained in DML type derivations."
}


%$%node Granularity analysis for logic programs, Using complexity information, Time analysis for Strict Languages, Time analysis
%$%subsection Granularity analysis for logic programs

@Article{DeLi93,
  author = 	 {Debray, S.K. and Lin, N.-W.},
  title = 	 {{Cost Analysis of Logic Programs}},
  journal =	 toplas,
  year =	 {1993},
  volume =	 {15},
  number =	 {5},
  pages =	 {826--875},
  month =	 {November},
}

@InProceedings{LHD94,
  author = 	 {{L\'opez Garc{\'i}a}, P. and Hermenegildo, M. and Debray,
                   SK.}, 
  title = 	 {{Towards Granularity Based Control of Parallelism in
		  Logic Programs}},
  OPTeditor =	 {H. Hong},
  volume =	 {5},
  series =	 LNCS,
  pages =	 {133--144},
  booktitle =    {{PASCO'94 --- First International Symposium on Parallel Symbolic
		  Computation}}, 
  year =	 {1994},
  organization = {RISC-Linz},
  publisher =	 {World Scientific},
  address =	 {Hagenberg/Linz, Austria, September 26--28},
  url =          {ftp://clip.dia.fi.upm.es/pub/papers/granul\_control94.ps.Z},
}


@Article{LHD95,
  author = 	 {{L\'opez Garcia}, P.  and Hermenegildo, M. and Debray, S.K.},
  title = 	 {{A Methodology for Granularity Based Control of
                  Parallelism in Logic Programs}},
  journal = 	 {???},
  year = 	 1995,
  annote =	 {Exact reference (journal) unknown; see ParForce deliverables}
}

@InProceedings{DGHL94,
  author = 	 {Debray, S.K. and {L\'opez Garcia}, P. and
                  Hermenegildo, M. and Lin, N.-W.},
  title = 	 {{Estimating the Computational Costs of Logic Programs}},
  editor = 	 {{Le Charlier}, B.},
  volume =	 {864},
  series =	 LNCS,
  pages =	 {255--265},
  booktitle =	 {SAS'94 --- Static Analysis Symposium},
  year =	 {1994},
  publisher =	 S-V,
  address =	 {Namur, Belgium},
  month =	 {September},
}

@InProceedings{DLH90,
  author = 	 {Debray, S.K. and Lin, N-W. and Hermenegildo, M.},
  title = 	 {{Task Granularity Analysis in Logic Programs}},
  pages =	 {174--188},
  booktitle =    {{PLDI'90 --- Programming Languages Design and
                   Implementation}}, 
  series =       {SIGPLAN Notices},
  volume =       {25(6)},
  OPTnumber =       {6},
  year =	 {1990},
  month =	 {June},
  publisher =    {ACM Press},
  abstract =     "While logic programming languages offer a great deal
                        of scope for parallelism, there is usually some
                        overhead associated with the execution of goals in
                        parallel because of the work involved in task creation
                        and scheduling. In practice, therefore, the
                        ``granularity'' of a goal, i.e.\ an estimate of the
                        work available under it, should be taken into account
                        when deciding whether or not to executed a goal
                        concurrently as a separate task. This paper describes a
                        method for estimating the granularity of a goal at
                        compile time. The runtime overhead associated with our
                        approach is usually quite small, and the performance
                        improvements resulting from the incorporation of
                        grainsize control can be quite good. This is shown by
                        means of experimental results.",
}


@InProceedings{HeLo95,
  author = 	 {Hermenegildo, M. and {L\'opez Garcia}, P.},
  title = 	 {{Efficient Term Size Computation for Granularity Control}},
  booktitle = 	 {International Conference on Logic Programming},
  year =	 1995,
  publisher =	 MIT,
}

@PhdThesis{Buen94,
  author = 	 {Bueno, F.},
  title = 	 {{Automatic Optimisation and Parallelisation of Logic Programs
                   through Program Transformation}},
  school = 	 {Facultad de InformÅ·tica},
  year = 	 {1994},
  month =        {October},
  address = 	 {Universidad PolitÅÈcnica de Madrid},
  url =          {ftp://clip.dia.fi.upm.es/pub/papers/bueno.phd.ps.gz},
}

@PhdThesis{Garc9?,
  author = 	 {{Garcia de la Banda}, M.},
  title = 	 {{Independence, Global Analysis, and Parallelism in
                   Dynamically Scheduled Constraint Logic Programming}},
  school = 	 {Facultad de InformÅ·tica},
  year = 	 {1994},
  month =        {October},
  address = 	 {Universidad PolitÅÈcnica de Madrid},
  url =          {ftp://clip.dia.fi.upm.es/pub/papers/maria.phd.ps.gz},
}

@TechReport{GaHe93,
  author = 	 {{Garcia de la Banda}, M.J. and Hermenegildo, M.},
  title = 	 {{Some Considerations on the Compile-Time Analysis of
                   Constraint Logic Programs}},
  institution =  {Facultad de InformÅ·tica},
  year = 	 {1993},
  address = 	 {Universidad PolitÅÈcnica de Madrid},
  url =          {ftp://clip.dia.fi.upm.es/pub/papers/clp\_ai.ps.Z},
}


@TechReport{GaHe9x,
  author = 	 {{Garcia de la Banda}, M.J. and Hermenegildo, M.},
  title = 	 {{Effectiveness of Combined Sharing and Freeness Analysis 
                   using Abstract Interpretation}},
  institution =  {Facultad de InformÅ·tica},
  year = 	 {199x},
  address = 	 {Universidad PolitÅÈcnica de Madrid},
  url =          {ftp://clip.dia.fi.upm.es/pub/papers/effsf.ps.Z},
}

@TechReport{GaHe9xa,
  author = 	 {Lopez, P.  and Hermenegildo, M.},
  title = 	 {{A Technique for Dynamic Term Size Computation via
                   Program Transformation}},
  institution =  {Facultad de InformÅ·tica},
  year = 	 {199x},
  address = 	 {Universidad PolitÅÈcnica de Madrid},
  url =          {ftp://clip.dia.fi.upm.es/pub/papers/size\_comp.ps.Z},
}

@InProceedings{Tick88,
         author =       {Tick, E.},
         title =        {{Compile-Time Granularity Analysis for Parallel Logic
                        Programming Languages}},
         booktitle =    {{International Conference on Fifth Generation Computer
                        Systems}},
         address =      {Tokyo, Japan, November 28--December 2},
         year =         {1988},
         pages =        {994--1000},
         publisher =    {ICOT, Tokyo},
         abstract =     "The paper describes a simple compiler analysis method
                        for determining the ``weight'' of procedures in
                        parallel logic programming languages. Using Flat
                        Guarded Horn Clauses (FGHC) as an example, the analysis
                        algorithm is described. Consideration of weights has
                        been incorporated in the scheduler of a real-parallel
                        FGHC emulator running on the Sequent Symmetry
                        multiprocessor. Alternative demand-distribution methods
                        are discussed, including {\em oldest-first\/} and {\em
                        heaviest-first\/} distributions. Performance
                        measurements, collected from a group of non-trivial
                        benchmarks on eight processors, show that the new
                        schemes do {\em not\/} perform significantly faster
                        than conventional distribution methods. This result is
                        attributed to a combination of factors overshadowing
                        the benefits of the new method: high system overheads,
                        the low cost of spawning a goal on a shared memory
                        multiprocessor, and the increase in synchronization
                        caused by the new methods. Directions of further
                        research are discussed, indicating where further
                        speedup can be attained.",
         keywords =     "Granularity Analysis",
       }

@InProceedings{ZTD*92,
  author = 	 {Zhong, X. and Tick, E. and Duvvuru, S. and Hansen, L. and
		  Sastry, A.V.S. and Sundararajan, R.},
  title = 	 {{Towards an Efficient Compile-Time Granularity Analysis
		  Algorithm}},
  pages =	 {809--816},
  booktitle =	 {International Conference on Fifth Generation Computer Systems},
  year =	 {1992},
  organization = {Institute for New Generation Computer Technology (ICOT)},
  month =	 {June}
}

@Article{Tick93,
  author =       {Tick, E. and Zhong, X.},
  title =        {{A Compile-Time Granularity Analysis Algorithm and its
                 Performance Evaluation}},
  journal =      {New Generation Computing},
  volume =       {11},
  number =       {3-4},
  pages =        {271--295},
  year =         {1993},
  keywords =     "logic functional parallel",
  abstract =     "A major implementation problem with implicitly
                 parallel languages, is that small grain size can lead
                 to execution overheads and reduced performance. The
                 authors present a new granularity-analysis scheme that
                 produces estimators, at compile time, of the relative
                 execution weight of each procedure invocation. These
                 estimators can be cheaply evaluated at runtime to
                 approximate the relative task granularities, enabling
                 intelligent scheduling decisions. The authors method
                 seeks to balance tradeoffs between analysis complexity,
                 estimator accuracy, and runtime overhead of evaluating
                 the estimator. To this end, rather than analyze data
                 size or dependencies, they introduce iteration
                 parameters to handle recursive procedures. This
                 simplification facilitates solving the recurrence
                 equations that describe the granularity estimators, and
                 reduces the runtime overhead of evaluating these
                 estimators. The algorithm is described in the context
                 of concurrent logic programming languages, although the
                 concepts are applicable to functional languages in
                 general. They show, for a benchmark suite, that the
                 method accurately estimates cost. Multiprocessor
                 simulation results quantify the advantage of
                 dynamically scheduling tasks with the granularity
                 information.",
        }


@Article{SCK99,
  author = 	 {Shen, K. and Costa, V.S. and King, A.},
  title = 	 {{Distance: A New Metric for Controlling Granularity for Parallel Execution}},
  journal = 	 EJFLP,
  year = 	 1999,
  volume =	 {Special Issue 1},
  publisher=     MIT,
  url =          {http://www.cs.tu-berlin.de/journal/jflp/articles/1999/S99-01/JFLP-A99-05.ps.gz},
  abstract =     "
Granularity control  is a method to improve  parallel execution performance
by limiting  excessive parallelism.  The general  idea is that  if the gain
obtained by executing a task in parallel is less than the overhead required
to  support   parallel  execution,  then   the  task  is   better  executed
sequentially.  Traditionally, in logic  programming, task size is estimated
from the sequential time-complexity of  evaluating the task. Tasks are only
executed in parallel if task size exceeds a predetermined threshold.

We argue in this paper that the  estimation of complexity on its own is not
an ideal metric for improving  the performance of parallel programs through
granularity  control. We present  a new  metric for  measuring granularity,
based on a notion of \emph{distance}.  We provide some initial results with
two very  simple methods of using  this metric for  granularity control. We
then  discuss how  more sophisticated  granularity control  methods  can be
devised using the new metric. " 
}

%$%node Using complexity information, Theoretical complexity analysis, Granularity analysis for logic programs, Time analysis
%$%subsection Using complexity information

@TechReport{RaMa90,
  author = 	 {Rabhi, F.A. and Manson, G.A.},
  title = 	 {{Using Complexity Functions to Control Parallelism in
		  Functional Programs}},
  institution =  {Department of Computer Science},
  year = 	 {1990},
  number =	 {CS-90-1},
  address =	 {University of Sheffield},
  month =	 {January}
}

@Article{RaMa91,
  author = 	 {Rabhi, F.A. and Manson, G.A.},
  title = 	 {Divide-and-Conquer and Parallel Graph Reduction},
  journal = 	 {Parallel Computing},
  year = 	 1991,
  volume =	 17,
  pages =	 {189--205},
  keywords =     {parallel graph reduction; divide-and-conquer; skeletons}
}

@TechReport{Rabh91,
  author = 	 {Rabhi, F.A.},
  title = 	 {{Dynamic Combinators: Run-Time Control of the Granularity
		  in Functional Programs}},
  institution =  {Department of Computer Science},
  year = 	 {1991},
  address =	 {University of Hull},
  month =	 {December}
}

@InProceedings{Rabhi92a,
  author =       {Rabhi, F.A.},
  editor =       {Joosen, W. and Milgrom, E.},
  title =        {{Run-Time Control of the Granularity in Functional
                 Languages}},
  booktitle =    {European Workshop on Parallel Computing},
  address =      {Barcelona, Spain},
  pages =        {356--359},
  publisher =    {IOS Press},
  year =         {1992},
  abstract =     "Addresses the problem caused by fine granularity in a
                 parallel implementation of functional programming
                 languages. It discusses the use of a technique to
                 increase the grain size based on static analysis of the
                 program combined with run-time testing. A summary of
                 the main conclusions is presented, taking into account
                 some previous experimental results.",
        }

@Article{Rabh91a,
         author =       {Rabhi, F.A.},
         title =        {{Experiments with a Transputer-Based Parallel Graph
                        Reduction Machine}},
         journal =      {Concurrency: Practice and Experience},
         volume =       {3},
         number =       {4},
         pages =        {413--422},
         month =        aug,
         year =         {1991},
         abstract =     "This paper is concerned with the implementation of
                        functional languages on a parallel architecture, using
                        graph reduction as a model of computation. Parallelism
                        in such systems is automatically derived by the
                        compiler but a major problem is the fine granularity,
                        illustrated in Divide-and-Conquer problems at the
                        leaves of the computational tree. The paper addresses
                        this issue and proposes a method based on static
                        analysis combined with run-time tests to remove the
                        excess in parallelism. We report experiments on a
                        prototype machine, simulated on several connected INMOS
                        transputers. Performance figures show the benefits in
                        adopting the method and the difficulty of automatically
                        deriving the optimum partitioning due to differences
                        among the problems.",
         keywords =     "Parallel Graph Reduction, Granularity Control",
       }



%$%node Theoretical complexity analysis, Misc related papers, Using complexity information, Time analysis
%$%subsection Theoretical complexity analysis

@Article{FSZ91,
  author = 	 {Flajolet, P. and  Salvy, B. and Zimmermann, P.},
  title = 	 {{Automatic Average-Case Analysis of Algorithms}},
  journal =	 TCS,
  year =	 {1991},
  volume =	 {79},
  pages =	 {37--109}
}

@PhdThesis{Zimm90,
  author = 	 {Zimmermann, W.},
  title = 	 {{Automatische Komplexit\"atsanalyse von funktionalen
		  Programmen (Automatic Complexity Analysis of Functional
		  Programs) (in German)}},
  school = 	 {University of Karlsruhe},
  year = 	 {1990}
}

@Article{Wegb75,
  author = 	 {Wegbreit, B.},
  title = 	 {{Mechanical Program Analysis}},
  journal =	 {Communications of the ACM},
  year =	 {1975},
  volume =	 {18},
  number =	 {9},
  pages =	 {528--539}
}


@InProceedings{AKM79,
  author = 	 {Adachi, A. and Kasai, T. and Moriya, E.},
  title = 	 {{A Theoretical Study on the Time Analysis of Programs}},
  editor = 	 {Be\v{c}v\'{a}\v{r}, J.},
  number =	 {74},
  series =	 LNCS,
  pages =	 {201--207},
  booktitle =    {{Mathematical Foundations of Computer Science}},
  year =	 {1979},
  publisher =	 S-V,
  address =	 {Olomouc, Czechoslovakia, September 3--7},
  abstract =     "An algorithm is developed that constructs time bound
                 programs for a class of imperative programs called
                 simple loop programs.",
}

@TechReport{Kasa80,
  author = 	 {Kasai et al.},
  title = 	 {{An Automatic Time Analysis System}},
  institution =  {Kyoto University},
  year = 	 {1980},
  type =	 {Technical Report},
  number =	 {RIMS-335},
  month =	 {May},
  abstract =     "This paper describes a system, TIMER, that generates
               upper and lower bounds for the complexity of programs
               written in a small imperative C-like language. The
               system is semi-automatic in that it requires some user
               annotation, and the estimates can be improved by
               further annotation of the program. As such it is a
               powerful tool for developing efficient algorithms, but
               it is not intended to be a program analysis system.",
}

@Article{Ivie78,
  author = 	 {Ivie, J.},
  title = 	 {{Some MACSYMA Programs for Solving Recurrence Relations}},
  journal =	 {Transactions on Mathematical Software},
  year =	 {1978},
  volume =	 {4},
  number =	 {1},
  pages =	 {24--33},
  month =	 {March},
}

%$%node Misc related papers,  , Theoretical complexity analysis, Time analysis
%$%subsection Misc related papers

@Article{PuSc97,
  author = 	 {Puschner, P.P. and Schedl, A.V.},
  title = 	 {{Computing Maximum Task Execution Times --- A Graph-Based Approach}},
  journal = 	 {Journal of Real-Time Systems},
  year = 	 1997,
  volume =	 13,
  number =	 1,
  pages =	 {67--91},
  keywords =     {real-time systems, maximum execution time, timing analysis, 
    performance, safety analysis, software development, applications of 
    graph theory, integer linear programming},
  annote =	 {found bib-entry via BIDS},
  abstract =     "The knowledge of program execution times is crucial for the 
    development and the verification of real-time software. Therefore, 
    there is a need for methods and tools to predict the timing behavior 
    of pieces of program code and entire programs.
    This paper presents a novel method for the analysis of program 
    execution times. The computation of MAximum eXecution Times (MAXTs) 
    is mapped onto a graph-theoretical problem that is a generalization 
    of the computation of a maximum cost circulation in a directed graph.
    Programs are represented by T-graphs, timing graphs, which are 
    similar to flow graphs. These graphs reflect the structure and the 
    timing behavior of the code. Relative capacity constraints, a 
    generalization of capacity constraints that bound the flow in the 
    edges, express user-supplied information about infeasible paths. To 
    compute MAXTs, T-graphs are searched for those execution paths which 
    correspond to a maximum cost circulation. The search problem is 
    transformed into an integer linear programming problem. The solution 
    of the linear programming problem yields the MAXT.
    The special merits of the presented method are threefold: It uses a 
    concise notation to characterize the static structure of a program 
    and its possible execution paths. Furthermore, the notation allows 
    for a description of the feasible paths through the program code that
    characterizes the behavior of the code sufficiently to compute the 
    exact maximum execution time of the program - not just a bound 
    thereof. Finally, linear program solving does not only yield maximum 
    execution times, bur also produces detailed information about the 
    execution time and the number of executions of every single program 
    construct in the worst case. This knowledge is valuable for a more 
    comprehensive analysis of the timing of a program."
}

% ----------------------------------------------------------------------------

% ###########################################################################
%$%node Granularity Improvement Mechanisms, Personal and departmental references, Time analysis, Top
%$%section Granularity Improvement Mechanisms
% ###########################################################################

% xref{Lazy Task Creation} for work in the context of parallel Lisp (such
% as lazy task creation).

%$%menu
%* Profiling approaches::	
%* User annotation approaches::	 
%* Run-time approaches::	
%$%end menu

%$%node Profiling approaches, User annotation approaches, Granularity Improvement Mechanisms, Granularity Improvement Mechanisms
%$%subsection Profiling approaches
% -----------------------------------------------------------------------------

%$%cindex load bounding

@InProceedings{OsDa92,
  author = 	 {Ostheimer, G. and Davie, A.J.T},
  title = 	 {{Load Bounding for Implicit Parallelism}},
  OPTeditor =       {Kuchen, H. and Loogen, R.},
  booktitle =    {{IFL'92 --- International Workshop on the Parallel
		  Implementation of Functional Languages}},
  year =	 {1992},
  address =	 {RWTH Aachen, Germany},
  month =	 {September},
  url =          {ftp://ftp.dcs.st-and.ac.uk/pub/staple/load-bounding.ps.Z},
  abstractURL =  {http://www.dcs.st-and.ac.uk/\~{}staple/publications.html#load.bounding},
  abstract =     "It is well known that exploiting parallelism beyond the available machine
parallelism can lead to vastly increased resource requirements. Programs
which can run in linear space under a sequential execution regime may
require exponential space in the context of unrestricted parallelism.

In this paper we present an efficient adaptive software solution for the
problem of avoiding excessive parallelism when evaluating implicitly
parallel functional programs. We give an informal proof for the
effectiveness of our method and discuss its limitations. The solution and
the proof are based on a compilation scheme mapping lazy functional
programs onto message-passing parallel architectures. Our strategy is
completely distributed and can be expected to scale well to large machine
configurations."
}

@InProceedings{OsDa93,
  author = 	 {Ostheimer, G. and Davie, A.J.T},
  title = 	 {{Modelling Parallel Graph Reduction in the Pi-Calculus}},
  booktitle =    {{IFL'93 ---  International Workshop on the Parallel
                   Implementation of Functional Languages}},
  year =	 {1993},
  month =        {September}, 
  address = 	 {Nijmegen, The Netherlands},
  url =          {ftp://ftp.dcs.st-and.ac.uk/pub/staple/nijmegen.ps.Z},
  abstractURL =  {http://www.dcs.st-and.ac.uk/\~{}staple/publications.html#nijmegen},
  abstract =     "We have derived from a practical compilation scheme presented previously a
formal model for parallel graph reduction of the lambda-calculus in the
pi-calculus. This work solves the known open problem of modelling a
call-by-need reduction strategy in the pi-calculus. Models for call-by-name
and a parallel version of call-by-value have been given by Robin Milner. We
can apply different reduction strategies to different parts of a
lambda-term to combine the benefits of lazy semantics with those of safe
parallelism."
}


@InProceedings{Osth91,
  author = 	 {Ostheimer, G.},
  title = 	 {{Parallel Functional Computation on STAR:DUST}},
  booktitle =    {{IFL'91 ---   International Workshop on the Parallel
                   Implementation of Functional Languages}},
  address =      {Southampton, UK, June 5--7},
  year =         1991,
  pages =        {393--407},
  editor =       {Glaser, H. and Hartel, P.},
  publisher =    "Technical Report CSTR 91-07, University of
                        Southampton",
  url =          {ftp://ftp.dcs.st-and.ac.uk/pub/staple/stardust.ps.Z},
  abstractURL =  {http://www.dcs.st-and.ac.uk/\~{}staple/publications.html#stardust},
  abstract =     "STAR:DUST ('St. Andrews RISC: Dataflow Using Sequential Threads') is a
processor design optimized for efficient execution of sequential threads
while supporting plug-and-play construction of large multiprocessor
systems. Besides satisfying the major RISC criteria (small instruction set,
simple instruction format, load/store principle, pipelining), STAR:DUST
employs a dataflow approach to communication and parallelism. We describe
the architecture and propose a runtime model for parallel functional
computation based on STAR:DUST's dataflow primitives."
}

@PhdThesis{Ost93,
  author = 	 {Ostheimer, G},
  title = 	 {{Parallel Functional Programming for Message-Passing Multiprocessors}},
  school = 	 {University of St Andrews},
  year = 	 1993,
  month =	 {March},
  note =	 {Also: Research Report CS/93/8},
  url =          {ftp://ftp.dcs.st-and.ac.uk/pub/staple/par-fun.ps.Z},
  abstractURL =
                  {http://www.dcs.st-and.ac.uk/\~{}staple/publications.html#parallel.functional},
  abstract =     "We propose a framework for the evaluation of implicitly parallel functional
programs on message passing multiprocessors with special emphasis on the
issue of load bounding. The model is based on a new encoding of the
lambda-calculus in Milner's pi-calculus and combines lazy evaluation and
eager (parallel) evaluation in the same framework. The pi-calculus encoding
serves as the specification of a more concrete compilation scheme mapping a
simple functional language into a message passing, parallel program. We
show how and under which conditions we can guarantee successful load
bounding based on this compilation scheme. Finally we discuss the
architectural requirements for a machine to support our model efficiently
and we present a simple RISC-style processor architecture which meets those
criteria."
}

%$%cindex throttling

@InProceedings{RuSa87,
  author = 	 {Ruggiero, C.A. and Sargeant, J.},
  title = 	 {{Control of Parallelism in the Manchester Dataflow Machine}},
  booktitle =    {{FPCA'87 ---  Conference on Functional Programming Languages
		  and Computer Architecture}}, 
  address =      {Portland, Oregon, September 14--16},
  year =         {1987},
  pages =        {1--15},
  series =       LNCS,
  volume =       {274},
  publisher =    S-V,
  owner =        {pcl},
  descr =        {phdf-manchester},
}

	  
@TechReport{Sarg88,
  author = 	 {Sargeant, J.},
  title =        {{Load Balancing, Locality and Parallelism Control in
		    Fine-Grain Parallel Machines}},
  number =       {UMCS-86-11-5},
  type =         {Internal Report},
  institution =  {Department of Computer Science}, 		  
  address =      {University of Manchester},
  year =         {1987},
}		  
	  
@InProceedings{Sarg93,
  author = 	 {Sargeant, J.},
  title = 	 {{Improving Compilation of Implicit Parallel Programs by
                   Using Runtime Information}},
  OPTnumber =	 {ANL-91/34},
  pages =	 {129--148},
  booktitle =    {{Workshop on the Compilation of Symbolic Languages for
                  Parallel Computers}},
  year =	 {1993},
  address =	 {Argonne National Laboratory},
  month =	 {July},
}

%$%xref{Wats88b,LAGER,,man.bib,}

@PhdThesis{Brat94,
  author = 	 {Bratvold, T.A.},
  title = 	 {{Skeleton-Based Parallelisation of Functional Programs}},
  school = 	 {Department of Computing and Electrical Engineering},
  year = 	 {1994},
  address =	 {Heriot-Watt University, Edinburgh},
  month =	 {November},
  url =          {ftp://ftp.cee.hw.ac.uk/pub/funcprog/tab.phd.ps.Z}, 
  abstract =     "The computational power of parallel computers could be applied with great
benefit to many application areas, but the lack of adequate programming
tools complicates the development, verification and portability of parallel
software. Hence, parallel processing is not widely used in general purpose
computing.

Parallel programming would be simplified by tools providing abstractions
from details of parallelism without sacrificing performance. Skeletons are
templates of parallel operations that allow programs to be expressed with a
high level of abstraction while retaining parallelism in a structured form
suitable for efficient compilation. Functional programming languages
provide a natural framework for skeleton-based parallel programming as
skeletons can be elegantly expressed as higher order functions.

This thesis investigates an implicit skeleton-based approach to the
parallelisation of functional programs. It describes the design and
implementation of SkelML, a skeleton-based parallelising compiler for
Standard ML that exploits parallelism in certain higher order functions and
in function composition. The compiler supports a profiler based on
structural operational semantics, and uses performance models to predict
execution times of program components. This information is utilised in the
allocation of machine resources. Automatic transformation is supported for
performance optimisation.

The thesis considers the underlying philosophy of the approach, discusses
in detail the compiler internals, and presents a number of results obtained
by applying a prototype implementation of the compiler to example
programs. This work is novel in presenting a practical implementation
integrating all of the above components, demonstrating that they form a
promising framework for skeleton-based compilation. Hence it adds weight to
the argument that skeletons are well suited as a basis for parallel
programming."
}

@PhdThesis{Busv93,
  author = 	 {Busvine, D.J.},
  title = 	 {{Detecting Parallel Structures in Functional Programs}},
  school = 	 {Department of Computing and Electrical Engineering},
  year = 	 {1993},
  address =	 {Heriot-Watt University, Edinburgh},
  month =	 {November},
  url =          {ftp://ftp.cee.hw.ac.uk/pub/funcprog/djb.phd.tar.Z}, 
  abstract =     "It has been suggested that functional languages provide a convenient
vehicle for parallel programming due to the implicit parallelism present in
functional programs. Exploiting this parallelism has not however proved
easy. Most of the previous work in this area has concerned parallel graph
reduction but recently some researchers have argued that a more structured
approach is needed to fully exploit parallel architectures. This thesis
undertakes an exploration of some of these alternatives to graph
reduction. A system has been developed to produce parallel implementations
of Standard ML programs on a transputer network. Information about the
program to be implemented is captured from test executions of the
program. It is argued that profiling in this way provides a convenient and
powerful means of obtaining application related information. Two distinct
techniques are then examined for extracting useful parallelism. In the
first a static distribution of the program across available resources is
obtained using a heuristic approach. The idea behind the static approach is
to use profiling information to identify where the implicit paralellism in
the source program is worth exploiting. The second technique uses a
skeleton-based approach. Skeletons are structures of parallel programming
which are found to be useful in many diff erent contexts. It is shown that,
using profiling information, it is possible to identify automatically
opportunities for employing one particular skeleton, namely the processor
farm. A set of test programs is used to explore how effectively these
techniques detect and exploit program parallelism. The relative merits of
the static and skeleton approaches are compared with reference to these
results."
}

@InProceedings{MIK97,
  author = 	 {Michaelson, G.J. and Ireland, A. and King, P.J.B.},
  title = 	 {{Towards a Skeleton Based Parallelising Compiler for SML}},
  booktitle = 	 {{IFL'97 --- International Workshop on the Implementation of
                  Functional Languages}},
  year = 	 1997,
  address = 	 {September 10--12, University of St.\ Andrews, Scotland},
  annote = 	 {draft proceedings for now},
  url =  {ftp://ftp.cee.hw.ac.uk/pub/funcprog/mik.ifl97.ps.Z},
  abstract =     "A design for a skeleton based parallelising compiler for a pure functional
subset of Standard ML is presented. The compiler will use Structural
Operational Semantics based prototype instrumentation to determine
communication and processing costs at sites of higher order function
use. Useful parallelism will be identified through performance models for
equivalent algorithmic skeletons. Parallelism will be optimised through
proof plan driven program transformation. The compiler will generate C
utilising MPI for inter-process communication."
}

@InProceedings{SBMK98,
  author = 	 {Scaife, N. and Bristow, P. and Michaelson, G. and King, P.},
  title = 	 {{Engineering a Parallel Compiler for SML}},
  booktitle = 	 {{IFL'98 --- International Workshop on the Implementation of
                  Functional Languages}},
  year = 	 1998,
  OPTseries =	 LNCS,
  address = 	 {September 9--11, University College London, UK},
  publisher =	 {Draft proceedings},
  note = 	 {To appear in Springer Verlag LNCS},
}

@InProceedings{HMK98,
  author = 	 {Hamdan, M. and Michaelson, G. and King, P.},
  title = 	 {{A Scheme for Nesting Algorithmic Skeletons}},
  booktitle = 	 {{IFL'98 --- International Workshop on the Implementation of
                  Functional Languages}},
  year = 	 1998,
  OPTseries =	 LNCS,
  address = 	 {September 9--11, University College London, UK},
  publisher =	 {Draft proceedings},
  note = 	 {To appear in Springer Verlag LNCS},
}

@Article{MSBK01,
  author = 	 {Michaelson, G. and Scaife, N. and Bristow, P. and King, P.},
  title = 	 {{Nested Algorithmic Skeletons from Higher Order Functions}},
  journal = 	 {Parallel Algorithms and Applications},
  year = 	 2001,
  volume =	 16,
  pages =	 {181--206},
  annote =	 {the main summary paper on Greg's PMLS project},
  url =          {ftp://ftp.cee.hw.ac.uk/pub/funcprogmsbk.paa00.ps.Z},
  keywords =     {higher order functions; skeletons; parallelising compilation},
  abstract =     "Algorithmic  skeletons   provide  a  promising  basis   for  the  automatic
utilisation of  parallelism at sites  of higher-order function  use through
static program analysis. However, decisions about whether or not to realise
particular higher-order  function instances as  skeletons must be  based on
information about  available processing  resources, and such  resources may
change subsequent to program analysis.

In  principle, nested  higher-order  functions may  be  realised as  nested
skeletons.  However,  where  higher-order  function arguments  result  from
partially applied functions, free-variable  bindings must be identified and
communicated through  the corresponding  skeleton hierarchy to  where those
arguments are actually applied.

Here, a  skeleton based parallelising  compiler from Standard ML  to native
code  is presented.  Hybrid skeletons,  which can  change from  parallel to
serial  evaluation at  run-time, are  considered and  mechanisms  for their
nesting are discussed. Compilation  stages are illustrated through a simple
nested higher-order  function based  algorithm for multiplying  matrices of
arbitrary length integers and performance figures for compiled code running
on a F ujitsu AP3000 are discussed."
}

@TechReport{PaCl94,
  author = 	 {Parrot, D. and Clack, C.},
  title = 	 {{Advice from Bumble-Bees on Scheduling Parallel
                   Functional Tasks}},
  institution =  {Department of Computer Science},
  year = 	 1994,
  address =	 {University College London, UK},
  month =	 {July},
  annote =	 {use profiling approach; don't believe in compile-time estimates}
}

@Article{BuRa94,
  author = 	 {Burton, F.W. and {Rayward Smith}, V.J.},
  title = 	 {{Worst Case Scheduling for Parallel Functional Programming}},
  journal = 	 JFP,
  year = 	 1994,
  volume =	 4,
  number =	 1,
  pages =	 {65--75},
  month =	 {January}
}

@InProceedings{Sark89a,
  author = 	 {Sarkar, V.},
  title = 	 {{Determining Average Program Execution Times and their Variance}},
  booktitle =    {{PLDI'89 --- Programming Languages Design and
                   Implementation}}, 
  series =       {SIGPLAN Notices},
  year =	 {1989},
  month =	 {June},
  publisher =    {ACM Press},
  OPTpages = 	 {298--312},
  url =  {http://www.csg.lcs.mit.edu:8001/Users/vivek/ps/Sark89.ps},
  abstract =     "This paper presents a general framework for determining average program
execution times and their variance, based on the program's interval
structure and control dependence graph. Average execution times and
variance values are computed using frequency information from an optimized
counter-based execution profile of the program."
}

@InProceedings{Sark97,
  author = 	 {Sarkar, V.},
  title = 	 {{Analysis and Optimization of Explicitly Parallel Programs using the
     Parallel Program Graph Representation}},
  booktitle = 	 {{Workshop on Languages and Compilers for Parallel Computing}},
  year =	 1997,
  address =	 {Minneapolis, MN, U.S.A.},
  month =	 aug,
  note =	 {Also: CSG-Memo-402},
  abstractURL =  {http://www.csg.lcs.mit.edu:8001/cgi-bin/abstract.pl?/home/www/ftp/pub/papers/csgmemo/memo-402.abs},
  url =          {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-402.ps.Z},
  abstract =     "As more and more programs are written in explicitly parallel
     programming languages, it becomes essential to extend the scope of
     sequential analysis and optimization techniques to explicitly
     parallel programs. Since the definition of a program dependence
     graph (PDG) is strongly tied to its underlying sequential program,
     the PDG is an inadequate intermediate representation for analysis
     and optimization of explicitly parallel programs. In this paper, we
     propose the use of Parallel Program Graphs (PPGs) as a general
     parallel program representation for analysis and optimization of
     explicitly parallel programs. PPGs are comprised of parallel control
     flow edges and synchronization edges, and can represent a broad
     class of deterministic parallel programs. We highlight the main
     differences between PPGs and PDGs and show how PPGs are strictly
     more general than PDGs. We also present a solution to the reaching
     definitions analysis problem for PPGs to illustrate how PPGs can be
     used to perform analysis and optimization of explicitly parallel
     programs."
}

% -----------------------------------------------------------------------------
%$%node User annotation approaches, Run-time approaches, Profiling approaches, Granularity Improvement Mechanisms
%$%subsection User annotation approaches
% -----------------------------------------------------------------------------

@article{Huda86,
  author =       {Hudak, P.},
  title =        {{Para-Functional Programming}},
  journal =      i3ec,
  month =        {August},
  year =         {1986},
  volume =       {19},
  number =       {8},
  pages =        {60--70},
  owner =        {pcl},
  descr =        {plfun},
}


@InCollection{Huda91,
  author = 	 {Hudak, P.},
  title = 	 {{Para-Functional Programming in Haskell}},
  publisher =	 {ACM Press, New York and Addison-Wesley, Reading},
  year =	 {1991},
  editor =	 {Szymanski, B.K.},
  booktitle =    {{Parallel Functional Languages and Compilers}},
}

@book{Szym91,
  editor =     {Szymanski, B.K.},
  title =      {{Parallel Functional Languages and Compilers}},
  publisher =  {ACM Press, New York and Addison-Wesley, Reading},
  year =       {1991},
  series =     {Frontier Series},
  owner =      {risc},
  descr =      {plfun},
}

% -----------------------------------------------------------------------------
%$%node Run-time approaches,  , User annotation approaches, Granularity Improvement Mechanisms
%$%subsection Run-time approaches
% -----------------------------------------------------------------------------

%$%cindex Dutch Parallel Machine

@InProceedings{Vree87,
  author = 	 {Vree, W.G.},
  title = 	 {{The Grain Size of Parallel Computations in a Functional
                   Program}},
  booktitle = 	 {{Parallel processing and Applications}},
  editor =	 {Chiricozzi, E. and {d'Amico}, A.},
  year =	 1987,
  publisher =	 {Elsevier Science Publishers},
  address =	 {L'Aquila, Italy},
  month =	 {September},
  pages =	 {363--370},
  annote =	 {Reference entry in: http://www.fwi.uva.nl/research/arch/publications/all.html}
}

@InProceedings{Vree90,
  author = 	 {Vree, W.G},
  title = 	 {{Implementation of Parallel Graph Reduction by
                   Explicit Annotation and Program Transformation}},
  booktitle = 	 {{Mathematical Foundations of Computer Science}},
  volume =	 452,
  series =	 LNCS,
  year =	 1990,
  publisher =	 S-V,
  address =	 {Bansk· Bystrica, Czechoslovakia},
  month =	 {August},
  pages =	 {135--151}
}

@Article{HoVr92,
  author = 	 {Hofman, R.F.H. and Vree, W.G.},
  title = 	 {{Distributed Hierarchical Scheduling with Explicit Grain
                  Size Control}}, 
  journal = 	 {Future Generation Computer Systems},
  year = 	 1992,
  volume =	 8,
  month =	 {July},
  pages =	 {111--119},
}

@InProceedings{HoVr92a,
  author = 	 {Hofman, R.F.H. and  Vree, W.G.},
  title = 	 {{Evaluation of Distributed Hierarchical Scheduling with
                  Explicit Grain Size Control}}, 
  booktitle = 	 {SHPCC'92 --- Scalable High Performance Computing},
  year =	 1992,
  publisher =	 {IEEE Computer Society Press},
  address =	 {Los Alamitos, California},
  month =	 {April},
  pages =	 {186--189},
  annote =	 {Reference in: http://www.fwi.uva.nl/research/arch/publications/all.html}
}

@InProceedings{HLV92,
  author = 	 {Hofman, R.F.H. and Langendoen, K.G. and  Vree, W.G.},
  title = 	 {{Scheduling Consequences of Keeping Parents at Home}},
  booktitle = 	 {{ICPADS'92 --- Parallel and Distributed Systems}},
  year =	 1992,
  address =	 {National Tsing Hwa Univ., Taiwan},
  month =	 {December},
  pages =	 {580--588},
  annote =	 {Reference in: http://www.fwi.uva.nl/research/arch/publications/all.html}
}

@InProceedings{HLV91,
  author = 	 {Hofman, R.F.H. and Langendoen, K.G. and  Vree, W.G.},
  title = 	 {{Scheduling Performance under the Influence of
                   Optimisations for Shared Memory Graph Reduction}}, 
  booktitle = 	 {4th Computer systems},
  year =	 1991,
  address =	 {Amsterdam, The Netherlands},
  month =	 {October},
  pages =	 {1--15},
}

@InProceedings{LaVr91,
  author = 	 {Langendoen, K.G. and  Vree, W.G.},
  title = 	 {{FRATS: a Parallel Reduction Strategy for Shared Memory}},
  booktitle = 	 {{Programming Language Implementation and Logic Programming}},
  number =	 528,
  series =	 LNCS,
  year =	 1991,
  publisher =	 S-V,
  address =	 {Passau, Germany},
  month =	 {August},
  pages =	 {99-110},
}

@InProceedings{LMV92,
  author = 	 {Langendoen, K.G. and  Muller, H.L. and Vree, W.G.},
  title = 	 {{Memory Management for Parallel Tasks in Shared Memory}},
  booktitle = 	 {IWMM'92 --- International Workshop on Memory Management},
  number =	 637,
  series =	 LNCS,
  year =	 1992,
  publisher =	 S-V,
  address =	 {St.\ Malo, France},
  month =	 {September},
  pages =	 {165--178},
}

@techreport{MoBe94,
  author =   {Mountjoy, J. and Beemster, M.},
  title =    {{Functional Languages and Very Fine Grained Parallelism:
                  Initial Results}},
  institution = {Department of Computer Systems},
  address =     {University of Amsterdam},
  type =        {Technical report},
  number =      {CS-94-24},
  month =       dec,
  year =        {1994},
  annote =      {tries to expose instruction level parallelism out of a
                 lazy functional language; focus on symbolic computation
                  applications (they generate irregular parallelism)},
  url =         {ftp://ftp.fwi.uva.nl/pub/computer-systems/functional/reports/fine\_grained\_parallelism.ps.Z},
  abstract =    "A functional language compiler can be used as a powerful tool  in the
scheduling of programs  for  hardware capable  of  fine grained instruction
level  parallelism. There  have been many  attempts  to effectively utilise
functional languages   as     a means  of  easily    programming   parallel
hardware. These attempts  have generally concentrated on  parallelising the
underlying reduction strategies to achieve  this goal. This paper  explores
an   alternative use of  a functional   language  compiler; as  a  tool for
producing annotated code  which can be  effectively scheduled for the class
of  transport triggered architectures.  These  architectures are capable of
much fine grained instruction level parallelism, and it  is the task of the
compiler to schedule  code to utilise  this power, in  a  manner similar to
that of a  compiler for a VLIW  architecture. We believe that  a functional
language  compiler  has  more freedom   in  the  generation   of code  than
imperative language compilers, due to  the level of abstraction provided by
the language. This  enables the compiler to have  a more intimate knowledge
of data and  instruction dependencies, and this  will hopefully lead to  an
improvement of  the current scheduling  techniques, and to  a better use of
the hardware."
}

@Article{Vree89,
  author = 	 {Vree, W.G.},
  title = 	 {{Experiments with Coarse Grain Parallel Graph Reduction}},
  journal = 	 {Future Generation Computer Systems},
  year = 	 1989,
  volume =	 4,
  month =	 {March},
  pages =	 {299--306},
}

@article{VrHa92,
  author =   {Vree, W.G. and Harte, P.H.},
  title =    {{Communication Lifting: Fixed Point Computation for Parallelism}},
  journal =  JFP,
  volume =   {5},
  number =   {4},
  pages =    {549--581},
  month =    oct,
  year =     {1995},
  url =  {ftp://ftp.fwi.uva.nl/pub/computer-systems/functional/reports/JFP\_communication\_lifting.ps.Z},
  abstract = "Communication lifting is a program transformation that  can be applied to a
synchronous process network to  restructure the network. This restructuring
in theory improves sequential  and parallel performance. The transformation
has been formally specified and proved  correct and it has been implemented
as an automatic program transformation tool. This  tool has been applied to
a  small     set   of    programs   consisting  of      synchronous process
networks. Measurements indicate  performance  gains in  practice both  with
sequential and parallel evaluation.  Communication lifting is  a worthwhile
optimisation to be included in a compiler for a lazy functional language."
}

@Article{BVH*87,
  author = 	 {Barendregt, H.P. and  {van Eekelen}, M.C.J.D. and
                  Hartel, P.H. and  Hertzberger, L.O. and Plasmeijer, M.J. and 
                  Vree, W.G.},
  title = 	 {{The Dutch Parallel Reduction Machine Project}},
  journal = 	 {Future Generation Computer Systems},
  year = 	 1987,
  volume =	 3,
  month =	 {December},
  pages =	 {261--270},
  annote =	 {Describes the first Dutch project on functional languages, a
     collaboration of the University of Amsterdam (hardware), Utrecht (theory)
     and Nijmegen (software).},
  url =          {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/barh87-PRMprojekt.ps.gz},
  abstractURL =  {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/barh87-PRMprojekt.abs},
  abstract =     "In November 1984 three research groups at the universities of Amsterdam, Nijmegen
and Utrecht started a cooperative project sponsored by the Dutch Ministry of
Science and Education (Science Council). The first phase lasting until the end of
1987 is a pilot study and has as aim to answer the following question. Is it
possible and realistic to construct an efficient parallel reduction machine? The
present paper gives an outline of the problems concerning parallel reduction
machines and of our research towards their solutions.",
}

@Article{BHH*93,
  author = 	 {Beemster, M. and Hartel, P.H. and Hertzberger, L.O. and
                  Hofman, R.F.H. and Langendoen, K.G. and Li, L.L. and
                  Milikowski, R. and Vree, W.G. and
                  Barendregt, H.P. and Mulder, J.C.},
  title = 	 {{Experience with a Clustered Parallel Reduction Machine}},
  journal = 	 {Future Generation Computer Systems},
  year = 	 1993,
}

@Article{HHL*95,
  author = 	 {Hartel, P.H. and Hofman, R.F.H. and Langendoen, K.G. and
                  Muller, H.L. and Vree, W.G. and Hertzberger, L.O.},
  title = 	 {{A Toolkit for Parallel Functional Programming}},
  journal = 	 {Concurrency --- Practice and Experience},
  year = 	 1995,
  volume =	 7,
  number =       8,
  month =	 dec,
  pages =	 {765--793},
  keywords =     {Parallel functional programming, simulation, real execution, validation, case study},
  url =          {ftp://ftp.fwi.uva.nl/pub/computer-systems/functional/reports/CPE\_toolkit.ps.Z},
 abstract =      "Our toolkit for the design and implementation of parallel functional
programs supports the stepwise development of parallel programs from a high
level sequential specification to an optimised parallel implementation. The
toolkit is used as follows: 

  1. The algorithm to be implemented is specified in a functional
     language. The program is debugged and tested using an interpreter.

  2. The program is compiled for a sequential machine. Its performance is
     analysed an improved.

  3. Annotation driven transformations are applied to the program to
     indicate parallel tasks. Simulations at task level, basic block level
     and bus transaction level make it possible to analyse the parallel
     performance of the program at three levels of detail.

  4. When the performance is optimised using the simulators, the program is
     executed on a genuine parallel machine. 

Several programs have been developed with the toolkit. A program that
simulates tidal row in an estuary of the North sea is presented as a case
study to demonstrate the merits of the toolkit when developing complex
parallel programs. The toolkit not only supports the design of parallel
applications; it also allows the study of important concepts in parallel
computer architecture. These include the behaviour of cached memory
systems, bus protocols, scheduling algorithms and memory management
algorithms."
}

@Article{Beem94,
  author = 	 {Beemster, M.},
  title = 	 {{Strictness Optimization for Graph Reduction
                   Machines (why id might not be strict)}},
  journal = 	 toplas,
  year = 	 1994,
  volume =	 16,
  month =	 {September},
  pages =	 {1449--1466},
}

@Article{HaVe88,
  author = 	 {Hartel, P.H. and Veen, A.H.},
  title = 	 {{Statistics on Graph Reduction of SASL Programs}},
  journal = 	 {Software --- Practice and Experience},
  year = 	 1988,
  volume =	 18,
  month =	 {March},
  pages =	 {239--253},
}

@InProceedings{HeVr89,
  author = 	 {Hertzberger, L.O. and Vree, W.G.},
  title = 	 {{A Coarse Grain Parallel Architecture for Functional Languages}},
  booktitle = 	 {PARLE'89 --- Parallel Architectures and Languages Europe},
  number = 	 {365/366},
  series = 	 LNCS,
  year = 	 {1989},
  publisher =    S-V,
  address = 	 {Eindhoven, The Netherlands},
  month = 	 {June},
  pages = 	 {269--285},
  annote =	 {Reference in: http://www.fwi.uva.nl/research/arch/publications/all.html}
}

@Article{Hart91,
  author = 	 {Hartel, P.H.},
  title = 	 {{Performance of Lazy Combinator Graph Reduction}},
  journal = 	 {Software --- Practice and Experience},
  year = 	 1991,
  volume =	 21,
  month =	 {March},
  pages =	 {299--329},
}

% most of the above references come from:
%    annote =	 {Reference in: http://www.fwi.uva.nl/research/arch/publications/all.html}

@InProceedings{HSH*91,
  author = 	 {Hoekstra, A.G. and Sloot, P.M.A. and Haan, M.J. and
                  Hertzberger, L.O.},
  title = 	 {{Time Complexity Analysis for Distributed Memory
                   Computers: Implementation of a Parallel Conjugate Gradient Method}},
  booktitle = 	 {Computing Science in the Netherlands},
  year =	 1991,
  address =	 {Amsterdam, The Netherlands},
  pages =	 {249--266}
}

@InProceedings{MiVr91,
  author = 	 {Milikowski, R. and Vree, W.G.},
  title = 	 {{The G-line, a Distributed Processor for Graph Reduction}},
  booktitle = 	 {{PARLE'91 --- Parallel Architectures and Languages Europe}},
  number =	 {505/506},
  series =	 LNCS,
  year =	 1991,
  publisher =	 S-V,
  address =	 {Veldhoven, The Netherlands},
  month =	 {June},
  pages =	 {119--136}
}

% Check publications on simulation
% No tech reports in here so far

% PhDs

@PhdThesis{Hart88,
  author = 	 {Hartel, P.H.},
  title = 	 {{Performance Analysis of Storage Management in Combinator
                  Graph Reduction}} ,
  school = 	 {Department of Computer Systems},
  address = 	 {University of Amsterdam},
  year = 	 1988,
  month =	 {October},
}

@PhdThesis{Vree89a,
  author = 	 {Vree, W.G.},
  title = 	 {{Design Considerations for a Parallel Reduction Machine}},
  school = 	 {Department of Computer Systems},
  address = 	 {University of Amsterdam},
  year = 	 1989,
  month =	 {December},
}

@PhdThesis{Lang93,
  author = 	 {Langendoen, K.G.},
  title = 	 {{Graph Reduction on Shared-Memory Multiprocessors}},
  school = 	 {Department of Computer Systems},
  address = 	 {University of Amsterdam},
  year = 	 1993,
  month =	 {April},
}

@PhdThesis{Hofm94,
  author = 	 {Hofman, R.F.H.},
  title = 	 {{Scheduling and Grain Size Control}},
  school = 	 {Department of Computer Systems},
  address = 	 {University of Amsterdam},
  year = 	 {1994},
  month =        {May},
  url =          {ftp://ftp.fwi.uva.nl/pub/computer-systems/functional/reports/thesis\_Hofman.ps.Z},
  abstract =     "Traditionally,  multiprocessor   scheduling   is   considered a   difficult
problem. Its basic  question is: what  is  an assignment of  the tasks that
make up a parallel application to  the processors of a given multiprocessor
to achieve minimal  completion time?  This  problem is intractable, in  the
sense   that it  is   NP-Complete [Garey79], except   if the  application's
structure  is extremely simple  (for instance, the dependencies between the
tasks  constitute  a  tree  and    each task   takes equally long).    This
intractability   is  the cause  of the  communis   opinio on multiprocessor
scheduling. In  practical situations, heuristics  must be used to achieve a
``reasonable'' completion time. It has been known for more than two decades
[Graham69] that a large class of  heuristics, the so-called list scheduling
strategies, have a  performance that is not  far from optimal. However, the
list  scheduling strategies only  work with  a  multiprocessor in which the
cost  of  assigning a task to   any processor is  zero.  Much less is known
generally about heuristics for machines  where allocation to one  processor
is  more expensive  than  to  another processor,   like distributed  memory
multiprocessors; other trade-offs must be considered, for instance moving a
task to  a less-loaded processor in such  a machine yields  transport costs
but may lead to a better equilibrium in processor load.

The main topic  of  this thesis  is development and   practical performance
analysis  of a  number of scheduling   heuristics  for multiprocessors with
unequal  allocation costs.  This   research took  place   within the  Dutch
Parallel  Reduction Machine   Project,   whose goal is  efficient  parallel
implementation  of  lazy    functional   languages   [Hartel88a,    Vree89,
Langendoen93, Barendregt92]. A  number of  multiprocessor architectures was
designed in the  course  of the  Project. The older   machine, APERM, is  a
distributed  memory machine where dual  ported memories  (DPMs) are used as
interconnection medium  between processors, so tasks  that are allocated to
another processor must  be transported through the  DPM network. The second
machine, HyperM, is a layered architecture. At the outer layer, it consists
of  distributed   memory nodes   that  are  interconnected by  a high-speed
network;  at  the  inner layer,  the  nodes   of HyperM  are  shared memory
multiprocessors  themselves. At first  sight some list scheduling algorithm
would suffice to achieve ``reasonable''  scheduling performance within  the
nodes; however, issues  in the runtime  support system, i.c. in the  memory
management scheme, and the use of caches  cause unequal assignment costs to
different processors. The  consequences  for the performance of  scheduling
strategies within     the nodes  are    investigated. Scalability   of  the
distributed memory machine  HyperM  is a major  design  issue, and this  is
reflected  in the  algorithms  we  devised  and studied for   scheduling at
HyperM's outer layer, where tasks may be allocated to another node.

The  granularity  of parallel applications  is  generally recognised  as an
issue of primary importance. The grain of the computation should not be too
small,  otherwise the ratio of useful  work  to overhead is disturbed; this
might even lead to slow-down instead of speed-up.  Too large grains lead to
a loss of   parallelism, however. The optimal  grain  size  depends on  the
architecture,     because     the     overhead costs    depend     on   the
architecture. Nowadays,  it  becomes more and   more usual to  require from
programmers in functional  languages to control   the granularity of  their
applications. This is  also the  case in  the  programming paradigm of  the
Dutch  Parallel  Reduction Machine project. The   tasks in the applications
must be annotated by the  programmer with a  grain size descriptor. The new
idea behind a  number of heuristics studied  in this thesis  is to use this
grain size descriptor for  load balancing purposes,  too. By definition, it
correlates with the execution time  of spawned  tasks. This correlation  is
sampled by  the system and used  in the scheduling  heuristics to achieve a
good trade-off between overhead costs and load imbalance."
}

% Well, that's actually a hybrid (compile-time/run-time)

% ###########################################################################

%$%node Personal and departmental references, Parallel Lisp, Granularity Improvement Mechanisms, Top
%$%section Personal and departmental references


% ToDo: This section should not exist. Incorporate entries into the proper 
% structure of this file.

%$%menu
%* All references I have about Goldberg::  
%* From DIKU's WWW server::	
%$%end menu

%$%node All references I have about Goldberg, From DIKU's WWW server, Personal and departmental references, Personal and departmental references
%$%subsection All references I have about Goldberg

%$%cindex Alfalfa

% Sort of ad-hoc analysis, but probably the std work

@PhdThesis{Gold88,
  author = 	 {Goldberg, B.},
  title = 	 {{Multiprocessor Execution of Functional Programs}},
  school = 	 {Department of Computer Science},
  year = 	 {1988},
  address =	 {Yale University},
  month =	 {April},
  number =       {{YALEU/DCS/RR-618}},
}

@inproceedings{GoHu86,
  author =       {Goldberg, B. and Hudak, P.},
  title =        {{Alfalfa: Distributed Graph Reduction on a Hypercube 
                   Multiprocessor}},
  booktitle =    {{Workshop on Graph Reduction and Techniques}},
  year =         {1986},
  pages =        {94--113},
  series =       LNCS,
  volume =       {279},
  publisher =    S-V,
  owner =        {pcl},
  descr =        {plfun,phred},
}

@article{HuGo85a,
  owner =        {pcl},
  author =       {Hudak, P. and Goldberg, B.},
  title =        {{Distributed Execution of Functional Programs Using Serial
                   Combinators}},
  year =         {1985},
  journal =      i3ec,
  volume =       {34},
  number =       {10},
  month =        {October},
  pages =        {881--891},
  descr =        {plfun},
}

@InProceedings{HuGo85b
    ,key={hudak}
    ,author={Hudak, P. and Goldberg, B.}
    ,title={Serial Combinators: Optimal Grains of Parallelism}
    ,booktitle={FPCA'85 --- Conference on Functional Programming Languages and Computer Architecture}
    ,month=Sep
    ,year=1985
    ,publisher=S-V
    ,series = LNCS
    ,number =  201
    ,pages={382-388}
    }

@InProceedings{Gold88b
    ,Key={goldberg}
    ,Author={Goldberg, B.}
    ,Title={Buckwheat: Graph Reduction on a Shared Memory Multiprocessor}
    ,Booktitle={LFP'88 --- Lisp and Functional Programming}
    ,Organization={ACM SIGPLAN/SIGACT/SIGART}
    ,Year=1988
    ,Month=Aug
    ,Address={Salt Lake City, Utah}
    ,Pages={40-51}
    }

@Article{Gold88c,
  author = 	 {Goldberg, B.},
  title = 	 {{Multiprocessor Execution of Functional Programs}},
  journal = 	 {International Journal of Parallel Programming},
  year = 	 1988,
  volume =	 17,
  number =	 5,
  month =	 {October},
  pages =	 {425--473},
}

@Article{Mahe95,
  author = 	 {Maheshwari, P.},
  title = 	 {{Partitioning and Scheduling of Parallel Functional
                   Programs for Larger Grain Execution}}, 
  journal = 	 JPDC,
  year = 	 {1995},
  volume = 	 {26},
  number = 	 {2},
  pages = 	 {151--165},
  abstract =     "This paper discusses how to exploit parallelism efficiently by 
    improving the granularity of functional programs on a multiprocessor.
    The challenge is to partition a functional program (or a process) 
    into appropriately sized subprocesses to make sure that the 
    computation time of the local subprocess is at least greater than the
    communication overheads involved in sending other subprocesses for 
    remote evaluation. Asymptotic complexity analyses of a function, to 
    estimate the computation time and also the communication involved in 
    sending the arguments and receiving the results from the remote 
    processor, are found to be quite useful. It is shown how some 
    parallel programs can be run more efficiently with prior information 
    on time complexities (in big-O notation) and relative time 
    complexities of its subexpressions with the help of analytical 
    reasoning and some practical examples on the larger grain distributed
    multiprocessor machine LAGER. Ordered scheduling of the processes, 
    determined by the priorities based on the relative time complexities,
    shows further improvement over run-time dynamic load balancing 
    methods as well as better utilization of resources."
}

@InProceedings{Mahe91,
  author = 	 {Maheshwari, P.},
  title = 	 {{Partitioning and Scheduling of Parallel Functional
                   Programs Using Complexity Information}},
  booktitle = 	 {International Conference on Computing and Information},
  number =	 497,
  series =	 LNCS,
  year =	 1991,
  publisher =	 S-V,
  address =	 {Ottawa, Canada}
}

@InProceedings{ChGo92,
  author = 	 {Chuang, T.-R. and Goldberg, B.},
  title = 	 {{Backwards Analysis for Higher-Order Functions Using
                   Inverse Images}},
  pages = 	 {109--118},
  booktitle =    {{LFP'92 --- Conference on Lisp and Functional Programming}},
  year =	 {1992},
  publisher =	 {ACM Press},
  address =	 {San Francisco, California, June 22--24},
}

@TechReport{Huda84,
  author = 	 {Hudak, P.},
  title = 	 {{ALFL Reference Manual and Programmer's Guide}},
  institution =  {Department of Computer Science},
  year = 	 {1984},
  type =         {Research Report}, 
  number =	 {{YALEU-DCS-RR-322}},
  address =	 {Yale University},
  month =	 {October}
}

@InProceedings{MiHu95,
  author = 	 {Mirani, R. and Hudak, P.},
  title = 	 {{First-Class Schedules and Virtual Maps}},
  booktitle =    {{FPCA'95 ---  Conference on Functional Programming
		  Languages and Computer Architecture}}, 
  year =	 {1995},
  publisher =	 {ACM Press},
  address =      {La Jolla, California, June 26--28},
  annote =	 {Rather close to evaluation strategies}
}

%$%node From DIKU's WWW server,  , All references I have about Goldberg, Personal and departmental references
%$%subsection From DIKU's WWW server

@InProceedings{GoSe91,
  author =       {Gomard, C. K.  and Sestoft, P.},
  semno =        "D-150",
  title =        {{Evaluation Order Analysis for Lazy Data Structures}},
  booktitle =    {Glasgow Functional Programming Workshop},
  year =         {1991},
  editor =       {Heldal and Holst and Wadler},
  pages =        {112--127},
  publisher =    S-V,

}

% From DIKU's WWW server (see diku-1991-bib.html)}

% key was: somefunnynumber:93,
@InProceedings{Rose93,
  author =       {Rosendahl, M.},
  semno =        {D-180},
  title =        {{Higher-Order Chaotic Iteration Sequences}},
  booktitle =    {PLILP'93 --- Symposium on Programming Language Implementation and Logic Programming},
  address =      {Tallinn, Estonia},
  year =         {1993},
  publisher =    S-V,
  number =       {714},
  series =       LNCS,
  pages =        {332--345},
  summary =      "Chaotic iteration sequences is a method for
                 approximating fixpoints of monotonic functions proposed
                 by Patrick and Radhia Cousot. It may be used in
                 specialisation algorithms for Prolog programs and in
                 abstract interpretation when only parts of a fixpoint
                 result is needed to perform program optimisations. In
                 the first part of this paper we reexamine the
                 definition of chaotic iteration sequences and show how
                 a number of other methods to compute fixpoints may be
                 formulated in this framework. In the second part we
                 extend the technique to higher-order functions.",
  url =          {ftp://ftp.diku.dk/diku/semantics/papers/D-180.dvi.Z}, 
}

% From DIKU's WWW server (see diku-1994-bib.html)

@MastersThesis{Rose86,
  author = 	 {Rosendahl, M.},
  title = 	 {{Automatic Program Analysis}},
  school = 	 {DIKU},
  year = 	 {1986},
  address =	 {University of Copenhagen}
}

@InProceedings{JonesRosendahlALP94,
  author =       {Jones, N.D. and Rosendahl, M.},
  semno =        {D-204},
  title =        {{Higher-Order Minimal Function Graphs}},
  booktitle =    {ALP'94},
  year =         {1994},
  series =       LNCS,
  publisher =    S-V,
  OPTaddress =   "",
  month =        sep,
  note =         "11 pages",
  keywords =     "Higher-order functional languages, abstract
                 interpretation, minimal function graphs, closure
                 analysis.",
  summary =      "We present a minimal function graph semantics for a
                 higher-order functional language with applicative
                 evaluation order. The semantics captures the
                 intermediate calls performed during the evaluation of a
                 program. This information may be used in abstract
                 interpretation as a basis for proving the soundness of
                 program analyses. An example of this is the ``closure
                 analysis'' of partial evaluation.",
  url =          {ftp://ftp.diku.dk/diku/semantics/papers/D-204.dvi.Z}, 
}

@article{JoRo97,
  author=    {Jones, N.D. and Rosendahl, M.},
  title=     {{Higher-Order Minimal Function Graphs}},
  journal=   JFLP,
  volume=    {1997},
  number=    {2},
  publisher= MIT,
  month=     {February},
  year=      {1997},
  url =      {http://www.cs.tu-berlin.de/journal/jflp/articles/1997/A97-02/JFLP-A97-02.ps.gz},
  abstractURL =      {http://www.cs.tu-berlin.de/journal/jflp/articles/1997/A97-02/A97-02.html},
  abstract =         "We present a minimal function graph semantics for a
     higher-order functional language with applicative evaluation
     order. The approach extends previous results on minimal function
     graphs to higher-order functions. The semantics captures the
     intermediate calls performed during the evaluation of a
     program. This information may be used in abstract interpretation as
     a basis for proving the soundness of program analyses. An example of
     this is the ``closure analysis'' of partial evaluation."
}

% From DIKU's WWW server (see diku-1994-bib.html)

@InCollection{JoNi94,
  semno =        "D-58",
  author =       {Jones, N.D. and Nielson, F.},
  title =        {{Abstract Interpretation: a Semantics-Based Tool for
                 Program Analysis}},
  booktitle =    {Handbook of Logic in Computer Science},
  publisher =    {Oxford University Press},
  year =         {1994},
  annote =         "121 pages. To appear",
  summary =      "This is a broad overview of Abstract Interpretation,
                 to be a large chapter (around 100 pages) in the
                 above-mentioned handbook. It consists of three main
                 parts: an Introduction with motivation and Descriptions
                 of the main methods used in the field; a mathematical
                 development of the logical relations approach with
                 several applications; and short descriptions of a broad
                 spectrum of Semantics-Based Program Analyses.",
  keywords =     "abstract interpretation, flow analysis, static program
                 analysis",
  url =          {ftp://ftp.diku.dk/diku/semantics/papers/D-58.ps.Z}, 
}

% From DIKU's WWW server (see diku-1994-bib.html)

@MastersThesis{Jens90,
  author =       {Jensen, T.P.},
  title =        {{Context Analysis of Functional Programs}},
  school =       DIKU,
  year =         {1990},
  semno =        "D-45",
  OPTaddress =   "",
  month =        Jan,
  note =         "65 pages",
  summary =      "Contexts are introduced as a means for describing the
                 use of a data structure. A backwards analysis for
                 determining contexts is developed and used to optimise
                 the use of storage in functional programs.",
  keywords =     "backwards analysis, compile-time garbage collection",
}


% -----------------------------------------------------------------------------
%$%node Parallel Lisp, Theoretical work, Personal and departmental references, Top
%$%section Parallel Lisp
% -----------------------------------------------------------------------------

% Some of these references also fall under the topic Granularity
% improvement mechanisms

%$%menu
%* Lazy Task Creation::		
%* General Runtime-System Stuff::  
%* Visualisation::		
%$%end menu

%$%node Lazy Task Creation, General Runtime-System Stuff, Parallel Lisp, Parallel Lisp
%$%subsection Lazy Task Creation
% -----------------------------------------------------------------------------

@InProceedings{KHM89,
  author = 	 {Kranz, D.A. and {Halstead Jr.}, R.H. and Mohr, E.},
  title = 	 {{Mul-T: A High-Performance Parallel Lisp}},
  booktitle =    {{PLDI'91 --- Programming Languages Design and
                   Implementation}},
  volume =	 {24(7)},
  OPTnumber =	 7,
  series =	 {SIGPLAN Notices},
  year =	 1989,
  address =	 {Portland, Oregon, June 21--23},
  pages =	 {81--90},
  annote =	 {one of first mentions of lazy futures; uses mostly
                  load-based inlining while developing Mul-T},
  abstract =     "Mul-T is a parallel Lisp system, based on Multilisp's
                 {\tt future} construct, that has been developed to run
                 on an Encore Multimax processor. Mul-T is an extended
                 version of the Yale T system and uses the T system's
                 ORBIT compiler to achieve ``production quality''
                 performance on stock hardware --- about 100 times
                 faster than Multilisp. Mul-T shows that futures can be
                 implemented cheaply enough to be useful in a
                 production-quality system. Mul-T is fully operational,
                 including a user interface that supports managing
                 groups of parallel tasks.",
  keywords =     "Lisp, Futures",
}

@inproceedings{MKH90,
  author =    {Mohr, E. and Kranz, D.A. and {Halstead Jr.}, R.H.},
  title =     {{Lazy Task Creation: A Technique for Increasing the 
                Granularity of Parallel Programs}},
  booktitle = {{LFP'90 --- Conference on Lisp and Functional Programming}},
  address =   {Nice, France, June 27--29},
  year =      {1990},
  pages =     {185--197},
  descr =     {plfun,psben,psenv},
  url =       {ftp://crl.dec.com/pub/DEC/CRL/tech-reports/90.7.ps.Z},
  abstract =  "Many parallel algorithms are naturally expressed at a fine level of
granularity, often finer than a MIMD parallel system can exploit
efficiently. Most builders of parallel systems have looked to either the
programmer or a parallelizing compiler to increase the granularity of such
algorithms. In this paper we explore a third approach to the granularity
problem by analyzing two strategies for combining parallel tasks dynamically
at run-time. We reject the simpler load-based inlining method, where tasks are
combined based on dynamic load level, in favor of the safer and more robust
lazy task creation method, where tasks are created only retroactively as
processing resources become available.

These strategies grew out of work on Mul-T, an efficient parallel
implementation of Scheme, but could be used with other languages as well. We
describe our Mul-T implementations of lazy task creation for two contrasting
machines, and present performance statistics which show the method's
effectiveness. Lazy task creation allows efficient execution of naturally
expressed algorithms of a substantially finer grain than possible with
previous parallel Lisp systems.
",
}

@article{MKH91,
  author =    {Mohr, E. and Kranz, D.A. and {Halstead Jr.}, R.H.},
  title =     {{Lazy Task Creation: A Technique for Increasing the 
                Granularity of Parallel Programs}},
  journal =   {{IEEE Transactions on Parallel and Distributed Systems}},
  volume =    {2},
  number =    {3},
  pages =     {264--280},
  month =     {July},
  year =      {1991},
  descr =     {plfun,psben,psenv},
  keywords =  {parallel programming languages, load balancing, program partitioning, process migration, parallel Lisp, task management},
  url =       {ftp://cag.lcs.mit.edu/pub/papers/futures.ps.Z},
  abstract =  "Many parallel algorithms are naturally expressed at a fine level of
granularity, often finer than a MIMD parallel system can exploit
efficiently. Most builders of parallel systems have looked to either the
programmer or a parallelizing compiler to increase the granularity of such
algorithms. In this paper we explore a third approach to the granularity
problem by analyzing two strategies for combining parallel tasks
dynamically at run-time. We reject the simpler load-based inlining method,
where tasks are combined based on dynamic load level, in favor of the safer
and more robustlazy task creation method, where tasks are created only
retroactively as processing resources become available. These strategies
grew out of work on Mul-T [15], an efficient parallel implementation of
Scheme, but could be used with other languages as well. We describe our
Mul-T implementations of lazy task creation for two contrasting machines,
and present performance statistics which show the method's
effectiveness. Lazy task creation allows efficient execution of naturally
expressed algorithms of a substantially finer grain than possible with
previous parallel Lisp systems."
}


@Article{Hals85,
  author =       {{Halstead, Jr.}, R.H.},
  title =        {{Multilisp: A Language for Concurrent Symbolic
                 Computation}},
  journal =      toplas,
  volume =       {7},
  number =       {4},
  month =        oct,
  year =         {1985},
  pages =        {501--538},
  abstract =     "Multilisp is a version of the Lisp dialect Scheme
                 extended with constructs for parallel execution. Like
                 Scheme, Multilisp is oriented toward symbolic
                 computation. Unlike some parallel programming
                 languages, Multilisp incorporates constructs for
                 causing side effects and for explicitly introducing
                 parallelism. The potential complexity of dealing with
                 side effects in a parallel context is mitigated by the
                 nature of the parallelism constructs and by support for
                 abstract data types: a recommended Multilisp
                 programming style is presented which, if followed,
                 should lead to highly parallel, easily understandable
                 programs. Multilisp is being implemented on the 32
                 processor {\em Concert\/} multiprocessor; however, it
                 is ultimately intended for use on larger
                 multiprocessors. The current implementation, called
                 {\em Concert Multilisp\/}, is complete enough to run
                 the Multilisp compiler itself and has been run on
                 Concert prototypes including up to eight processors.
                 Concert Multilisp uses novel techniques for task
                 scheduling and garbage collection. The task scheduler
                 helps control excessive resource utilization by means
                 of an unfair scheduling policy; the garbage collector
                 uses a multiprocessor algorithm based on the
                 incremental garbage collector of Baker.",
  keywords =     "Lisp, Futures",
}

@PhdThesis{Rush95,
  author = 	 {Rushall, D.},
  title = 	 {{Task Exposure in the Parallel Implementation of
		   Functional Programming Languages}},
  school = 	 {Department of Computer Science},
  year = 	 {1995},
  address =	 {University of Manchester},
  abstract =     "A new mechanism for task exposure in large grain parallel implementations
of functional programming languages is presented. The scheme performs
`lazy-task creation'; provisionally performing all work inline, but saving
enough information that parallelism may be extracted later. However, unlike
the scheme presented by Mohr, Krantz and Halstead (1990) the new mechanism
`sparks' new child tasks, rather than stealing continuations. An
experimental implementation has been constructed to assess the feasibility
of the mechanism, which seems to perform at least as well as two benchmark
schemes. The lazy-task creation mechanism aims to both delay commitment to
either inline or parallel evaluation, and allow the use of optimised inline
code with minimal bookkeeping overhead. The technique operates by examining
and extracting work from live execution stacks; identifying potential
parallelism by interpreting the continuations saved during a function
call. In this way, the cost of task extraction is borne almost entirely by
the task extraction mechanism itself. When the machine load is high, and
task creation infrequent, the overheads should be very low. The thesis
opens with an introduction to the advantages of functional programming
languages, and outlines the issues in their implementation, both on single
and multiprocessor machines. The new mechanism for lazy-task creation is
presented, and the issues of code generation and run-time support
considered. For simplicity, the code generator is specified using an
attribute grammar, and a tool is presented by which an executable Haskell
program can be generated. Finally, the results of the experimental
implementation are examined, and its performance compared to `sparking' and
`spawning' task exposure mechanisms."
}

@TechReport{Sarg91,
  author = 	 {Sargeant, J.},
  title = 	 {{Use of Lazy Task Creation in Dynamic Computations}},
  institution =  {Department of Computer Science},
  year = 	 1991,
  type =	 {Internal EDS report},
  number =	 {EDS.RD.3I.M017},
  address =	 {University of Manchester},
  annote =	 {mentioned in Rushall's thesis}
}


@InProceedings{HwRu92,
  author =       {Hwang, S. and Rushall, D.},
  title =        {{The $\nu$-STG machine: A Parallelized Spineless
                 Tagless Graph Reduction Machine in a Distributed Memory
                 Architecture (Draft Version)}},
  booktitle =    {{IFL'92 --- International Workshop on the Parallel
                 Implementation of Functional Languages}},
  address =      {Aachen, Germany, September 28--30},
  year =         {1992},
  editor =       {Kuchen, H. and Loogen, R.},
  abstract =     "This paper describes the $\nu$-STG machine a
                 parallelized Spineless Tagless Graph Reduction (STG)
                 machine in a distributed memory architecture. In the
                 $\nu$-STG machine, a stack for each task is distributed
                 by allocating a {\em context frame\/} for each {\em
                 tail-call sequence\/} on the {\em heap\/}. Two sparking
                 mechanisms, {\em BSpark\/} and {\em ISpark\/}, are
                 supported to introduce parallelism, which are similar
                 to those in the HDG machine. The {\em BSpark\/} cannot
                 be ignored and should be synchronized with the parent
                 task to prevent it being blocked and resumed too
                 frequently, while {\em ISpark\/} may be ignored. Even
                 though a variable is sparked by {\em BSpark\/} or {\em
                 ISpark\/}, it may be evaluated {\em in-line\/} by the
                 parent task. Creating a new task to evaluate a {\em
                 boxed\/} variable according to the spark annotation is
                 delayed until it is really needed. A message passing
                 mechanism is used to distribute jobs and synchronize
                 them in a machine. A {\em lazy task creation
                 mechanism\/} is supported to exploit paallelism in {\em
                 unboxed\/} arithmetic expression by {\em boxing up\/}
                 on demand. A simple printing mechanism is added to
                 print outputs clearly.",
  keywords =     "Parallel Graph Reduction, G-Machine",
}

@PhdThesis{Feel93,
  author = 	 {Feeley, M.},
  title = 	 {{An Efficient and General Implementation of Futures on Large Scale Shared-Memory Multiprocessors}},
  school = 	 {Brandeis University},
  year = 	 1993,
  month =	 {April},
  url =          {http://www.iro.umontreal.ca/~feeley/papers/futures.ps.gz}
}

% ---------------------------------------------------------------------------
%$%node General Runtime-System Stuff, Visualisation, Lazy Task Creation, Parallel Lisp
%$%subsection General Runtime-System Stuff
% ---------------------------------------------------------------------------

%$%cindex Cilk

@InProceedings{BJK*95,
  author = 	 {Blumofe, R.D. and Joerg, C.F. and Kuszmaul, B.C. and 
		  Leiserson, C.E. and Randall, K.H. and Zhou, Y.},
  title = 	 {{Cilk: An Efficient Multithreaded Runtime System}},
  booktitle =	 {PPoPP'95 --- Symposium on Principles and Practice of Parallel
		    Programming},
  pages =        {207--216},
  year =	 {1995},
  address =	 {Santa Barbara, California, July 19--21},
  url =          {ftp://theory.lcs.mit.edu/pub/cilk/PPoPP95.ps.Z},
  abstract =     "Cilk  (pronounced ``silk'') is a  C-based  runtime system for multithreaded
parallel programming. In this paper, we document the efficiency of the Cilk
work-stealing scheduler, both empirically and  analytically . We show  that
on real and synthetic applications, the ``work'' and ``critical path'' of a
Cilk computation can be used to accurately model performance. Consequently,
a Cilk programmer  can focus on reducing the  work and critical path of his
computation,   insulated from load  balancing  and other runtime scheduling
issues.   We  also  prove   that   for  the   class   of  ``fully  strict''
(well-structured) programs,  the  Cilk scheduler achieves space,  time, and
communication bounds  all  within a constant  factor of   optimal. The Cilk
runtime system currently runs on the Connection Machine  CM5 MPP, the Intel
Paragon MPP,  the Silicon Graphics Power Challenge  SMP,  and the MIT Phish
network  of workstations.   Applications  written in  Cilk  include protein
folding, graphic  rendering,   backtrack search, and   the *Socrates  chess
program, which won third prize in the 1994 ACM International Computer Chess
Championship.",
}

@InProceedings{BlLi97,
  author = 	 {Blumofe, R.D. and Lisiecki, P.A.},
  title = 	 {{Adaptive and Reliable Parallel Computing on Networks of Workstations}},
  booktitle = 	 {USENIX 1997 Annual Technical Symposium},
  year = 	 {1997},
  address = 	 {Anaheim, California, January 6--10},
  url =          {ftp://theory.lcs.mit.edu/pub/cilk/USENIX97.ps.gz},
  abstract =     "In this  paper, we present  the design of  Cilk-NOW, a runtime  system that
adaptively and reliably executes functional  Cilk programs in parallel on a
network  of  UNIX workstations.  Cilk  (pronounced  ``silk'')  is a  parallel
multithreaded extension  of the  C language, and  all Cilk  runtime systems
employ a provably efficient thread-scheduling algorithm. Cilk-NOW is such a
runtime system,  and in addition, Cilk-NOW  automatically delivers adaptive
and  reliable  execution for  a  functional  subset  of Cilk  programs.  By
adaptive execution, we  mean that each Cilk program  dynamically utilizes a
changing set of otherwise-idle workstations. By reliable execution, we mean
that the  Cilk-NOW system as  a whole and  each executing Cilk  program are
able  to  tolerate machine  and  network  faults.  Cilk-NOW provides  these
features  while   programs  remain  fault  oblivious,   meaning  that  Cilk
programmers need  not code for  fault tolerance. Throughout this  paper, we
focus on end-to-end design decisions, and we show how these decisions allow
the  design  to  exploit  high-level  algorithmic properties  of  the  Cilk
programming model in order to simplify and streamline the implementation."
}

@InProceedings{FLR98,
  author = 	 {Frigo, M. and Leiserson, C.E. and Randall, K.H.},
  title = 	 {{The Implementation of the Cilk-5 Multithreaded Language}},
  booktitle = 	 {{PLDI'98 --- Conference on Programming Language Design and Implementation}},
  year =	 1998,
  address =	 {June 17--19, Montreal, Canada},
  publisher =	 {ACM Press}, 
  annote =       {std reference for Cilk-5},
  keywords =     {Critical path, multithreading, parallel computing, programming language, runtime system, work.},
  url =          {ftp://theory.lcs.mit.edu/pub/cilk/cilk5.ps.gz},
  abstract =     "
The fifth release of  the multithreaded language  Cilk uses a provably good
``work-stealing'' scheduling algorithm similar to the first system, but the
language has been completely  redesigned and the runtime system  completely
reengineered. The efficiency of the new implementation was aided by a clear
strategy   that arose from   a    theoretical analysis of the    scheduling
algorithm: concentrate on minimizing overheads that contribute to the work,
even at the   expense of   overheads   that  contribute  to the    critical
path. Although it may  seem  counterintuitive to  move overheads  onto  the
critical  path, this ``work-first'' principle has  led to a portable Cilk-5
implementation in  which the typical cost  of spawning a parallel thread is
only between 2 and  6 times the cost  of a C function call  on a variety of
contemporary machines.    Many Cilk programs  run  on   one  processor with
virtually no degradation  compared to  equivalent  C programs. This   paper
describes how  the work-first  principle was  exploited  in  the design  of
Cilk-5's   compiler and its  runtime   system.  In  particular, we  present
Cilk-5's  novel ``two-clone''  compilation  strategy  and its Dijkstra-like
mutual-exclusion   protocol   for implementing  the   ready   deque in  the
work-stealing scheduler."
}

@PhdThesis{Frig99,
  author = 	 {Frigo, M.},
  title = 	 {{Portable High-Performance Programs}},
  school = 	 {Department of Electrical Engineering and Computer Science},
  year = 	 1999,
  address =	 {M.I.T.},
  month = 	 {June},
  abstractURL =  {http://supertech.lcs.mit.edu/cilk/papers/abstracts/Frigo-phd-thesis.html},
  url =          {ftp://theory.lcs.mit.edu/pub/cilk/frigo-phd-thesis.ps.gz},
  abstract =     "This dissertation discusses how to write computer programs that attain both
high performance  and portability, despite  the fact that  current computer
systems have different degrees of parallelism, deep memory hierarchies, and
diverse processor architectures.

To cope with parallelism  portably in high-performance programs, we present
the  ``Cilk''  multithreaded programming  system.   In  the Cilk-5  system,
parallel programs scale  up to run efficiently on  multiple processors, but
unlike  existing parallel-programming  environments, such  as MPI  and HPF,
Cilk programs  ``scale down'' to run  on one processor as  efficiently as a
comparable C  program. The  typical cost of  spawning a parallel  thread in
Cilk-5 is only between  2 and 6 times the cost of  a C function call.  This
efficient implementation  was guided  by the {\em  work-first principle\/},
which dictates  that scheduling overheads  should be borne by  the critical
path of  the computation and not by  the work.  We show  how the work-first
principle inspired Cilk's novel  ``two-clone'' compilation strategy and its
Dijkstra-like mutual-exclusion protocol for implementing the ready deque in
the work-stealing scheduler.

To  cope portably  with  the memory  hierarchy,  we present  asymptotically
optimal algorithms  for rectangular matrix  transpose, FFT, and  sorting on
computers  with  multiple  levels  of  caching.   Unlike  previous  optimal
algorithms, these algorithms are cache oblivious: no variables dependent on
hardware parameters, such  as cache size and cache-line  length, need to be
tuned to achieve optimality.  Nevertheless, these algorithms use an optimal
amount of work and move data optimally among multiple levels of cache.  For
a cache with  size $Z$ and cache-line length  $L$ where $Z=\Omega(L^2)$ the
number  of   cache  misses   for  an  $mÅ◊   n$  matrix   transpose  is
$\Theta(1+mn/L)$.  The number  of cache misses for either  an $n$-point FFT
or the  sorting of $n$  numbers is $\Theta(1+(n/L)(1+\log_Z n))$.   We also
give a $\Theta(mnp)$-work algorithm to multiply an $mÅ◊ n$ matrix by an
$nÅ◊ p$  matrix that incurs $\Theta(1 +  (mn+np+mp)/L + mnp/L\sqrt{Z})$
cache faults.

To  attain portability  in  the face  of  both parallelism  and the  memory
hierarchy at  the same  time, we examine  the {\em  location consistency\/}
memory model and the {\em backer\/} coherence algorithm for maintaining it.
We prove good asymptotic bounds on the execution time of Cilk programs that
use location-consistent shared memory.

To cope with the diversity  of processor architectures, we develop the FFTW
self-optimizing  program,  a  portable  C  library  that  computes  Fourier
transforms.  FFTW is unique in that it can automatically tune itself to the
underlying  hardware  in  order   to  achieve  high  performance.   Through
extensive benchmarking, FFTW has been shown to be typically faster than all
other  publicly  available FFT  software,  including  codes  such as  Sun's
Performance Library  and IBM's ESSL that  are tuned to  a specific machine.
Most of  the performance-critical code of FFTW  was generated automatically
by  a  special-purpose  compiler  written  in Objective  Caml,  which  uses
symbolic evaluation  and other compiler techniques  to produce ``codelets''
--- optimized sequences of  C code that can be  assembled into ``plans'' to
compute a Fourier transform.  At  runtime, FFTW measures the execution time
of many plans and uses  dynamic programming to select the fastest. Finally,
the plan drives a special interpreter that computes the actual transforms."
}

@PhdThesis{Joer96,
  author = 	 {Joerg, C.F.},
  title = 	 {{The Cilk System for Parallel Multithreaded Computing}},
  school = 	 {Department of Electrical Engineering and Computer Science},
  year = 	 1996,
  address =	 {M.I.T.},
  month =	 {January},
  annote =	 {discusses development of Cilk in 4 stages; focus on RTS issues},
  documentURL =  {ftp://theory.lcs.mit.edu/pub/cilk/joerg-phd-thesis.ps.gz},
  abstractURL =  {http://supertech.lcs.mit.edu/cilk/papers/abstracts/joerg-phd-thesis.html},
  abstract =     "
Although cost-effective  parallel machines are  now commercially available,
the widespread use of  parallel processing is  still  being held back,  due
mainly to the troublesome nature of parallel programming. In particular, it
is  still difficult   to    build efficient implementations   of   parallel
applications whose communication  patterns  are either highly  irregular or
dependent   upon  dynamic  information.    Multithreading  has   become  an
increasingly   popular way   to  implement   these  dynamic,  asynchronous,
concurrent programs. Cilk (pronounced  ``silk'') is our C-based multithreaded
computing system that  provides provably good performance  guarantees. This
thesis describes the evolution of the Cilk language and runtime system, and
describes applications which affected the evolution of the system.

Using Cilk, programmers are able  to express  their applications either  by
writing multithreaded code written  in a continuation-passing style, or  by
writing code using normal  call/return semantics and specifying which calls
can be performed   in parallel.  The  Cilk  runtime  system takes  complete
control of  the   scheduling, load-balancing, and communication   needed to
execute    the  program, thereby   insulating  the   programmer from  these
details. The programmer can rest assured  that his program will be executed
efficiently since the Cilk  scheduler  provably achieves time, space,   and
communication    bounds all within  a    constant  factor of optimal.   For
distributed  memory  environments,    we  have   implemented a     software
shared-memory  system for Cilk. We have   defined a ``dag-consistent'' memory
model which is a lock-free consistency model well  suited to the needs of a
multithreaded program. Because dag consistency is a weak consistency model,
we have been able to implement coherence efficiently in software.

The most   complex application written in  Cilk  is the  *Socrates computer
chess  program.    *Socrates is   a  large,   nondeterministic, challenging
application whose complex  control  dependencies make it inexpressible   in
many  other parallel programming systems.  Running on an 1824-node Paragon,
*Socrates finished second in the 1995 W orld Computer Chess Championship.

Currently, versions of Cilk run on the Thinking Machines CM-5, the Intel
Paragon, various SMPs, and on networks of workstations. The same Cilk
program will run on all of these platforms with little, if any,
modification. Applications written in Cilk include protein folding, graphic
rendering, backtrack search, and computer chess. " }

@PhdThesis{Rand98,
  author = 	 {Randall, K.H.},
  title = 	 {{Cilk: Efficient Multithreaded Computing}},
  school = 	 {Department of Electrical Engineering and Computer Science},
  year = 	 1998,
  address =	 {M.I.T.},
  month =	 {June},
  documentURL =  {ftp://theory.lcs.mit.edu/pub/cilk/randall-phdthesis.ps.gz},
  abstractURL =  {http://supertech.lcs.mit.edu/cilk/papers/abstracts/randall-phdthesis.html},
  abstract =     "
This  thesis describes   Cilk,    a parallel multithreaded   language   for
programming  contemporary shared memory  multiprocessors (SMP's). Cilk is a
simple  extension of C which  provides constructs  for parallel control and
synchronization. Cilk  imposes very low overheads  --- the typical  cost of
spawning a  parallel thread is only between  2 and 6 times the  cost of a C
function call on a variety of contemporary machines. Many Cilk programs run
on one processor  with virtually  no  degradation compared to equivalent  C
programs. We present the  ``work-first'' principle which guided  the design
of  Cilk's scheduler  and  two  consequences  of  this principle,  a  novel
``two-clone''  compilation  strategy  and a  Dijkstra-like mutual-exclusion
protocol for implementing the ready queue in the work-stealing scheduler.

To facilitate debugging  of Cilk programs, Cilk provides  a tool called the
Nondeterminator-2 which finds  nondeterministic bugs called ``data races''.
We   present   two  algorithms,   All-Sets   and   Brelly,   used  by   the
Nondeterminator-2 for  finding data races. The All-Sets  algorithm is exact
but can sometimes have poor  performance; the Brelly algorithm, by imposing
a  locking discipline on  the programmer,  is guaranteed  to run  in nearly
linear time. For a program that  runs serially in time T, accesses V shared
memory locations, and holds at  most k locks simultaneously, Brelly runs in
O(k T f(V;V)) time and O(k V) space, where f is Tarjan's functional inverse
of Ackermann's function.

Cilk can be run on clusters of SMP's as well. We define a novel weak memory
model called ``dag consistency'' which provides a natural consistency model
for use with  multithreaded languages like Cilk.  We provide  evidence that
Backer, the  protocol that implements  dag consistency, is both empirically
and theoretically efficient. In particular, we prove bounds on running time
and communication for  a Cilk program executed  on top of Backer, including
all costs associated with Backer itself. We believe this proof is the first
of  its kind in this regard.  Finally, we  present the MultiBacker protocol
for clusters of  SMP's which extends Backer to  take  advantage of hardware
support for shared memory within an SMP."
}


@Manual{Cilk97,
  title = 	 {{Cilk-5.1 (Beta-1) Reference Manual}},
  key =		 {Cilk Manual},
  author =	 {Supercomputing Technologies Group},
  address = 	 {Department of Electrical Engineering and Computer Science, M.I.T.},
  month =	 {September},
  year =	 1997,
  url =          {ftp://theory.lcs.mit.edu/pub/cilk/manual5.1.1.ps.gz},
}

%$%cindex Filaments

@Article{LFA96,
  author = 	 {Lowenthal, D.K. and Freeh, V.W. and Andrews, G.R.},
  title = 	 {{Using Fine-Grain Threads and Run-Time Decision-Making in
                   Parallel Computing}}, 
  journal = 	 JPDC,
  year = 	 {1996},
  volume = 	 {37},
  number = 	 {1},
  pages = 	 {41--54},
  annote = 	 {light weight threads, run-time methods for increasing
                  granularity, load balancing, cache coherence}, 
  url =          {ftp://ftp.cs.arizona.edu/reports/1996/TR96-01.ps},
  abstract =     "
    Programming distributed-memory multiprocessors and networks of 
    workstations requires deciding what can execute concurrently, how 
    processes communicate, and where data is placed. These decisions can 
    be made statically by a programmer or compiler, or they can be made 
    dynamically at run time. Using run-time decisions leads to a simpler 
    interface-because decisions are implicit-and it can lead to better 
    decisions-because more information is available. This paper examines 
    the costs, benefits, and details of making decisions at run time. The
    starting point is explicit fine-grain parallelism with any number 
    (even thousands) of threads. Five specific techniques are considered:
    (1) implicitly coarsening the granularity of parallelism, (2) using 
    implicit communication implemented by a distributed shared memory, 
    (3) overlapping computation and communication, (4) adaptively moving 
    threads and data between nodes to minimize communication and balance 
    load, and (5) dynamically remapping data to pages to avoid false 
    sharing. Details are given on the performance of each of these 
    techniques as well as on their overall performance for several 
    scientific applications.",
}

@TechReport{LFA95,
  author = 	 {Lowenthal, D.K. and Freeh, V.W. and Andrews, G.R.},
  title = 	 {{Efficient Support for Fine-Grain Parallelism on
                   Shared-Memory Machines}}, 
  institution =  {Department of Computer Science},
  year = 	 {1995},
  number = 	 {TR95-14},
  address = 	 {University of Arizona},
  month = 	 {December},
  url =          {ftp://ftp.cs.arizona.edu/reports/1995/TR95-14.ps},
  abstract =     "
Programming distributed-memory multiprocessors and networks of workstations
requires deciding what can execute concurrently, how processes communicate,
and where data  is placed.   These  decisions can be  made  statically by a
programmer or compiler, or they can be made dynamically at run time.  Using
run-time decisions leads to   a  simpler interface because   decisions  are
implicit and  it can lead  to better decisions  because more information is
available.  This paper examines the  costs, benefits, and details of making
decisions at   run   time.  The   starting point  is  explicit   fine-grain
parallelism with  any number (even thousands)  of  threads. Five  sp ecific
techniques are  considered:  (1) implicitly coarsening  the  granularity of
parallelism, (2) using implicit communication  implemented by a distributed
shared   memory   , (3)  overlapping   computation   and communication, (4)
adaptively moving threads and data between  nodes to minimize communication
and  balance load, and (5)  dynamically  remapping data to  pages to  avoid
false sharing.  Details are   given on  the  performance of  each  of these
techniques as  well   as their overall  performance  on  several scientific
applications."
}

%$%cindex Concert

@InProceedings{CDG*97,
  author = 	 {Chien, A. and Dolby, J. and Ganguly, B. and Karamcheti,
                  V. and Zhang, X.},
  title = 	 {{Supporting High Level Programming with High Performance: The Illinois
     Concert System}},
  booktitle = 	 {{HIPS'97 -- High-level Parallel
                   Programming Models and Supportive Environments}},
  year =	 1997,
  month =	 {April},
  keywords =     {concurrent languages, concurrent object-oriented programming, compiler optimization, runtime systems, objectoriented optimization},
  url =          {http://www-csag.cs.uiuc.edu/papers/hips97.ps},
  abstract =      "Programmers of concurrent applications are faced with a complex performance
space in which data distribution  and concurrency management exacerbate the
difficulty of    building  large, complex   applications.  To address these
challenges,  the  Illinois  Concert  system  provides  a global  namespace,
implicit concurrency control and  granularity management,  implicit storage
management, and  object-oriented programming  features. These features  are
embodied in a  language ICC++ (derived  from C++)  which  has been  used to
build  a number of  kernels and  applications. As  high level features  can
potentially incur overhead, the Concert system employs  a range of compiler
and runtime  optimization techniques to efficiently  support the high level
programming model. The compiler techniques include type inference, inlining
and specialization; and the runtime techniques include caching, prefetching
and hybrid stack/heap multithreading. The effectiveness of these techniques
permits  the   construction  of  complex   parallel applications   that are
flexible,   enabling convenient  application   modification  or tuning.  We
present performance results for  a   number of application programs   which
attain good speedups and absolute performance.",
}

@TechReport{CDG*??,
  author = 	 {Chien, A. and Dolby, J. and Ganguly, B. and  Karamcheti, V. and   Zhang, X.},
  title = 	 {{High Level Parallel Programming: The Illinois Concert System}},
  institution =  {Department of Computer Science},
  year = 	 {19??},
  OPTkey = 	 {},
  OPTtype = 	 {???},
  OPTnumber = 	 {},
  OPTaddress = 	 {University of Illinois},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {check how this paper was published},
  keywords =     {concurrent languages, concurrent object-oriented programming, compiler optimization, runtime systems, object-oriented optimization},
  abstract =     "
Programmers  of concurrent applications are  faced with complex performance
tradeoffs, since data  distribution  and concurrency  management exacerbate
the  difficulty of building  large,  complex applications. To address these
challenges, the  Illinois  Concert  system  provides a  global   namespace,
implicit concurrency  control and  granularity management, implicit storage
management, and object-oriented  programming  features. These  features are
embodied in a language  ICC++  (derived from C++)   which has been used  to
build a  number of  kernels and  applications.  As high level features  can
potentially incur overhead, the Concert  system employs a range of compiler
and  runtime optimization techniques to efficiently  support the high level
programming model. The compiler techniques include type inference, inlining
and specialization; and the runtime techniques include caching, prefetching
and hybrid stack/heap multithreading. The effectiveness of these techniques
permits the construction of complex parallel applications that are portable
and flexible,  enabling convenient application   modification or tuning. We
present performance results  for  a number  of application  programs  which
attain good speedups and absolute performance."
}

@Article{KPC97,
  author = 	 {Karamcheti, V. and Plevyak, J. and Chien, A.},
  title = 	 {{Runtime Mechanisms for Efficient Dynamic Multithreading}},
  journal = 	 JPDC,
  year = 	 1997,
  volume =	 37,
  pages =	 {21--40},
  url =          {http://www-csag.cs.uiuc.edu/papers/rtperf.ps},
  abstract =     "High performance on distributed memory machines for programming models with
dynamic thread   creation and   multithreading requires   efficient  thread
management   and    communication.   Traditional  multithreading  runtimes,
consisting of few general-purpose, bundled  mechanisms that assume  minimal
compiler  and  hardware support, are   suitable  for computations involving
coarse-grained threads but provide low efficiency in  the presence of small
granularity  threads and irregular communication  behavior. We describe two
mechanisms of   the Illinois  Concert   runtime system which  address  this
shortcoming. The first,   hybrid   stack-heap  execution,  exploits   close
coupling with  the compiler  to  dynamically form coarse-grained  execution
units; threads are lazily  created as required  by runtime situations.  The
second, pull   messaging,   exploits  hardware  support to     implement  a
distributed message queue with receiver-initiated data transfer, delivering
robust   performance  across  a  wide    range  of  dynamic   communication
characteristics.  We measure their performance impact  based  on a Cray T3D
implementation of the Concert system. Individually, the mechanisms increase
absolute execution efficiency  by  up to  50\%. Together, they  increase the
feasible space of efficient computations, enabling compute granularities an
order  of  magnitude smaller.  Performance results  for two large irregular
applications    demonstrate    that   expressing  programs  using   dynamic
multithreading need not compromise on performance.",
}

@InProceedings{KaCh93,
  author = 	 {Karamcheti, V. and Chien, A.},
  title = 	 {{Concert --- Efficient Runtime Support for Concurrent Object-Oriented
     Programming Languages on Stock Hardware}},
  booktitle = 	 {Supercomputing '93},
  year =	 1993,
  month =	 {November},
  pages =	 {598--607},
  url =          {http://www-csag.cs.uiuc.edu/papers/runtime.ps},
  abstract =     "Inefficient implementations   of  global namespaces,  message passing,  and
thread scheduling  on    stock multicomputers  have   prevented  concurrent
object-oriented   programming  (COOP) languages   from   gaining widespread
acceptance.  Recognizing  that  the  architectures  of stock multicomputers
impose a  hierarchy of  costs  for these operations,   we have  described a
runtime system which    provides  different versions of  each    primitive,
exposing   performance  distinctions  for  optimization.  We  confirm   the
advantages of a cost-hierarchy based runtime system organization by showing
a  variation of   two  orders of magnitude  in   version  costs for  a  CM5
implementation. Frequency measurements  based on COOP  application programs
demonstrate that  a 39\%  invocation cost  reduction is feasible  by simply
selecting cheaper versions of runtime operations.",
}

@PhdThesis{Kara98,
  author = 	 {Karamcheti, V.Y.},
  title = 	 {{Run-Time Techniques for Dynamic Multithreaded Computations }},
  school = 	 {University of Illinois at Urbana-Champaign},
  year = 	 1998,
  annote =	 {discusses load balance and data locality; implemented in Concert},
  abstract =     "
Programming  models  based   on dynamic multithreading    enable convenient
expression  of irregular  parallel   applications  by  providing high-level
features such  as object-oriented programming, a  global object name space,
and  elective concurrency.  Despite  these programmability advantages, such
models  have  not gained  widespread use because   they are  challenging to
implement efficiently on  scalable parallel machines  due to their dynamic,
irregular thread structure (e.g., granularity variations) and unpredictable
data access patterns.

In this dissertation, we present an execution framework consisting of novel
run-time mechanisms that overcome these challenges. The framework separates
concerns   of local and parallel   efficiency,   enabling mechanisms to  be
independently   developed  and   optimized   for  each.   Local  efficiency
techniques,   hybrid  stack-heap  execution   and  pull  messaging, support
low-overhead thread management  and communication that deliver  performance
conditioned upon run-time locality  and  load balance. Parallel  efficiency
techniques,  view caching  and a   hierarchical load balancing   framework,
exploit knowledge of application behavior  to create good locality and load
balance  situations   at  run  time,   achieving performance    robust over
computation irregularity.

The  execution  framework has been  implemented  in   the Illinois  Concert
System, targeting  the ICC++   language. Evaluation  of mechanisms  on  two
target platforms, the  Cray  T3D and the SGI  Origin  2000, for four  large
irregular applications  demonstrates  their  individual  effectiveness  and
collective sufficiency: Each application achieves performance comparable to
the best achievable using low-level means. Additionally, these improvements
persist across  a      range  of  hardware    communication    architecture
interfaces. Our results  imply   that high-level expressions  of   parallel
programs need not sacrifice on performance."
}

%$%cindex Charm

@Article{KRSG94,
  author = 	 {Kale, L.V. and Ramkumar, B. and Sinha,  A.B. and Gursoy, A.},
  title = 	 {{The Charm Parallel Programming Language and System}},
  journal = 	 {IEEE Transactions on Parallel and Distributed Systems},
  year = 	 1994,
  url =  {http://charm.cs.uiuc.edu/papers/CharmSys1TPDS94.ps},
  abstractURL =  {http://charm.cs.uiuc.edu/papers/CharmSys1TPDS94.html},
  abstract =     "We describe a parallel programming system for developing machine independent
programs for all MIMD machines. Many useful approaches to this problem are
seen to require a common base of support, which can be encapsulated in a
language that abstracts over resource management decisions and machine
specific details. This language can be used for implementing other high level
approaches as well as for efficient application programming. The requirements
for such a language are defined, and the language supported by the Charm
system is described, and illustrated with examples. Charm is one of the first
languages to support message driven execution, and embodies unique
abstractions such as branch office chares and specifically shared
variables. In Part II of this paper, we talk about the runtime support system
for Charm. The system thus provides ease of programming on MIMD platforms
without sacrificing performance.",
}

@incollection{CharmppPPWCPP96,
  author = "L. V. Kale and Sanjeev Krishnan",
  title = "{Charm++: Parallel Programming with Message-Driven Objects}",
  booktitle = "Parallel    Programming using C++",
  publisher = "MIT Press",
  year = "1996",
  editor = "Gregory V. Wilson and Paul Lu",
  pages = "175-213",
}

@inproceedings{StructDaggerEURO96,
  author = "L. V. Kale and Milind Bhandarkar",
  title = "{Structured Dagger: A Coordination Language for
           Message-Driven Programming}",
  booktitle = "Proceedings of Second International Euro-Par Conference",
  year = "1996",
  volume = "1123-1124",
  series = "Lecture Notes in Computer Science",
  pages = "646-653",
  month = "September",
}

@inproceedings{RuntimeOptsPDPTA96,
  author = "Sanjeev Krishnan and L. V. Kale",
  title = "{Automating Runtime Optimizations for Load Balancing
           in Irregular Problems}",
  booktitle = "Proc. Conf. on Parallel and Distributed Processing
               Technology and Applications",
  year = "1996",
  address = "San Jose, California",
  month = "August",
}

@article{CharmSys1TPDS94,
    author  = "L. V. KalÅÈ and B. Ramkumar and A. B. Sinha and A.  Gursoy ",
    title   = "{The CHARM Parallel Programming Language and System:
            Part I -- Description of Language Features}",
    journal = "IEEE Transactions on Parallel and Distributed Systems",
    year    = 1994
}

@article{CharmSys2TPDS94,
    author  = "L. V. KalÅÈ and B. Ramkumar and A. B. Sinha and V. A. Saletore",
    title   = "{The CHARM Parallel Programming Language and System:
            Part II -- The Runtime system}",
    journal = "IEEE Transactions on Parallel and Distributed Systems",
    year    = 1994
}


@Article{DGF97,
  author = 	 {Dinapoli, D. and Giordano, M. and Furnari, M.M.},
  title = 	 {{A PVM-Based Distributed Parallel Symbolic System}},
  journal = 	 {Journal of Advances in Engineering Software},
  year = 	 1997,
  volume =	 28,
  number =	 5,
  pages =	 {303--312},
  abstract =     "The purpose of this work is to provide a parallel programming 
    environment for symbolic applications suited to run on Distributed 
    Memory Parallel Systems (DMPS). This paper describes the 
    implementation of such an environment that is based on Sabot's 
    Paralation model (in particular its LISP implementation, Paralation 
    LISP). In our opinion, the clear distinction between the 
    communication and computation aspects in the Paralation model allows 
    a clearer and easier exploitation of the opportunities of parallelism
    in symbolic applications. Furthermore, the simple semantics of its 
    parallel constructs seem well suited to run on a community of loosely
    coupled LISP interpreters. We chose to implement the Paralation model
    constructs on the top of the Parallel Virtual Machine (PVM) package 
    because this tool allows one to have a cheap virtual distributed 
    memory parallel machine simply by using some LAN interconnected 
    powerful workstations."
}

@Manual{PVM93,
  key =		 {PVM},
  title = 	 {{Parallel Virtual Machine Reference Manual, Version 3.2}},
  author =	 {Oak Ridge National Laboratory},
  address =	 {University of Tennessee},
  month =	 {August},
  year =	 1993
}

@Article{MPI94,
  key =          {MPI},
  author = 	 {MPI Forum},
  title = 	 {{MPI: A Message-Passing Interface Standard}},
  journal = 	 {International Journal of Supercomputing Applications},
  year = 	 1994,
  volume =	 8,
  number =	 {3/4}
}

@Manual{PVM94,
  title = 	 {{PVM: Parallel Virtual Machine --- A Users' Guide and Tutorial for Networked Parallel Computing}},
  key =		 {PVM},
  author =	 {Geist, A. and Beguelin, A. and  Dongarra, J. and Jiang, W. and  Manchek, R. and Sunderam, V.},
  organization = {MIT Press},
  year =	 1994,
  url =          {http://www.netlib.org/pvm3/book/pvm-book.ps}
}

%$%node Visualisation,  , General Runtime-System Stuff, Parallel Lisp
%$%subsection Visualisation

@InProceedings{Hals94,
  author = 	 {{Halstead Jr.}, R.},
  title = 	 {{Self-Describing Files + Smart Modules = Parallel Program Visualisation}},
  booktitle = 	 {{Theory and Practice of Parallel Programming}},
  number =	 907,
  series =	 LNCS,
  year =	 1994,
  publisher =	 S-V,
  pages =	 {253--283},
  annote =	 {First (?) reference to VISTA package},
}

@InProceedings{Hals95,
  author = 	 {{Halstead Jr.}, R.},
  title = 	 {{Understanding the Performance of Parallel Symbolic
                  Programs}},  
  booktitle = 	 {{PSLS'95 --- International Workshop on Parallel Symbolic Languages
                  and Systems}},
  series =	 LNCS,
  year =	 1995,
  publisher =	 S-V,
  pages =	 {81--107},
  number =	 1068,
  month =        {October},
  address =      {Beaune, France}, 
}

%$%xref{Hals89,,,ws-pfp.bib,}


% ---------------------------------------------------------------------------
%$%node Theoretical work, Simulators, Parallel Lisp, Top
%$%section Theoretical work
% Lambda calculus, general background for fctal prging, scheduling etc
% ---------------------------------------------------------------------------

@Book{Chur41,
  author = 	 {Church, A.},
  title = 	 {{The Calculi of Lambda Conversion}},
  publisher = 	 {Princeton University Press},
  year = 	 1941
}


%$%xref{ariola95:_call_by_need,call-by-need,,wadler.bib,}
@InProceedings{AFM*95,
  author = 	 {Ariola, Z. and Felleisen, M. and Maraist, J.
                  and Odersky, M. and Wadler, P.},
  title = 	 {A call-by-need lambda calculus},
  booktitle = 	 {POPL'95 --- Symposium on Principles of Programming Languages},
  year =	 1995,
  address =	 {San Francisco, California},
  month =	 {January},
  annote =       {std reference for call-by-need lambda calculus},
  url =  {ftp://ftp.dcs.gla.ac.uk/pub/glasgow-fp/authors/Philip\_Wadler/need.ps},
  abstract =     "The mismatch  between the operational semantics of the lambda calculus and
the actual  behavior of implementations  is a  major  obstacle for compiler
writers. They cannot explain  the behavior of  their evaluator in  terms of
source level     syntax,   and   they cannot    easily    compare  distinct
implementations of  different lazy strategies.   In this paper we derive an
equational characterization of    call-by-need and prove  it   correct with
respect  to the original lambda calculus.  The theory is a strictly smaller
theory   than  the lambda calculus.    Immediate applications of the theory
concern the  correctness proofs of a  number of  implementation strategies,
e.g.,   the  call-by-need   continuation  passing  transformation   and the
realization of sharing via assignments."
}


@Article{ArFe97,
  author = 	 {Ariola, Z.M. and Felleisen, M.},
  title = 	 {{The Call-By-Need Lambda Calculus}},
  journal = 	 JFP,
  year = 	 1997,
  volume =	 7,
  number =	 3,
  month =	 {May},
  annote =	 {To appear; maybe better std ref for call-by-need lmbda calc}
}

@Article{Grah69,
  title =        "Bounds on Multiprocessing Timing Anomalies",
  author =       "R. L. Graham",
  journal =      "SIAM Journal of Applied Mathematics",
  volume =       "17",
  number =       "3",
  year =         "1969",
  month =        mar,
  pages =        "416--429",
  annote =       "original reference for list scheduling",
}

@Article{GLLR79,
  title =        "Optimization and Approximation in Deterministic
                 Sequencing and Scheduling: {A} Survey",
  author =       "R. L. Graham and E. L. Lawler and J. K. Lenstra and A.
                 H. G. Rinnooy Kan",
  journal =      "Ann. Discrete Mathematics",
  volume =       "5",
  year =         "1979",
  pages =        "287--326",
}

% ?? Check this
@Article{GaWo93,
  author =       "Galambos, G. and Woeginger, G.",
  title =        "An On-Line Scheduling Heuristic with Better Worst Case
                 Ratio Than Graham's List Scheduling",
  journal =      "SIAM Journal on Computing",
  volume =       "22",
  number =       "2",
  year =         "1993",
  pages =        "349--355",
  keywords =     "combinatorial problems, scheduling, worst case bounds, 
                  online algorithms", 
  abstract =     "The problem of on-line scheduling a set of independent jobs on m 
    machines is considered. The goal is to minimize the makespan of the 
    schedule. Graham's List Scheduling heuristic [R. L Graham, SIAM J. 
    Appl. Math., 17(1969), pp. 416-429] guarantees a worst case 
    performance of 2 - 1/m for this problem. This worst case bound cannot
    be improved for m = 2 and m = 3. For m greater-than-or-equal-to 4, 
    approximation algorithms with worst case performance at most 2 - 1/m 
    - epsilon(m) are presented, where epsilon(m) is some positive real 
    depending only on m.",
}

@Article{EZL89,
  author =       {Eager, D.L. and Zahorjan, J. and Lazowska, E.D},
  title =        {{Speedup Versus Efficiency in Parallel Systems}},
  journal =      {IEEE Trans. on Computers},
  volume =       {38},
  number =       {3},
  pages =        {408},
  month =        mar,
  year =         {1989},
  keywords =     {TOC},
}

@Article{Eager86,
  author =       "D. L. Eager and E. D. Lazowska and J. Zahorjan",
  title =        "Adaptive Load Sharing in Homogeneous Distributed
                 Systems",
  journal =      "IEEE Trans. on Softw. Eng.",
  volume =       "12",
  number =       "5",
  pages =        "662",
  month =        may,
  year =         "1986",
  keywords =     "TSE",
}

@Article{AFB92,
  author =       {Aharoni, G. and Feitelson, D.G.  and Barak, A.},
  title =        {{A Run-Time Algorithm for Managing the Granularity of
                 Parallel Functional Programs}},
  journal =      JFP,
  volume =       {2},
  number =       {4},
  pages =        {387--405},
  year =         {1992},
  abstract =     "An online (run-time) algorithm is presented that
                 manages the granularity of parallel functional
                 programs. The algorithm exploits useful parallelism
                 when it exists, and ignores ineffective parallelism in
                 programs that produce many small tasks. The idea is to
                 balance the amount of local work with the cost of
                 distributing the work. This is achieved by ensuring
                 that for every parallel task spawned, an amount of work
                 that equals the cost of the spawn is performed locally.
                 The authors analyse several cases and compare the
                 algorithm to the optimal execution. In most cases the
                 algorithm competes well with the optimal algorithm,
                 even though the optimal algorithm has information about
                 the future evolution of the computation that is not
                 available to the online algorithm. This is quite
                 remarkable considering extreme cases were chosen that
                 have contradicting optimal executions. Moreover, they
                 show that no other online algorithm can be consistently
                 better than it. They also present experimental results
                 that demonstrate the effectiveness of the algorithm.",
}

@Article{ABR97,
  author = 	 {Aharoni, G. and Barak, A. and Ronen, A.},
  title = 	 {{A Competitive Algorithm for Managing Sharing in the
                  Distributed Execution  of Functional Programs}},
  journal = 	 JFP,
  year = 	 1997,
  volume =	 7,
  number =	 4,
  month =	 {July},
  annote =	 {To appear; Check it!}
}

@InProceedings{Kenn94,
  author = 	 {Kennaway, J.R.},
  title = 	 {{A Conflict Between Call-by-Need Computation and Parallelism}},
  booktitle = 	 {{Workshop on Conditional Term Rewriting Systems}},
  pages =	 {247--261},
  year =	 1994,
  volume =	 968,
  series =	 LNCS,
  address =	 {Jerusalem, Israel},
  publisher =	 S-V, 
  url =          {ftp://ftp.sys.uea.ac.uk/pub/kennaway/publications/rootseq.ps.Z},
}

@InProceedings{TrGa95,
  author = 	 {Tremblay, G. and  Gao, G.R.},
  title = 	 {{The impact of laziness on parallelism and the limits of strictness analysis}},
  year =         1995,
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  pages =        {119--133},
  address =	 {Denver, Colorado, April 10--12},
  abstractURL =  {http://www-acaps.cs.mcgill.ca/~tremblay/TremblayGao95.html},
  documentURL =  {ftp://sisal.llnl.gov/pub/hpfc/papers95/paper11.ps},
}

@inproceedings{FoGo96,
 author       =   {Fournet, C and Gonthier, G.},
 title        =   {{The Reflexive Chemical Abstract Machine 
                  and the Join-Calculus}},
 address     ={St Petersburg, Florida},
 publisher   ={ACM},
 OPTeditor   ={{Steele Jr.}, G.L.},
 volume      ={23},
 booktitle   ={{POPL'96 --- Symposium on Principles of Programming
                   Languages}},
 month       ={January},
 year        ={1996},
 pages       ={372--385},
 year        ={1996},
 url         ={http://pauillac.inria.fr/~fournet/papers/popl-96.ps.gz},
 abstract    ="
By adding reflexion to the chemical machine  of Berry and Boudol, we obtain
a formal  model  of   concurrency  that  is consistent   with mobility  and
distribution. Our model provides the foundations  of a programming language
with  functional and  object-oriented features. It  can  also be seen as  a
process  calculus, the  join-calculus, which  we   prove equivalent  to the
$\pi$-calculus of Milner, Parrow and Walker."
                  }

% ---------------------------------------------------------------------------
%$%node Simulators, Functional Algorithms, Theoretical work, Top
%$%section Simulators
% ---------------------------------------------------------------------------

@InProceedings{RuWa93,
  author = 	 {Runciman, C. and Wakeling, D.},
  title = 	 {{Profiling Parallel Functional Computations (without
                  Parallel Machines)}},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  year =	 1993,
  publisher =	 S-V,
  address =	 {Ayr, Scotland, July 5--7},
  editor = 	 {{O'Donnell}, J.T. and Hammond, K.},
  series =	 {Workshops in Computing},
  pages =	 {236--251},
}

@PhdThesis{Benn93,
  author = 	 {Bennett, A.J.},
  title = 	 {{Parallel Graph Reduction for Shared-Memory Architectures}},
  school = 	 {Department of Computing},
  year = 	 1993,
  address =	 {Imperial College, London}
}

@TechReport{GHW90,
  author = 	 {Glaser, H. and Hartel, P. and Wild, J.},
  title = 	 {{A Pragmatic Approach to the Analysis and Compilation of
                  Lazy Functional Languages}},
  institution =  {Department of Electronics and Computer Science},
  year = 	 1990,
  number =	 {CSTR 90-10},
  address =	 {University of Southampton},
  annote =	 {About FAST compiler}
}

@TechReport{KLB82,
  author = 	 {Keller, R.M. and Lin, F.C.H. and Badovinatz, P.R.},
  title = 	 {{The Rediflow Simulator}},
  institution =  {Department of Computer Science},
  year = 	 1982,
  address =	 {{University of Utah}},
  annote =	 {Yet another simulator}
}


@TechReport{Wats89,
  author = 	 {Watson, I.},
  title = 	 {{Simulation of a Physical EDS Machine Architecture}},
  institution =  {Department of Computer Science},
  year = 	 1989,
  address =	 {University of Manchester},
  month =	 {September},
  annote =	 {Simulator specific to EDS hardware}
}


@PhdThesis{Eeke88,
  author = 	 {{van Eekelen}, M.},
  title = 	 {{Parallel Graph Rewriting}},
  school = 	 {University of Nijmegen},
  year = 	 {1988},
  month =	 {December}
}

@inproceedings{Desc89,
author = {Deschner, J.M.},
title = {{Simulating Multiprocessor Architectures for Compiled
Graph-Reduction}}, 
booktitle = {Functional Programming: Proceedings of the 1989 Glasgow Workshop},
address = {Fraserburgh, Scotland, August 21--23},
year = {1989},
editor = {Davis, K. and Hughes, R.J.M.},
series =  {Workshops in Computing},
publisher = {Springer},
pages = {225--237},
abstract = {Software simulations of hardware architectures are useful tools
for evaluating a systems performance prior to its construction. This is
especially true of multi-processor architectures where the design complexity
is increased by orders of magnitude. It is also necessary to be able to
simulate accurately the behavior of a parallel machine after it has been
built. Statistics may then continue to be gathered without adversely affecting
the behavior of the hardware. This paper describes in detail a system for
simulating the parallel execution of functional programs based on compiled
graph-reduction.},
keywords = {Parallel Graph Reduction, Simulation},
}

@TechReport{Mora86,
  author = 	 {Morais, D.R.},
  title = 	 {{Id World: An Environment for the Development of
                  Dataflow Programs Written in Id}},
  institution =  {Laboratory of Computer Science},
  year = 	 1986,
  number =	 {MIT-LCS-TR-365},
  address =	 {M.I.T.},
  month =	 {May},
  annote =	 {About simulator and environment}
}

% ---------------------------------------------------------------------------
%$%node  Functional Algorithms, Parallel Fctal Pging, Simulators, Top
%$%section Functional Algorithms
% ---------------------------------------------------------------------------

%$%menu
%* Functional Programming::	
%* General Algorithms::		
%* Skeletons::			
%$%end menu

%$%node Functional Programming, General Algorithms, Functional Algorithms, Functional Algorithms
%$%subsection Functional Programming

@Article{Hugh89,
  author = 	 {Hughes, R.J.M.},
  title = 	 {{Why Functional Programming Matters}},
  journal =      {The Computer Journal},
  year =	 {1989},
  volume =	 {32},
  number =	 {2},
  pages =	 {98--107},
  month =	 {April},
  url =          {http://www.cs.chalmers.se/\~{}rjmh/Papers/whyfp.ps},
  abstractURL =  {http://www.cs.chalmers.se/\~{}rjmh/Papers/whyfp.html},
  abstract =     "As software becomes more and more complex, it is more and more important to
structure it well. Well-structured software is easy to write, easy to debug,
and provides a collection of modules that can be re-used to reduce future
programming costs. Conventional languages place conceptual limits on the way
problems can be modularised. Functional languages push those limits back. In
this paper we show that two features of functional languages in particular,
higher-order functions and lazy evaluation, can contribute greatly to
modularity. As examples, we manipulate lists and trees, program several
numerical algorithms, and implement the alpha-beta heuristic (an algorithm
from Artificial Intelligence used in game-playing programs). Since modularity
is the key to successful programming, functional languages are vitally
important to the real world."
}

@Book{FiHa88,
  author = 	 {Field, A.J. and Harrison, P.G.},
  title = 	 {{Functional Programming}},
  publisher = 	 {Addison-Wesley},
  year = 	 {1988}
}

@Book{BiWa88,
  author = 	 {Bird, R.S. and Wadler, P.},
  title = 	 {{Introduction to Functional Programming}},
  publisher = 	 P-H,
  year = 	 {1988}
}


%$%node General Algorithms, Skeletons, Functional Programming, Functional Algorithms
%$%subsection General Algorithms

@Book{RaLa97,
  author = 	 {Rabhi, F.A. and Lapalme, G.},
  title = 	 {{Algorithms: A Functional Programming Approach}},
  publisher = 	 P-H,
  year = 	 1997,
  month =	 {September},
  annote =	 {algorithms in functional languages},
  keywords =     {Algorithms},
  abstractURL =  {http://www.enc.hull.ac.uk/CS/parallel/publications/far/book.ps}
}

@Article{BrOk96,
  author = 	 {Brodal, G.S. and Okasaki, C.},
  title = 	 {{Optimal Purely Functional Priority Queues}},
  journal = 	 JFP,
  year = 	 {1996},
  volume = 	 {6},
  number = 	 {6},
  pages =        {839--857},
  month = 	 dec,
  descr =        {algorithms (functional)},
  url =  {http://foxnet.cs.cmu.edu/people/cokasaki/priority.ps},
  sourceCodeURL = {http://foxnet.cs.cmu.edu/people/cokasaki/priority.html},
  abstractURL =  {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#priority},
  abstract =     {Brodal recently introduced the first implementation of imperative priority
queues to support findMin, insert, and meld in O(1) worst-case time, and
deleteMin in O(log n) worst-case time. These bounds are asymptotically
optimal among all comparison-based priority queues. In this paper, we adapt
Brodal's data structure to a purely functional setting. In doing so, we
both simplify the data structure and clarify its relationship to the
binomial queues of Vuillemin, which support all four operations in O(log n)
time. Specifically, we derive our implementation from binomial queues in
three steps: first, we reduce the running time of insert to O(1) by
eliminating the possibility of cascading links; second, we reduce the
running time of findMin to O(1) by adding a global root to hold the minimum
element; and finally, we reduce the running time of meld to O(1) by
allowing priority queues to contain other priority queues. Each of these
steps is expressed using ML-style functors. The last transformation, known
as data-structural bootstrapping, is an interesting application of
higher-order functors and recursive structures.
                 }
}

@InProceedings{Brod96,
  author = 	 {Brodal, G.S.},
  title = 	 {{Worst-Case Priority Queues}},
  booktitle = 	 {{SODA'96 --- Symposium on Discrete Algorithms}},
  year =	 1996,
  organization = {ACM SIAM},
  month =	 jan,
  pages =	 {52--58},
  annote =	 {algorithms (imperative)},
  url =          {http://www.mpi-sb.mpg.de/\~{}brodal/Papers/soda96.ps.gz},
  abstractURL =  {http://www.daimi.aau.dk/\~{}gerth/publications.html#soda96},
  abstract =     {An implementation of priority queues is presented that
     supports the operations MakeQueue, FindMin, Insert, Meld and
     DecreaseKey in worst case time O(1) and DeleteMin and Delete in worst
     case time O(log n). The space requirement is linear. The data
     structure presented is the first achieving this worst case
     performance.}
}


@Article{ARS94,
  author = 	 {Augustsson, L. and Rittri, M. and Synek, D.},
  title = 	 {{On Generating Unique Names}},
  journal = 	 JFP,
  year = 	 1994,
  volume =	 4,
  number =	 1,
  pages =	 {117--123},
  month =	 {January},
  annote =	 {used in \cite{Juna98}}
}

% ---------------------------------------------------------------------------
%$%node Skeletons,  , General Algorithms, Functional Algorithms
%$%subsection Skeletons
% ---------------------------------------------------------------------------

%$%cindex Skeletons

@Book{Cole89,
  author = 	 {Cole, M.},
  title = 	 {{Algorithmic Skeletons: Structured Management of Parallel
                   Computation}}, 
  publisher = 	 MIT,
  year = 	 {1989}
}

@inproceedings{DGTY95,
        author = {Darlington, J. and Guo, Y.K. and To, H.W. and Yang, J.},
        title = {{Skeletons for Structured Parallel Composition}},
        booktitle = {{PPoPP'95 --- Symposium on Principles and Practice of  Parallel Programming}},
        address = {Santa Barbara, CA, July†19-21},
        organization = {ACM SIGPLAN},
        year = {1995},
        pages = {19--28},
        url = {http://www-ala.doc.ic.ac.uk/papers/Y.Guo/ppopp.ps.Z},
        abstract = "In  this paper, we propose  a straightforward solution   to the problems of
compositional  parallel   programming by using  skeletons  as   the uniform
mechanism for structured composition. In our approach parallel programs are
constructed by composing procedures in a conventional base language using a
set  of high-level, predefined,  functional,  parallel  computational forms
known as skeletons.  The ability to compose  skeletons provides us with the
essential tools for  building further and more complex application-oriented
skeletons specifying  important  aspects of parallel computation.  Compared
with  the process  network  based composition  approach,  such  as PCN, the
skeleton  approach   abstracts  away  the   fine  details   of   connecting
communication  ports to    the  higher  level mechanism    of   making data
distributions  conform, thus avoiding the complexity  of  using lower level
ports as the means of  interaction. Thus, the  framework provides a natural
integration of    the compositional programming    approach  with the  data
parallel programming paradigm."
}

@inproceedings{DGTY95a,
        author = {Darlington, J. and Guo, Y.K. and To, H.W. and Yang, J.},
        title = {{Functional Skeletons for Parallel Coordination}},
        booktitle = {{EUROPAR'95}}, 
        year = {1995},
        keywords = {Programming Language, Parallel Computing, Skeleton, Coordination Language, High Performance Fortran},
        url = {http://www-ala.doc.ic.ac.uk/papers/Y.Guo/europar.ps.Z},
        abstract = "In this paper we propose  a methodology for structured parallel programming
using functional skeletons to compose and co-ordinate concurrent activities
themselves defined in a standard  imperative language. Skeletons are higher
order functional forms  with built-in parallel behaviour.  We show how such
forms can be used uniformly to abstract all aspects of a parallel program's
behaviour  including   data   partitioning,   placement  and re-arrangement
(communication)   as well as  computation.    Skeletons are naturally  data
parallel and are  capable of expressing computation  and co-ordination at a
higher level than  other  process oriented co-ordination   notations. Being
functional skeletons inherit all the desirable properties of this paradigm:
abstraction,  modularity and   transformation  and  we   show how   program
transformation can    be   used to    achieve optimisation     of  parallel
programs. Examples of the application of this methodology  are given and an
implementation technique outlined."
}

@PhdThesis{Pela93,
  author = 	 {Pelagatti, S.},
  title = 	 {{A Methodology for the Development and the Support of
                  Massively Parallel Programs}},
  school = 	 {Universita Delgi Studi Di Pisa},
  year = 	 1993,
  annote =	 {about P3L; composable skeletons somewhat similar to SCL},
}

@InProceedings{Rabh93,
  author = 	 {Rabhi, F.A.},
  booktitle = 	 {{Workshop on Abstract Machine Models for Highly Parallel Computers}},
  title = 	 {{Exploiting Parallelism in Functional Languages: a
                  ``Paradigm-Oriented'' Approach}},
  publisher = 	 {Oxford University Press},
  year = 	 1993,
  editor =	 {Dew, P. and Lake, T.},
  pages =	 {118--139},
  url =  {http://www.enc.hull.ac.uk/\~{}far/oup.ps.gz},
  abstract =     "Deriving parallelism automatically from functional programs is simple in
theory but very few practical implementations have been realised. Programs
may contain too little or too much parallelism causing a degradation in
performance. Such parallelism could be more efficiently controlled if
parallel algorithmic structures (or skeletons) are used in the design of
algorithms. A structure captures the behaviour of a parallel programming
paradigm and acts as a template in the design of an algorithm. This paper
presents some important parallel programming paradigms and defines a
structure for each of these paradigms. The iterative transformation
paradigm (or geometric parallelism) is discussed in detail and a framework
under which programs can be developed and transformed into efficient and
portable implementations is presented."
}

@InProceedings{Rabh95,
  author = 	 {Rabhi, F.A.},
  title = 	 {{A Parallel Programming Methodology Based on Paradigms}},
  booktitle = 	 {{Transputer and Occam Developments}},
  year =	 1995,
  publisher =	 {IOS Press},
  pages =	 {239--252},
  keywords =     {skeletons},
  url =          {http://www.enc.hull.ac.uk/\~{}far/methodology.ps.gz},
  abstract =     "Today's efforts are  mainly concentrated on providing ``standard'' parallel
languages   to    ensure the   portability   of   programs  across  various
architectures. It is  now believed that the next  level of abstraction that
will be addressed is the application level. This paper argues that there is
an intermediate  level    that  consist of   common parallel    programming
paradigms. It  describes some  of  these paradigms  and explains  the basic
principles behind a ``paradigm-oriented'' programming approach. Finally, it
points to  future directions which can make  it  feasible to build parallel
CASE tools that achieve automatic parallel code generation."
}

@InProceedings{ScRa96,
  author = 	 {Schwarz, J. and Rabhi, F.A.},
  title = 	 {{A Skeleton-Based Implementation of Iterative
                    Transformation Algorithms Using Functional Languages}},
  booktitle = 	 {{Abstract Machine Models for Parallel and Distributed Computing}},
  year =	 1996,
  publisher =	 {IOS Press},
  month =	 {April},
  pages =	 {119--133},
  keywords =     {skeletons},
  url =          {http://www.enc.hull.ac.uk/\~{}cspjs/leeds.ps.gz},
  abstract =     "This paper  is concerned  with  the ``skeleton-oriented'' approach   in the
design  and  implementation of parallel  programs.  In particular, the link
between  the  declarative    meaning  and the behaviour    of  skeletons is
investigated.  With the   static   iterative transformation SIT    skeleton
selected as a case study,  this paper shows a  structured approach for  the
implementation  of  this skeleton,   using  an underlying  graph  reduction
mechanism, assisted by the use of monads for state manipulation and the PVM
standard for communication handling."
}

@InProceedings{BoKu96,
  author = 	 {Botorog, G.H. and  Kuchen, H.},
  title = 	 {{Skil: An Imperative Language with Algorithmic Skeletons
                  for Efficient Distributed Programming}}, 
  booktitle = 	 {HPDC'96 --- International Symposium on High Performance Distributed 
                  Computing},
  year =	 1996,
  publisher =	 {IEEE Computer Society Press},
  pages =	 {243--252},
  keywords =     {skeletons},
  url =  {http://www-i2.informatik.rwth-aachen.de/botorog/Papers/hpdc96.ps.gz},
  abstractURL =  {http://www-i2.informatik.rwth-aachen.de/botorog/Papers/hpdc96-abs.html},
  abstract =     "In this paper we present Skil, an imperative language enhanced with
higher-order functions and currying, as well as with a polymorphic type
system. The high level of Skil allows the integration of algorithmic
skeletons, i.e. of higher-order functions representing parallel computation
patterns. At the same time, the language can be efficiently
implemented. After describing a series of skeletons which work with
distributed arrays, we give two examples of parallel programs implemented
on the basis of skeletons, namely shortest paths in graphs and Gaussian
elimination. Run-time measurements show that we approach the efficiency of
message-passing C up to a factor between 1 and 2.5."
}

@InProceedings{BoKu96a,
  author = 	 {Botorog, G.H. and  Kuchen, H.},
  title = 	 {{Using Algorithmic Skeletons with Dynamic Data Structures}},
  booktitle = 	 {Irregular'96},
  volume =	 1117,
  series =	 LNCS,
  year =	 1996,
  publisher =	 {Springer Verlag},
  pages =	 {263--276},
  keywords =     {skeletons},
  url =  {http://www-i2.informatik.rwth-aachen.de/botorog/Papers/irr96.ps.gz},
  abstractURL =  {http://www-i2.informatik.rwth-aachen.de/botorog/Papers/irr96-abs.html},
  abstract =     "Algorithmic skeletons are polymorphic higher-order functions representing
common parallelization patterns. A special category are data-parallel
skeletons, which perform operations on a distributed data structure. In
this paper, we consider the case of distributed data structures with
dynamic elements. We present the enhancements necessary in order to cope
with these data structures, both on the language level and in the
implementation of the skeletons. Further, we show that these enhancements
practically do not affect the user, who merely has to supply two additional
functional arguments to the communication skeletons. We then implement a
parallel sorting algorithm using dynamic data with the enhanced skeletons
on a MIMD distributed memory machine. Run-time measurements show that the
speedups of the skeleton-based implementation are comparable to those
obtained for a direct C implementation."
}

@InProceedings{BoKu96b,
  author = 	 {Botorog, G.H. and Kuchen, H.},
  title = 	 {{Efficient Parallel Programming with Algorithmic Skeletons}},
  booktitle = 	 {EuroPar'96 --- European
                  Conference on Parallel Processing},
  address =      {Lyon, France, 26--29 August},
  volume =	 1123,
  series =	 LNCS,
  year =	 1996,
  publisher =	 S-V,
  pages =	 {718--731},
  keywords =     {skeletons},
  url =  {http://www-i2.informatik.rwth-aachen.de/botorog/Papers/europar96.ps.gz},
  abstractURL =  {http://www-i2.informatik.rwth-aachen.de/botorog/Papers/europar96-abs.html},
  abstract =     "Algorithmic skeletons are polymorphic higher-order functions representing
common parallelization patterns and implemented in parallel. They can be
used as the building blocks of parallel and distributed applications by
integrating them into a sequential language. In this paper, we present a
new approach to programming with skeletons. We integrate the skeletons into
an imperative host language enhanced with higher-order functions and
currying, as well as with a polymorphic type system. We thus obtain a
high-level programming language which can be implemented very
efficiently. After describing a series of skeletons which work with
distributed arrays, we give two examples of parallel algorithms implemented
in our language, namely matrix multiplication and a statistical numerical
algorithm for solving partial differential equations. Run-time measurements
show that we approach the efficiency of message-passing C up to a factor
between 1 and 1.75."
}

@PhdThesis{Stol94,
  author = 	 {Stoltze, H.},
  title = 	 {{Implementierung einer parallelen funktionalen Sprache mit
                   algorithmischen Skeletten zur Lˆsung mathematisch-technischer Probleme}},
  school = 	 {RWTH Aachen},
  year = 	 1994,
  keywords =     {skeletons}
}

%$%cindex EDEN

@InProceedings{BLOP96,
  author = 	 {Breitinger, S. and Loogen, R. and {Ortega-Mallen}, Y. and
                  {Pena-Mari}, R.},
  title = 	 {{EDEN --- The Paradise of Concurrent Functional Programming}},
  booktitle = 	 {EuroPar'96 --- European
                  Conference on Parallel Processing},
  volume =	 1123,
  series =	 LNCS,
  year =	 1996,
  publisher =	 {Springer Verlag},
  keywords =     {concurrency, lazy languages},
  url =          {http://dalila.sip.ucm.es/funcional/publicaciones/EUROPAR-96.ps},
  abstract =     "The functional concurrent
     language Eden is an extension of the lazy functional language Haskell
     by constructs for the explicit specification of dynamic process
     systems. It employs stream-based communication and is tailored for
     distributed memory systems. Eden supports and facilitates the task of
     parallel and concurrent programming. To illustrate this statement the
     paper includes elegant solutions to traditional concurrency problems."
}

@InProceedings{BLOP96a,
  author = 	 {Breitinger, S. and Loogen, R. and Ortega-Mall\'{e}n,
                  Y. and Pena, R.},
  title = 	 {{The Eden Coordination Model for Distributed Memory Systems}},
  booktitle = 	 {{Workshop on Multi-Paradigm Logic Programming}},
  year =	 1996,
  month =	 {September},
  pages =        {25--34},
  url =  {http://marvin.cs.tu-berlin.de/\~{}chak/mplp/proc/breitinger.ps.gz},
  abstract =     "Eden is a concurrent declarative language that aims at both the programming
of    reactive  systems and  parallel    algorithms  on distributed  memory
systems. In this  paper, we explain  the computation and coordination model
of Eden. We show    how lazy evaluation  in   the computation language   is
fruitfully combined with  the coordination language that is  specifically
designed for multicomputers and  that   aims at maximum  parallelism.   The
two-level  structure of the   programming  language is reflected in   its
operational semantics, which is sketched shortly."
}

@InProceedings{GPP96,
  author = 	 {Gal\'an, L.A. and Pareja, C. and Pen\~a, R.},
  title = 	 {{Functional Skeletons Generate Process Topologies in Eden}},
  booktitle = 	 {{PLILP'96 --- Symposium on
                  Programming Language Implementation and Logic Programming}},
  OPTnumber = 	 {},
  OPTseries = 	 LNCS,
  year = 	 {1996},
  OPTpublisher = S-V,
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTpages = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {presents several common skeletons in Eden}
}

@InProceedings{LCL99,
  author = 	 {Lima, R.M.F. and Carvalho, F.H. and Lins, R.D.},
  title = 	 {{Haskell\#: A Message Passing Extension to Haskell}},
  booktitle =	 {CLaPF'99 --- Latin-American Conference on Functional Programming},
  year =	 1999,
  address =	 {Recife, Brazil},
  month =	 mar
}

% ---------------------------------------------------------------------------
%$%node Parallel Fctal Pging, Implementation of Functional Languages, Functional Algorithms, Top
%$%section Parallel Fctal Pging
% ---------------------------------------------------------------------------

%$%cindex Caliban

@Book{Kell89,
  author = 	 {Kelly, P.H.J.},
  title = 	 {{Functional Programming for Loosely-Coupled Multiprocessors}},
  publisher = 	 MIT,
  year = 	 {1989},
  series =	 {Research Monographs in Parallel and Distributed Computing},
  annote =	 {parallel functional, Caliban}
}

@PhdThesis{Tayl97,
  author = 	 {Taylor, F.S.},
  title = 	 {{Parallel Functional Programming by Partitioning}},
  school = 	 {Department of Computing},
  year = 	 1997,
  address =	 {Imperial College, London},
  url =          {http://www.lieder.demon.co.uk/thesis/thesis.ps.gz},
}

@PhdThesis{Roe91,
  author = 	 {Roe, P.},
  title = 	 {{Parallel Programming using Functional Languages}},
  school = 	 {Department of Computing Science},
  year = 	 {1991},
  address =	 {University of Glasgow},
  month =	 {February},
  url =          {http://www.fit.qut.edu.au/\~{}proe/papers/thesis.ps.gz}
}

@Misc{Tayl??,
  OPTkey = 	 {},
  OPTauthor = 	 {Taylor, F.S.},
  OPTtitle = 	 {{The Design and Implementation of the Caliban Compiler Phases }},
  OPThowpublished = {},
  OPTmonth = 	 {},
  OPTyear = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  abstract =     "
This    document  describes an implementation    of  Caliban, the front-end
language   for  the Fast  compiler. It  shows  the transformations  used to
translate the annotated source language  to standard Haskell$^-$  augmented
with a parallel primitive."
}

@InProceedings{TBK96,
  author = 	 {Talbot, S.A.M. and Bennett, A.J. and Kelly, P.H.J.},
  title = 	 {{Cautious, Machine-Independent Performance Tuning for Shared-Memory Multiprocessors}},
  booktitle = 	 {{EuroPar'96 --- European Conf. on Parallel Processing}},
  address =      {Lyon, France, 26--29 August},
  volume =	 1123,
  pages =	 {106ff},
  year =	 1996,
  series =	 LNCS,
  publisher =	 S-V,
  url =          {http://www.doc.ic.ac.uk/\~{}phjk/Publications/CautiousMachineIndependent..EuroPar97.ps.gz},
  abstract =     "
Coherent-cache  shared-memory    architectures  often   give  disappointing
performance  which can be  alleviated by manual  tuning.  We describe a new
trace analysis  tool, clarissa, which  helps diagnose problems and pinpoint
their causes. Unusually, clarissa works  by analysing potential contention,
instead of measuring predicted  contention by simulating a specific  memory
system  design. This is important  because, after tuning, the software will
be executed on  different inputs and different  configurations. The goal is
to  produce a program with  robustly good performance.  This paper explains
the principle behind cautious trace analysis, describes our implementation,
and presents our experience of using the tool."
}

@InProceedings{BKP94,
  author = 	 {Andrew J. Bennett, Paul H. J. Kelly and Ross Paterson},
  title = 	 {{Derivation and Performance of a Pipelined Transaction Processor}},
  booktitle = 	 {IEEE Symposium on Parallel and Distributed Processing,},
  year =	 1994,
  address =	 {Dallas, TX},
  month =	 {December},
  publisher =	 {IEEE Press},
  url =          {http://www.doc.ic.ac.uk/\~{}phjk/Publications/DerivationAndPerformanceOfPipelined..SPDP94.ps.gz},
  abstract =     "
Transaction  processing can be formulated  as  a simple functional  program
operating on  a   stream  of transaction requests  and    a tree-structured
database.  In this paper  we  use algebraic  transformation of  the initial
program to    yield  an  optimistic  implementation in    which unnecessary
synchronization is  eliminated,  thereby allowing  concurrent processing of
transactions.   A  detailed  simulation  is used    to study the  program's
behaviour and to assess scheduling policies based on the characteristics of
the target   architecture.  Our results show   that  good speedups  can  be
achieved, and that transformation can be used to derive a highly concurrent
program with better locality and grain size."
}

%$%cindex NESL

@Article{Blel96,
  author = 	 {Blelloch, G.E.},
  title = 	 {{Programming Parallel Algorithms}},
  journal = 	 CACM,
  year = 	 1996,
  volume =	 39,
  number =	 3,
  month =	 {March},
  pages =	 {85--97},
  url =          {http://www.cs.cmu.edu/\~{}scandal/cacm.html},
  abstractURL =  {http://www.cs.cmu.edu/\~{}scandal/cacm/cacm2.html},
  abstract =     "In the past 20 years there has been tremendous progress in developing and
analyzing parallel algorithms. Researchers have developed efficient
parallel algorithms to solve most problems for which efficient sequential
solutions are known. Although some of these algorithms are efficient only
in a theoretical framework, many are quite efficient in practice or have
key ideas that have been used in efficient implementations. This research
on parallel algorithms has not only improved our general understanding of
parallelism, but in several cases has led to improvements in sequential
algorithms. Unfortunately there has been less success in developing good
languages for programming parallel algorithms, particularly languages that
are well suited for teaching and prototyping algorithms. There has been a
large gap between languages that are too low level, requiring specification
of many details that obscure the meaning of the algorithm, and languages
that are too high-level, making the performance implications of various
constructs unclear. In sequential computing many standard languages such as
C or Pascal do a reasonable job of bridging this gap, but in parallel
languages building such a bridge has been significantly more difficult.

Our research involves developing a parallel language that is useful for
teaching as well as for implementing parallel algorithms. To achieve this,
an important goal has been to develop a language that allows high-level
descriptions of parallel algorithms while also having a well understood
mapping onto a performance model (i.e. bridges the gap). Based on our
research, we believe that the following two features are important for
achieving this goal:


   o A language-based performance model that uses work and depth rather
     than a machine-based model that uses ``running time''.
   o Support for nested data-parallel constructs. This is the ability to
     apply a function in parallel to each element of a collection of data,
     and the ability to nest such parallel calls.

In this article we describe these features and explain why they are
important for programming parallel algorithms. To make the ideas concrete,
we describe the programming language NESL [5], which we designed based on
the features, and go through several examples of how to program and analyze
parallel algorithms using the language. We have been using NESL for three
years in undergraduate and graduate courses on parallel algorithms [8]. The
algorithms we cover in this article are relatively straightforward. Many
more algorithms can be found through the Web version of this article."
}


@TechReport{Blel93,
  author = 	 {Blelloch, G.E.},
  title = 	 {{NESL: A Nested Data-Parallel Language (Version 2.6)}},
  institution =  {Carnegie-Mellon University},
  year = 	 1993,
  type =	 {Technical Report},
  number =	 {CMU-CS-93-129},
  month =	 {April}
}

@TechReport{Blel95,
  author = 	 {Blelloch, G.E.},
  title = 	 {{NESL: A Nested Data-Parallel Language (Version 3.1)}},
  institution =  {Carnegie-Mellon University},
  year = 	 1995,
  type =	 {Technical Report},
  number =	 {CMU-CS-95-170},
  month =	 {September},
  keywords =     {Data-parallel, parallel algorithms, supercomputers, nested parallelism, PRAM model, parallel programming languages, collection-oriented languages.},
  url =          {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-95-170.ps.gz},
  abstract =     "
This report  describes  Nesl, a strongly-typed,  applicative, data-parallel
language. Nesl  is  intended  to  be  used   as a portable  interface   for
programming a variety of parallel and vector computers, and  as a basis for
teaching parallel algorithms. Parallelism is  supplied through a simple set
of data-parallel constructs  based on sequences,  including a mechanism for
applying any function over  the elements of  a  sequence in parallel  and a
rich set of parallel functions that manipulate sequences.

Nesl fully supports nested sequences and nested parallelism --- the ability
to  take  a  parallel  function and  apply   it over multiple  instances in
parallel. Nested parallelism  is important for implementing algorithms with
irregular nested loops  (where the inner loop lengths  depend  on the outer
iteration)   and for divide-and-conquer   algorithms. Nesl  also provides a
performance model  for calculating the  asymptotic performance of a program
on various parallel machine  models. This is  useful for estimating running
times of algorithms on  actual machines and,  when teaching algorithms, for
supplying  a  close  correspondence between the   code  and the theoretical
complexity.

This report defines Nesl and describes several examples of algorithms coded
in  the language.  The  examples include   algorithms for median   finding,
sorting, string   searching, finding prime numbers,   and  finding a planar
convex  hull. Nesl currently  compiles to  an  intermediate language called
Vcode,   which runs  on   vector multiprocessors (the    CRAY C90  and J90)
distributed  memory  machines (the  IBM  SP2, Intel Paragon, and Connection
Machine CM-5) and sequential workstations. For many algorithms, the current
implementation gives  performance close to  optimized machine-specific code
for these machines.

Note: This report  is an updated version  of CMU-CS-92-103, which described
version 2.4 of the language, and of  CMU-CS-93-129, which described version
2.6 of the language.

Some other documents that describe Nesl are:
 * The user's manual [11].
 * An overview of the implementation with some timing results [8].
 * A formal definition of the Nesl cost model [23]."
}

%$%cindex HDG

@Article{KLB91,
  author = 	 {Kingdon, H. and Lester, D.R. and Burn, G.},
  title = 	 {{The HDG-machine: a Highly Distributed Graph-Reducer for a
		  Transputer Network}},
  journal =	 compj,
  year =	 {1991},
  volume =	 {34},
  number =	 {4},
  pages =	 {290--301},
  owner =        {pcl},
  descr =        {plfun,phred},
  url =          {http://theory.doc.ic.ac.uk/tfm/papers/BurnGL/HDGmachine.ps.gz},
  abstract =     "Distributed   implementations   of programming   languages   with  implicit
parallelism    hold  out the   prospect   that   the parallel programs  are
immediately scalable. This paper  presents some of  the results of our part
of Esprit 415, in which we considered the implementation of lazy functional
programming languages on distributed architectures. A compiler and abstract
machine  were designed to achieve  this goal. The abstract parallel machine
was formally specified, using Miranda."
}

%$%cindex pired

@InProceedings{GKK92,
  author =	{GÅ‰rtner, D. and Kimms,  A. and Kluge,  W.},
  title =	{{$\pi$-{\sc red$^+$} --- A Compiling Graph-Reduction System for
                  a Full-Fledged $\lambda$-Calculus}}, 
  OPTeditor =       {Kuchen, H. and Loogen, R.},
  booktitle =    {{IFL'92 ---  International Workshop on the Parallel
		  Implementation of Functional Languages}},
  year =	 {1992},
  address =	 {RWTH Aachen, Germany},
  month =	 {September}
}

@InProceedings{BHK*94,
  author =	{BÅ¸lck, T. and Held, A. and  Kluge,  W. and Pantke,
		    S. and Rathsack, A. and Scholz, S-B. and SchrÅˆder, R.},
  title =	{{Experience with the Implementation of a Concurrent Graph 
		  Reduction System on an nCube/2 Platform}}, 
  OPTeditor =	 {Buchberger, B. and Volkert, J.},
  series =	 LNCS,
  number =	 {854},
  booktitle =    {{CONPAR'94 --- Conference on Algorithms and Hardware for
		  Parallel Processing}},
  pages =        {497--508},
  year =	 {1994},
  address =	 {Linz, Austria, September 6--8}
}
		  
%   o PI-Red+ A compiling graph reduction system for a full fledged
%     Lambda-Calculus
%        o By D.Gartner and W.E Kluge

@Article{GaKl96,
  author   = {GÅ‰rtner, D. and Kluge, W.E.},
  title    = {$\pi$-{\sc red$^+$}---{A}n {I}nteractive {C}omputing {G}raph
              {R}eduction {S}ystem for an {A}pplied $\lambda$-{C}alculus},
  journal  = JFP,
  year     = 1996,
  month    = {September},
  url= {},
  topics   = {PI-RED}
  }

@PhdThesis{Scho96,
  author = 	 {Scholz, S-B.},
  title = 	 {{Single Assignment C -- Entwurf und Implementierung einer
        funktionalen C-Variante mit spezieller Unterst\"utzung shape-invarianter
        Array-Operationen (in German)}},
  school = 	 {Institut f\"ur Informatik und praktische Mathematik},
  year = 	 1996,
  address =	 {Universit\"at Kiel}
}

@InProceedings{Scho??,
  author = 	 {Scholz, S-B.},
  title = 	 {{A Case Study: Effects of WITH-Loop-Folding on the NAS Benchmark MG in SAC}},
  booktitle = 	 {},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {IFL?}
}

@InProceedings{Grel98,
  author = 	 {Grelck, C.},
  title = 	 {{Shared Memory Multiprocessor Support for SAC}},
  booktitle = 	 {IFL'98 --- Intl. Workshop on the Implementation of
                  Functional Languages},
  year = 	 1998,
  series =	 {LNCS},
  volume =       1595,
  pages =        {38--54},
  OPTeditor =	 {Hammond, K. and Davie, A.J.T. and Clack, C.},
  address = 	 {September 9--11, University College London, UK},
  publisher =	 {Springer-Verlag},
  url =          {http://www.informatik.uni-kiel.de/\~{}sacbase/papers/mt-support-london.ps.gz},
  abstract =     "
SAC  (Single Assignment  C)  is  a strict,  purely functional   programming
language primarily designed with numerical applications in mind. Particular
emphasis  is on efficient  support  for arrays  both  in terms  of language
expressiveness and in terms of runtime performance. Array operations in SAC
are  based on elementwise specifications  using so-called With-loops. These
language constructs    are also well-suited     for concurrent execution on
multiprocessor systems. This paper outlines an implicit approach to compile
SAC    programs    for  multi-threaded     execution    on  shared   memory
architectures. Besides the  basic compilation scheme,  a brief overview  of
the runtime    system is given.  Finally,  preliminary  performance figures
demonstrate  that  this approach is  well-suited to   achieve almost linear
speedups.",
  summarySAC = "2 SAC --- Single Assignment C

This section  is  to give a   very brief overview  of SAC.  A more detailed
introduction to the language may be  found in [21,  24]; its strict, purely
functional semantics is formally defined in [20].

The core language of SAC may be considered a functional subset of C, ruling
out  global  variables and  pointers   to keep the   language free  of side
effects. It is  extended  by the   introduction  of arrays as  first  class
objects. An  array is  represented  by two   vectors:  a data vector  which
contains the  elements  of the  array,  and a  shape vector which  provides
structural information.  The length  of   the shape vector  specifies   the
dimensionality  of  the array  whereas its   elements   define the  array's
extension  in   each dimension. Built-in   functions  allow to determine an
array's dimension or shape and to extract array elements.

Complex array operations may be specified by means of so-called With loops,
a  versatile  language construct similar  to   the array comprehensions  of
Haskell  or  Clean   and  to   the  For-loops of   Sisal.   It  allows  the
dimension-invariant, elementwise definition of  operations on entire arrays
as well as on subarrays selected through index ranges or strides.

  WithExpr   => with ( Generator ) Operation 
  Generator  => Expr Relop Identifier Relop Expr [ Filter ]
  Relop      => < | <=
  Filter     => step Expr [ width Expr ]
  Operation  => genarray ( Expr ; Expr )
             |  modarray ( Expr ; Expr ; Expr )
             |  fold (FoldFun ; Expr ; Expr )

  Fig. 1.The syntax of With-loops.

The syntax of With-loops is outlined in Fig.1. A  With-loop consists of two
parts: a generator part and an operation part. The generator part defines a
set of index vectors along with an  index variable representing elements of
this set. Two expressions that  must evaluate to  vectors of equal  length,
define the  lower and the  upper bounds of a range   of index vectors. This
continuous  range may be  restricted by a filter  which  defines strides of
arbitrary widths. For  instance, with a, b, s,  and  w denoting expressions
that evaluate to vectors of  length n, ( a <=  i vec <  b step s width w  )
specifies the set of index vectors

  [ivec |  forall i in { 0, ..., n-1 }  : a_i <= ivec_i < b_i and 
                                          ( ivec_i - a_i ) modulo s_i < w_i ]

The operation part specifies the operation to be performed on each element
of the index vector set defined by the generator. Three different operation
parts  exist. Let  shp   and idx  denote Sac-expressions  that  evaluate to
vectors, let array denote a Sac-expression  that evaluates to an array, and
let expr denote an arbitrary Sac-expression. Moreover, let f  old op be the
name of a binary commutative  and associative function with neutral element
neutral. Then
 
 - genarray (shp ,expr) generates an array  of shape shp  whose elements
   are the values of expr for all index vectors from  the specified set, and 0
   otherwise;

 - modarray (array, idx, expr) defines an array of shape shape(array) whose
   elements are the values of  expr for all  index vectors from the  specified
   set, and the values of array[idx] at all other index positions;

 - fold (f old  op, neutral, expr)  allows  the specification of  reduction
   operations.  Setting   out with  neutral, for  each  index vector  from the
   specified set the value of expr is folded using f old op.

The expressive power of the With-loop allows the specification of a comprehensive array library for Sac in the language itself. This library provides numerous dimension and shape independent high-level array operations similar
 to those  available in Apl [15]  or Fortran-90 [1] as intrinsic functions,
e.g. extensions of binary scalar  operations to combinations of scalars and
arrays   as well as  to arrays  of equal shape  by elementwise application,
various types  of subarray selection,  concatenation  of arrays along given
axes,  shifting and   rotating arrays,  or  the reduction   operations sum,
product, any, and all.  Since this library  can  easily be extended  by any
application programmer, Sac   allows  high-level programming   without  the
restriction of a fixed set of built-in operations.",
}

@Article{Klug83,
  author = 	 {Kluge, W.E.},
  title = 	 {{Cooperating Reduction Machines}},
  journal = 	 {IEEE Transactions on Computers},
  year = 	 1983,
  volume =	 {C-32},
  pages =	 {1002--1012},
  annote =	 {Describes ticket mechanism and string reduction; basis
                  for parallel implementation in \cite{BHK*94}},
}

@InProceedings{Sarg93,
  author = 	 {Sargeant, J.},
  title = 	 {{Uniting Functional and Object-Oriented Programming}},
  booktitle = 	 {{Intl. Symposium on Object Technologies for Advanced Software}},
  pages =	 {1--26},
  year =	 1993,
  volume =	 742,
  series =	 {LNCS},
  address =	 {Kanazawa, Japan},
  month =	 nov
}

@InProceedings{Kess94a,
  author = 	 {Kesseler, M.},
  title = 	 {{Uniqueness and Lazy Graph Copying}},
  month =	 {September},
  booktitle =    {{IFL'94 ---  International Workshop on the Implementation of
                   Functional Languages}}, 
  year =	 {1994},
  address =	 {University of East Anglia, Norwich, U.K., September 7--9}
}

@InProceedings{Kess94,
  author = 	 {Kesseler, M.},
  title = 	 {{Reducing Graph Copying Costs}},
  booktitle =    {{PASCO'94 --- First International Symposium on Parallel Symbolic
		  Computation}},
  year = 	 {1994},
  publisher =	 {World Scientific Publishing},
  address =	 {Linz, Austria, September 26--28},
  url =          {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/kesm94-CopyCosts.ps.gz}
}

@InProceedings{Kess95,
  author = 	 {Marco Kesseler},
  title = 	 {{Constructing Skeletons in Clean: The Bare Bones}},
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  pages =        {182--192},
  address =	 {Denver, Colorado, April 10--12},
  year =         {1995},
  url =          {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/kesm95-skeletons.ps.gz},
  ALTurl =          {ftp://sisal.llnl.gov/pub/hpfc/papers95/paper30.ps},
  abstract =     "Skeletons are well-suited to structure parallel programming. They allow the
easy use of  some  well-known parallel programming paradigms   to construct
portable,efficient programs. Much  research has been  focused on the use of
skeletons in   functional   programming languages,    because  they can  be
expressed elegantly as  higher order functions. On  the  other hand, little
attention  has been paid  to an  elementary weakness of  skeletons:  how to
implement them.  In this paper we will show  that functional languages with
some  low  level constructs  for  parallelism  can be  used  to efficiently
implement a range of high level skeletons.  We will construct skeletons for
data parallelism,  for   parallel I/O, and   for  stream processing.    Our
experiments  demonstrate   that no performance  penalty  needs  to be paid,
compared to more restrictive solutions."
}

%$%cindex Clean

@PhdThesis{Kess96,
  author = 	 {Kesseler, M.},
  title = 	 {{The Implementation of Functional Languages on Parallel
                   Machines with Distributed Memory}},
  school = 	 {University of Nijmegen},
  year = 	 1996,
  annote =	 {Concurrent Clean; graph copying}
}


%$%cindex ZAPP

@InProceedings{BuSl81,
  author = 	 {Burton, F.W. and Sleep, M.R.},
  title = 	 {{Executing Functional Programs on a Virtual Tree of
                   Processors}}, 
  booktitle =    {{FPCA'81 --- Conference on Functional Programming Languages and
                   Computer Architecture}},
  year =	 1981,
  address =	 {Portsmouth, New Hampshire},
  month =	 {October},
  pages =	 {187--194}
}

@InProceedings{McSl87,
  author = 	 {McBurney, A. and Sleep, M.R.},
  title = 	 {{Transputer Based Experiments with the ZAPP Architecture}},
  booktitle =	 {PARLE'87 --- Parallel Architectures and Languages Europe},
  series =	 LNCS,
  number =	 {258},
  pages =        {242--259},
  year =	 {1987},
  address =	 {Eindhoven, The Netherlands, June 12--16},
  publisher =	 S-V,
}

@Book{McSl89,
  author =       "D. L. McBurney and M. R. Sleep",
  title =        "Experiments with a Virtual Tree Machine Using
                 Transputers",
  publisher =    "School of Information Systems, University of East
                 Anglia",
  address =      "Norwich, UK",
  year =         "1989",
  keywords =     "functional parallel zapp",
  abstract =     "A number of experiments with a virtual process tree
                 architecture called ZAPP are described. Most of the
                 experiments involve the parallel execution of simple
                 process trees grown by rewrite rules which recursively
                 decompose a large grain of work into smaller grains.
                 Some experiments extend the model by supporting limited
                 notions of global communication in the process tree.",
  note =         "IEEE 0073-1129/89/0000/0355.",
}

@InBook{GMS93,
  author = 	 {Goldsmith, R. and McBurney, D.L. and Sleep, M.R.},
  title = 	 {{Term Graph Rewriting: Theory and Practice}},
  chapter = 	 {{Parallel Execution of Concurrent Clean on ZAPP}},
  publisher = 	 {Wiley},
  year = 	 1993,
  OPTeditor =	 {Sleep, M.R. and Plasmeijr, M.J. and {van Eekelen}, M.C.J.D.},
  annote =	 {Kesseler uses this as standard reference for ZAPP}
}

%$%cindex Dactl

@InProceedings{GKS91,
  author = 	 {Glauert, J.R.W.  and Kennaway, J.R. and Sleep, M.R.},
  title = 	 {{Dactl: An Experimental Graph Rewriting Language}},
  booktitle = 	 {{International Workshop on Graph Grammars}},
  volume =	 532,
  series =	 LNCS,
  year =	 1991,
  publisher =	 {Springer Verlag},
  address =	 {Bremen, Germany}
}

%$%cindex Gamma

@Article{BaLe90,
  author = 	 {Ban\^{a}tre, J-P. and {Le M\'{e}tayer}, D.},
  title = 	 {{The Gamma Model and its Discipline of Programming}},
  journal = 	 {Science of Programming},
  year = 	 1990,
  volume =	 15,
  number =	 1,
  pages =	 {55--77},
  annote =	 {first reference to Gamma model}
}

@Article{GlKu96,
  author = 	 {Gladitz, K. and Kuchen, H.},
  title = 	 {{Shared Memory Implementation of the Gamma-Operation}},
  journal = 	 {Journal of Symbolic Computation},
  year = 	 1996,
  volume =	 21,
  pages =	 {577--591},
  url =          {http://www-i2.informatik.rwth-aachen.de/\~{}herbert/jsc95.ps},
  abstract =     "The Gamma scheme (Banatre and Le Metayer 1990, Banatre and Le Metayer 1993)
allows the specification of  an application without introducing unnecessary
sequentiality. The   main  idea is  to simulate   a  chemical  reaction  on
multisets, also called bags. Some elements of a bag which fulfill a certain
predicate (also called  reaction condition) may be  taken from the multiset
and replaced by new elements which are  generated by combining the selected
elements using  the so-called reaction function.  In  the present paper, we
investigate the  efficient parallel implementation  of the Gamma scheme. In
particular, the reaction condition can be split into parts which are tested
as  early  as  possible.   Possible    atomic  conditions  are   arithmetic
comparisons. We use  the properties of the  arithmetic comparisons in order
to   restrict     the  range  of    bag    elements   which  has     to  be
considered.    Reconsiderations   of     the  same   bag     elements   are
avoided. Moreover,  a sophisticated  protocol is  needed  in order to avoid
deadlocks when    several   processes     compete  for  the       same  bag
elements. Experimental results show the impact of our improvements.
"
}

@InProceedings{GlKu94,
  author = 	 {Gladitz, K. and Kuchen, H.},
  title = 	 {{Parallel Implementation of the Gamma-Operation}},
  pages =	 {154--163},
  OPTeditor =	 {Hong, H.},
  volume =	 5,
  series =	 {Lecture Notes Series on Computing},
  booktitle =    {{PASCO'94 --- First International Symposium on Parallel Symbolic
		  Computation}}, 
  year =	 1994,
  organization = {RISC-Linz},
  publisher =	 {World Scientific},
  address =	 {Hagenberg/Linz, Austria, 26--28 September},
  keywords =     {Gamma, multisets, parallel implementation, locking},
  url =          {http://www-i2.informatik.rwth-aachen.de/\~{}herbert/pasco94.ps.gz},
  abstract =     "The  Gamma scheme [BL90, BL93]  allows the specification  of an application
without introducing unnecessary sequentiality. The main idea is to simulate
a chemical reaction on multisets, also called bags.  Some elements of a bag
which fulfill a certain predicate (also  called reaction condition ) may be
taken from the multiset and replaced by new elements which are generated by
combining the selected  elements using the  so-called reaction function. In
the present paper, we  investigate the efficient parallel implementation of
the Gamma scheme. In particular,  the reaction condition  can be split into
parts which are tested as early as possible. Possible atomic conditions are
arithmetic comparisons. We use the properties of the arithmetic comparisons
in order   to  restrict  the range  of    bag elements   which has   to  be
considered.      Reconsiderations  of   the    same      bag  elements  are
avoided. Moreover,  a sophisticated  protocol is  needed in  order to avoid
deadlocks   when   several   processes    compete    for  the   same    bag
elements. Experimental results show the impact of our improvements.
"
}

@InProceedings{KuGl93,
  author = 	 {Gladitz, K. and Kuchen, H.},
  title = 	 {{Parallel Implementation of Bags}},
  booktitle =    {{FPCA'93 ---  Conference on Functional Programming
		  Languages and Computer Architecture}}, 
  year =	 1993,
  publisher =	 {ACM Press},
  address =      {Copenhagen, Denmark, June 9--11},
  pages =	 {299--307},
  url =          {http://www-i2.informatik.rwth-aachen.de/\~{}herbert/FPCA93.ps.gz},
  abstract =     "Multisets (also called bags) are an interesting data structure for
parallelly implemented functional programming  languages, since they do not
force an unneeded restriction of the data flow and allow to exploit as much
parallelism as possible. Most operations on  multisets can be understood as
special cases of the so-called  Gamma scheme [BL90].  In the present paper, we
investigate efficient implementations  of several  instances of this  Gamma
scheme on MIMD-machines."
}

%gamma.ps.Z is:
%\bibitem{HMS:95} 
%{\em Compositional semantics of a notation for composing
%parallel programs}, submitted for publication, January 1995.

%Hankin C L, Le MÈtayer D and Sands D, A Calculus of Gamma Programs,
%Proc. of the Yale Workshop on Parallel Programming Languages, ed. Gelernter
%D, LNCS 757,S-V,

@Article{HLS95,
  author = 	 {Hankin, C.L. and {Le M\'{e}tayer}, D. and Sands, D.},
  title = 	 {{Compositional Semantics of a Notation for Composing
                  Parallel Programs}}, 
  journal = 	 {???},
  year = 	 1995,
  month =	 {January},
  note =	 {submitted for publication},
  annote =	 {using the Gamma model as a coordination language for
                  parallel computation},
  url =  {http://theory.doc.ic.ac.uk/people/Hankin/gamma.ps.Z},
  abstract =     "Gamma is   a minimal language based  on  local  multiset rewriting  with an
elegant  chemical reaction  metaphor,  it belongs   to  the same  group  of
languages  as Linda and  Unity.  The virtues  of this  paradigm in terms of
systematic  program construction and  design of parallel programs have been
argued in   previous papers. Gamma   can  also be  seen  as  a notation for
coordinating independent programs in a larger application. In this paper we
provide  solutions   to  alleviate the  main   limitation  of  the original
formalism  which was  the  lack of   modularity. We provide  sequential and
parallel  composition operators. We  first propose an operational semantics
which  gives  rise to   an ordering  and  an  equivalence  relation between
programs. Then we derive a compositional semantics which is consistent with
the corresponding behavioural  semantics. We   prove  a number of  laws  of
programs in this semantics. In the  later parts of the  paper we focus on a
number of program  schemes which have emerged  from our experience of Gamma
programming and we study  their properties.  The primitive program  schemes
(called  tropes)  take    the form of    parameterised conditional  rewrite
rules. We consider a number of examples which illustrate  the use of tropes
and we study their algebraic properties in  conjunction with the sequential
and parallel  combining forms.  Using   the   examples we illustrate    the
application of these properties in the  verification of some simple program
transformations."
}

@InProceedings{KPS94,
  author = 	 {Kuchen, H. and Plasmeijr, R. and Stoltze, H.},
  title = 	 {{Distributed Implementation of a Data Parallel Functional
                   Language}},
  booktitle = 	 {{PARLE'94 --- Parallel Architectures and Languages Europe}},
  number =	 817,
  series =	 {LNCS},
  year =	 1994,
  publisher =	 {Springer Verlag},
  pages =	 {464--477},
  url =          {http://www-i2.informatik.rwth-aachen.de/\~{}herbert/parle94.ps.gz},
  abstract =     "We discuss why existing implementations of functional languages on
MIMD-machines with distributed  memory are slow. This  is done by comparing
the  behavior  of     a  functional program  with a     corresponding Occam
program. The main reason is   that functional languages give   insufficient
means to control parallelism and communication. Our  approach is to support
data parallelism by providing a set of primitives on arrays which allow the
user  to   control the   parallelism and  communication  on  a  high level,
disabling problems like deadlocks. Only one unique  version of an array may
be referenced at a  time. This restriction  allows arrays to be  updated in
place  and enables   the user to   control the  space requirements of   the
program. The uniqueness of arrays  is checked by the compiler. Experimental
results    demonstrate the efficiency of    our   data parallel  functional
language."
}

@TechReport{KuGe91,
  author = 	 {Kuchen, H. and Geiler, G.},
  title = 	 {{Distributed Applicative Arrays}},
  institution =  {RWTH Aachen},
  year = 	 1991,
  type =	 {Technical Report},
  number =	 {AIB 91-5}
}

@InProceedings{Waug91,
         author =       {Waugh, K.G.},
         title =        "{Parallel Imperative Programs from Functional
                        Prototypes}",
         booktitle =    {{IFL'91 --- International Workshop on the Parallel
                        Implementation of Functional Languages}},
         address =      {Southampton, UK, June 5--7},
         year =         "1991",
         editor =       "Glaser, H. and Hartel, P.",
         publisher =    "Technical Report CSTR 91-07, University of
                        Southampton",
         pages =        "75--88",
         abstract =     "We have been considering the automatic translation of
                        sequential SML prototypes of vision algorithms into
                        efficient parallel Occam programs. We use function
                        instrumentation of the prototype to obtain typical and
                        pathological execution details and static analysis
                        allows us to consider argument sizes and flow. From
                        these results an abstract form of the functional
                        program is used to estimate partition size and hence
                        granularity. The transformations which apply to the
                        functional prototype can also to be applied to the
                        abstract form of the program. In this way we can use
                        the transformed abstracted program to determine the
                        effect of particular transformations on execution of
                        the prototype.",
         keywords =     "Program Transformation, Weight Analysis",
       }


@InProceedings{SoBo95,
  author = 	 {Sodan, A.C. and Bock, H.},
  title = 	 {{Extracting Characteristics from Functional Programs for
                   Mapping to Massively Parallel Machines}},
  year =         {1995},
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  pages =        {134--148},
  address =	 {Denver, Colorado, April 10--12},
  keywords =     {parallel Lisp, profiling, large-scale programming},
  url =          {ftp://sisal.llnl.gov/pub/hpfc/papers95/paper14.ps},
  abstract =     "For  problems  with highly  dynamic behavior, our  experiments  showed that
there are specific characteristics for different applications. We therefore
propose a mapping environment   providing   several strategies  for    both
granularity control and  dynamic load balancing.  For appropriate selection
and   parameterization  of   strategies,   we   extract   the   application
characteristics     using   trace-based     profiling     and   appropriate
evaluations.  Our profiling  approach differs   from  others in that   more
information (like argument sizes or the branching  factor) is collected and
more advanced evaluations  are performed. Large real-life applications have
been  successfully   measured,  and examples   demonstrate  differences  in
characteristics and  which the corresponding  strategies being  appropriate
are."
}

%0797073 ( 73rd article in volume 797 )
%Sodan, A.; Bi, H.
%parLisp-Parallel Symbolic Processing with Lisp on the Distributed Memory
%Machine MANNA
%In: Gentzsch, W. (ed.); Harms, U. (ed.), High-Performance Computing and
%Networking. Proceedings, Vol. 2: Networking and Tools.
%LNCS,
%ISBN 3-540-57981-8
%Conference: High-Performance Computing and Networking, International
%Conference and Exhibition; Munich, Germany; 18-20 Apr 1994
%Classification: J.2; F.2.1; D.1.3; D.3.2; D.3.4; D.4.8; C.2.1; C.2.2; C.2.3;
%C.2.4
%Keywords: Programming Languages in HPC
%0797073 ( 73rd article in volume 797 )
%Sodan, A.; Bi, H.
%parLisp-Parallel Symbolic Processing with Lisp on the Distributed Memory
%Machine MANNA
%In: Gentzsch, W. (ed.); Harms, U. (ed.), High-Performance Computing and
%Networking. Proceedings, Vol. 2: Networking and Tools.
%LNCS,
%ISBN 3-540-57981-8
%Conference: High-Performance Computing and Networking, International
%Conference and Exhibition; Munich, Germany; 18-20 Apr 1994
%Classification: J.2; F.2.1; D.1.3; D.3.2; D.3.4; D.4.8; C.2.1; C.2.2; C.2.3;
%C.2.4
%Keywords: Programming Languages in HPC

@PhdThesis{Char97,
  author = 	 {Charles, N.},
  title = 	 {{Introducing Parallelism in a Lazy Functional Language}},
  school = 	 {Department of Computer Science},
  year = 	 {1997},
  month =        {July},
  note =	 {PhD qualifying dissertation},
}

%$%cindex Brisk

@InProceedings{HDS98,
  author = 	 {Holyer, I. and Davies, N. and Spiliopoulou, E.},
  title = 	 {{Distribution in a Demand Driven Style}},
  booktitle = 	 {{COCL'98 --- Intl.\ Workshop on Component-based software development in Computational Logic}},
  pages =	 {29--41},
  year =	 1998,
  address =	 {University of Pisa, Italy},
  month =	 sep,
  documentURL =  {http://www.cs.bris.ac.uk/Tools/Reports/Ps/1998-holyer-0.ps.gz},
  abstract =     "In this paper  we present a model of distribution  as a natural consequence
of deterministic concurrency; a purely declarative form of concurrency that
preserves referential  transparency. This  extends the demand  driven model
used  as the  operational  basis  of functional  languages,  into one  with
multiple independent  demands without the need to  allow non-determinism to
appear at the user level.

The abstract model of this  distributed system uses a uniform access memory
model  in order  to  provide a  global  memory view.   This, combined  with
explicit  thread based  concurrency and  explicit representation  of demand
enables  the trustable  distribution of  reactive systems  across different
systems.   Since  referential transparency  is  preserved,  any pattern  of
distribution  has no  effect on  the semantics  of the  functional program.
Computation,  including both code  and data,  can be  moved in  response to
demand. This  suggests itself  as a natural  approach to  facilitate mobile
computation both as a conceptual and operational mode."
}

%$%cindex coordination languages

@Article{GeCa92,
  author = 	 {Gelernter, D. and Carriero, N.},
  title = 	 {{Coordination Languages and Their Significance}},
  journal = 	 CACM,
  year = 	 1992,
  volume =	 32,
  number =	 2,
  month =	 {February},
  pages =	 {97--107}
}

@TechReport{CFGK94,
  author = 	 {Carriero, N. and  Freeman, E. and Gelernter, D. and Kaminsky, D.},
  title = 	 {{Adaptive Parallelism and Piranha}},
  institution =  {Yale University},
  year = 	 {1994},
  type = 	 {Technical Report},
  month = 	 feb,
  annote = 	 {coins the term of adaptive parallelism},
  url =          {http://www.cs.yale.edu/Linda/papers/workshop.ps},
  abstract =     "``Adaptive parallelism''  refers to parallel computations  on a dynamically
changing  set of  processors:  processors  may join  or  withdraw from  the
computation  as it  proceeds. Networks  of fast  workstations are  the most
important setting for adaptive parallelism at present. Workstations at most
sites are  typically idle for significant  fractions of the  day, and those
idle  cycles   may  constitute  in  the  aggregate   a  powerful  computing
resource. For this reason and  others, we believe that adaptive parallelism
is  assured   of  playing  an  increasingly  prominent   role  in  parallel
applications development over the next decade.

The ``Piranha''  system now  up and running  on a heterogeneous  network at
Yale  is a general-purpose  adaptive parallelism  environment. It  has been
used to run a variety of production applications, including applications in
graphics,  theoretical physics,  electrical  engineering and  computational
fluid dynamics.  In this  paper we describe  the Piranha model  and several
archetypal Piranha  algorithms. Our main  goals are to  show that it  is an
effective  and  workable  approach,   to  explain  how  the  Piranha  model
constrains algorithm design, and  to suggest directions for theoretical and
practical research."
}

%% More:
%  Nicholas Carriero, Eric Freeman, and David Gelernter. Adaptive Parallelism on Multiprocessors: Preliminary Experience with
%        Piranha on the CM-5. Technical Report 969, Yale University, May 1993. (View in DVI or PostScript ). 

@Article{FoTa94,
  author = 	 {Foster, I. and Taylor, S.},
  title = 	 {{A Compiler Approach to Scalable Concurrent-Program Design}},
  journal = 	 TOPLAS,
  year = 	 1994,
  volume =	 16,
  number =	 3,
  pages =	 {577--604},
  annote =	 {std ref for PCN}
}

@Article{CrLe94,
  author = 	 {Crowl, L.A. and Leblanc, T.J.},
  title = 	 {{Parallel Programming with Control Abstraction}},
  journal = 	 TOPLAS,
  year = 	 1994,
  volume =	 16,
  number =	 3,
  pages =	 {524--576},
  annote =	 {imperative coordination language with implementation}
}

@InProceedings{HeNi91,
  author = 	 {Heytens, M. and Nikhil, R.S.},
  title = 	 {{List Comprehensions in Agna, a Parallel, Persistent Object System}},
  booktitle =    {{FPCA'91 --- Conference on  Functional Programming Languages and
                 Computer Architectures}},
  address =      {Harvard, Massachusetts, USA},
  month =        aug,
  year =         1991,
  series =       {Lecture Notes in Computer Science},
  volume =       523,
  OPTeditor =       {Hughes, J.},
  publisher =    S-V,
  pages =	 {569--591},
}

% ---------------------------------------------------------------------------
%$%node Implementation of Functional Languages, Parallel Garbage Collection, Parallel Fctal Pging, Top
%$%section Implementation of Functional Languages
% ---------------------------------------------------------------------------

%$%menu
%* SECD::			
%* Graph Reduction::		
%* Dataflow/Sisal::		
%$%end menu

@Manual{PH*96,
  title = 	 {{Haskell 1.3 --- A Non-Strict, Purely  Functional Language}},
  key =		 {Haskell-1.3},
  author =	 {Peterson, J.C. and Hammond, K. and {et al.}},
  year =	 1996,
  month =	 {May},
  url =          {http://www.haskell.org/definition/haskell-report-1.3.ps.gz},
}

@Manual{PHA*97,
  title = 	 {{Report on the Non-Strict Functional Language, Haskell, Version 1.4}},
  key =		 {Haskell-1.4},
  author =	 {Peterson, J.C. and Hammond, K. and Augustsson, L. and Boutel, B. and  Burton, F.W. and  Fasel, J.H. and  Gordon, A.D. and Hughes, R.J.M. and  Hudak, P. and  Johnsson, T. and Jones, M.P. and  {Peyton Jones}, S.L. and Reid, A. and Wadler, P.L.},
  year =	 1997,
  month =        {April},
  url =          {http://www.haskell.org/definition/haskell-report-1.4.ps.gz}
}

%$%node SECD, Graph Reduction, Implementation of Functional Languages, Implementation of Functional Languages
%$%subsection SECD
% Actually that's sequential!!!!

@InProceedings{AbSy85,
  author =       {Abramski, S. and Sykes, R.},
  title =        {{Secd-m: A Virtual Machine for Applicative
                 Programming}},
  OPTeditor =       {Jouannaud, J-P.},
  booktitle =    {FPCA'85 --- Conference on  Functional Programming Languages and
                 Computer Architecture},
  address =      {Nancy, France, September 16--19},
  year =         {1985},
  pages =        {81--98},
  series =       {Lecture Notes in Computer Science},
  volume =       {201},
  publisher =    {Springer},
  abstract =     "We present a virtual machine to support {\em
                 applicative multiprogramming\/} --- the description of
                 concurrent, asynchronous systems such as operating
                 systems in a functional style. The machine extend's
                 Landin's secd machine to support multiple concurrent
                 expression evaluation, non-determinism in the form of
                 the fair merge, and a full range of input and output
                 devices. This allows system programs to be written in a
                 functional style. The secd-m machine has been
                 implemented and a number of functional concurrent
                 programs demonstrated.",
  keywords =     "SECD, Abstract Machine, Operating System",
}

@TechReport{DaMc90,
  author = 	 {Davie, A.J.T. and McNally, D.J.},
  title = 	 {{CASE - A Lazy Version of an SECD Machine with a Flat Environment}},
  institution =  {University of St Andrews},
  year = 	 1990,
  type =	 {Technical Report},
  number =	 {CS/90/19},
  annote =	 {lazy SECD machine},
  url =          {ftp://ftp.dcs.st-and.ac.uk/pub/staple/CASE.ps.Z},
  abstractURL =  {http://www.dcs.st-and.ac.uk/\~{}staple/publications.html#case},
  abstract =     "Graph reduction has been the basis of most fast running implementations of
functional languages, with little attention being paid recently to Landin's
SECD approach. CASE is an abstract machine which supports applicative
programming and is a variation of Landin's classical SECD machine. Its
environment is organized in a novel way which makes variable access more
efficient by removing the overhead of following a static chain or of
setting up a display as in more conventional stack architectures. The CASE
machine also has features allowing the execution of code to reflect lazy
evaluation semantics. We describe the architecture of the machine, its
operational semantics, and code generated by a typcial compiler for some
sample programs. We also discuss some optimisations made to facilitate
efficient code generation on real hardware and give some measurements of
the efficiency of the machine.
"
}

%$%cindex CAM

@Book{Curi86,
  author = 	 {Curien, P-L.},
  title = 	 {{Categorical Combinators, Sequential Algorithms and
                  Functional Programming}},
  publisher = 	 {Pitman},
  year = 	 1986,
  annote =	 {Std CAM reference; taken from \cite{FiHa88}}
}

%$%cindex combinatory logic

@Book{CuFe58,
  author = 	 {Curry, H.B. and Feys, R.},
  title = 	 {{Combinatory Logic}},
  publisher = 	 {North Holland},
  year = 	 1958,
  volume =	 {Vol.\ 1},
  annote =	 {std reference to combinatory logic; from
                  \cite{FiHa88}[p. 273]}
}

%$%menu
%* Graph Reduction::		
%* Dataflow/Sisal::		
%$%end menu

%$%node  Graph Reduction, Dataflow/Sisal, SECD, Implementation of Functional Languages
%$%subsection Graph Reduction

%$%menu
%* Sequential Graph Reduction::	 
%* Parallel Graph Reduction::	
%$%end menu

@article{HGW91,
	author =  {Hartel, P.H. and Glaser, H.W. and Wild, J.M.},
	title =   {{Compilation of Functional Languages Using Flow Graph
                  Analysis}},
	journal = SPE,
	volume =  {24},
	number =  {2},
	pages =   {127--173},
	month =   feb,
	year =    {1994},
        keywords = {Compilation of lazy functional languages, flow graph,
                  program analysis, program synthesis, strictness},
        annote =  {focus on analyses, esp strictness analysis, in this flow
                  graph framework},
	url =  {ftp://ftp.fwi.uva.nl/pub/computer-systems/functional/reports/SPE\_flowgraph.ps.Z},
        abstract = "A system based on  the notion of a  flow graph is  used to formally specify
and to implement a  compiler for a  lazy functional language. The  compiler
takes a simple functional language as  input and generates C. The generated
C program can  then  be compiled, and   loaded with an   extensive run-time
system to  provide   the facility  to experiment  with   different analysis
techniques. The  compiler provides  a   single, unified, efficient,  formal
framework  for  all  the   analysis and   synthesis phases, including   the
generation of C.  Many of the  standard techniques, such as strictness  and
boxing analyses, have been included."
}

%$%node Sequential Graph Reduction, Parallel Graph Reduction, Graph Reduction, Graph Reduction
%$%subsubsection Sequential Graph Reduction

%$%cindex Graph Reduction

@PhdThesis{Wads71,
  author = 	 {Wadsworth, C.P.},
  title = 	 {{Semantics and Pragmatics of the Lambda Calculus}},
  school = 	 {University of Oxford},
  year = 	 1971,
  annote =	 {first reference to graph reduction}
}

%$%xref{Peyt87,Graph Reduction Textbook,,ws-pfp.bib,}

%$%cindex Spineless G-machine

@InProceedings{BPR88,
  author = 	 {Burn, G. and {Peyton Jones}, S.L. and Robson, J.},
  title = 	 {{The Spineless G-machine}},
  booktitle = 	 {LFP'88 --- Conference on Lisp and Functional Programming},
  year =	 1988,
  address =	 {Salt Lake City, Utah},
  month =	 {August},
  pages =	 {244--258},
  annote =	 {std reference for Spineless G-machine}
}

%$%cindex TIM
%$%cindex STGM

@Article{Peyt92,
  author = 	 {{Peyton Jones}, S.L.},
  title = 	 {{Implementing Lazy Functional Languages on Stock Hardware:
	           the Spineless Tagless G-machine}},
  journal = 	 JFP,
  year = 	 1992,
  volume =	 2,
  number =	 2,
  month =	 {July},
  pages =	 {127--202},
  url =          {ftp://ftp.dcs.gla.ac.uk/pub/glasgow-fp/papers/spineless-tagless-gmachine.ps.Z},
  abstract =     "The Spineless Tagless G-machine is an abstract machine designed to support
nonstrict higher-order functional languages. This presentation of the
machine falls into three parts. Firstly, we give a general discussion of
the design issues involved in implementing non-strict functional
languages. Next, we present the STG language, an austere but
recognisably-functional language, which as well as a denotational meaning
has a well-defined operational semantics. The STG language is the
``abstract machine code'' for the Spineless Tagless G-machine. Lastly, we
discuss the mapping of the STG language onto stock hardware. The success of
an abstract machine model depends largely on how efficient this mapping can
be made, though this topic is often relegated to a short section. Instead,
we give a detailed discussion of the design issues and the choices we have
made. Our principal target is the C language, treating the C compiler as a
portable assembler."
}

%$%cindex G-machine

@phdthesis{John87,
  author =  {Johnsson, T.},
  title =   {{Compiling Lazy Functional Languages, Part I}},
  school =  {Department of Computer Sciences},
  address = {Chalmers University of Technology, GÅˆteborg, Sweden},
  year =    {1987},
  owner =   {pcl},
  descr =   {plfun},
}

@phdthesis{Augu87,
  author =   {Augustsson, L.},
  title =    {{Compiling Lazy Functional Languages, Part II}},
  school =   {Department of Computer Sciences},
  address =  {Chalmers University of Technology, GÅˆteborg, Sweden},
  year =     {1987},
  owner =    {pcl},
  descr =    {plfun},
}

@inproceedings{John85,
  author =    {Johnsson, T.},
  title =     {{Lambda Lifting: Transforming Programs to Recursive Equations}},
  booktitle = {{Functional Programming Languages and Computer Architecture}},
  address =   {Nancy, France, September 16--19},
  year =      {1985},
  pages =     {190--203},
  series =    {Lecture Notes in Computer Science},
  OPTeditor =    {Jouannaud, Jean-Pierre},
  volume =    {201},
  publisher = {Springer-Verlag},
  owner =     {pcl},
  descr =     {pagc},
}

@Book{Peyt87,
  author =       {{Peyton Jones}, S.L.},
  title =        {{The Implementation of Functional Programming
                 Languages}},
  publisher =    P-H,
  OPTseries =       {International Series in Computer Science},
  OPTaddress =      {New York},
  year =         {1987},
  abstract =     {This standard work on the implementation of functional
                 languages by graph reduction also contains a section on
                 parallel graph reduction discussing the issues of
                 conservative/speculative parallelism, task granularity
                 and task locality.},
  keywords =     {Parallel Graph Reduction},
}

@PhdThesis{Sant95,
  author = 	 {Santos, A.},
  title = 	 {{Compilation by Transformation in Non-Strict Functional Languages}},
  school = 	 {Department of Computing Science},
  year = 	 {1995},
  address =	 {University of Glasgow},
  month =	 {September}
}

%$%cindex profiling

@InProceedings{SaPe95,
  author = 	 {Sansom, P.M. and {Peyton Jones}, S.L.},
  title = 	 {{Time and Space Profiling for Non-Strict Higher-Order Functional Languages}},
  booktitle = 	 {POPL'95 --- Symposium on Principles of Programming Languages},
  year =	 1995,
  publisher =	 {ACM Press},
  address =	 {San Francisco, California},
  month =	 {January},
  url =          {ftp://ftp.dcs.glasgow.ac.uk/pub/glasgow-fp/papers/profiling.ps.Z},
  abstract =     "We present the first profiler for a compiled, non-strict, higher-order,
purely functional language capable of measuring {\em time} as well as {\em
space} usage. Our profiler is implemented in a production-quality
optimising compiler for Haskell, has low overheads, and can successfully
profile large applications.


A unique feature of our approach is that we give a formal specification of
the attribution of execution costs to cost centres. This specification
enables us to discuss our design decisions in a precise framework. Since it
is not obvious how to map this specification onto a particular
implementation, we also present an implementation-oriented operational
semantics, and prove it equivalent to the specification."
}


%$%xref{SaPe95,Profiling,,prg-trafo.bib,}

@Article{SaPe97,
  author =       {Sansom, P.M. and {Peyton Jones}, S.L.},
  title =        {{Formally Based Profiling for Higher-Order Functional
                 Languages}},
  journal =      TOPLAS,
  volume =       {19},
  number =       {2},
  pages =        {334--385},
  month =        {March},
  year =         {1997},
  CODEN =        "ATPSDT",
  ISSN =         "0164-0925",
  bibdate =      "Wed May 14 09:00:40 MDT 1997",
  bibsource =    "http://www.acm.org/pubs/toc/",
  url =  "http://www.dcs.gla.ac.uk/fp/authors/Patrick\_Sansom/1997\_profiling\_TOPLAS.ps.gz",
  OPTurl = "http://www.acm.org/pubs/articles/journals/toplas/1997-19-2/p334-sansom/p334-sansom.pdf",
  keywords =     "applicative languages. {\bf d.3.4}: software;
                 compilers. {\bf d.3.4}: software; debugging aids. {\bf
                 d.3.2}: software; language classifications; logics and
                 meanings of programs; operational semantics.;
                 optimization. {\bf f.3.2}: theory of computation;
                 processors; programming languages; semantics of
                 programming languages; software engineering; testing
                 and debugging; {\bf d.2.5}: software",
  subject =      "{\bf D.2.5}: Software, SOFTWARE ENGINEERING, Testing
                 and Debugging, Debugging aids. {\bf D.3.2}: Software,
                 PROGRAMMING LANGUAGES, Language Classifications,
                 Applicative languages. {\bf D.3.4}: Software,
                 PROGRAMMING LANGUAGES, Processors, Compilers. {\bf
                 D.3.4}: Software, PROGRAMMING LANGUAGES, Processors,
                 Optimization. {\bf F.3.2}: Theory of Computation,
                 LOGICS AND MEANINGS OF PROGRAMS, Semantics of
                 Programming Languages, Operational semantics.",
}

@PhdThesis{Sans94,
  author = 	 {Sansom, P.},
  title = 	 {{Execution Profiling for Non-Strict Functional Languages}},
  school = 	 {Department of Computing Science},
  address =	 {University of Glasgow},
  year = 	 1994,
  month =	 nov,
  url =          {http://research.microsoft.com/Users/simonpj/Papers/1994\_nonstrict-profiling\_THESIS.ps.gz},
}

@PhdThesis{Jarv96,
  author = 	 {Jarvis, S.A.},
  title = 	 {{Profiling Large-Scale Lazy Functional Programs}},
  school = 	 {Department of Computer Science},
  year = 	 1996,
  address =	 {University of Durham},
  annote =	 {introduces cost centre stacks}
}

@InProceedings{Part92,
  author = 	 {Partain, W.},
  title = 	 {{The nofib Benchmark Suite of Haskell Programs}},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  year =	 1992,
  publisher =	 S-V,
  pages =	 {178--194}},
}

@InProceedings{Hart95,
  author = 	 {Hartel, P.H.},
  title = 	 {{Benchmarking Implementations of Functional Languages II
                  --- Two Years Later}},
  booktitle = 	 {{IFL'95 ---  International Workshop on the Implementation of Functional Languages}},
  editor =	 {Johnsson, T.},
  year =	 1995,
  address =	 {BÅÂstad, Sweden},
  month =	 {September},
  pages =        {63--68},
  annote =	 {Later in JFP?? Check!},
  url =          {ftp://ftp.fwi.uva.nl/pub/computer-systems/functional/reports/benchmarkII.ps.Z},
  abstract =     "Six implementations of different lazy functional languages are compared
using a common benchmark of a dozen medium-sized programs. The experiments
that were carried out two years ago have been repeated to chart progress in
the development of these compilers. The results have been extended to
include all three major Haskell compilers. Over the last two years, the
Glasgow Haskell compiler has been improved considerably. The other
compilers have also been improved, but to a lesser extent. The Yale Haskell
compiler is slower than the Glasgow and Chalmers Haskell compilers. The
compilation speed of the Clean compiler is still unrivalled. Another
extension is a comparison of results on different architectures so as to
look at architectural influences on the benchmarking procedure. A high-end
architecture should be avoided for benchmarking activities, as its
behaviour is uneven. It is better to use a midrange machine if possible."
}

@article{HFA*96,
	author = {Hartel, P.H. and Feeley, M.  and Alt, M.  and Augustsson,
                  L.  and Baumann, P.  and Beemster, M.  and Chailloux, E.
                  and Flood, C.H.  and Grieskamp, W.  and {van Groningen},
                  J.H.G.  and	 Hammond, K. and Hausman, B.  and Ivory,
                  M.Y.  and  Jones, R.E. and Kamperman, J.  and Lee, P.  and
	          Leroy, X. and Lins, R.D. and Loosemore, S. and RÅˆjemo, N.
                  and Serrano, M.  and Talpin, J.-P.  and
	          Thackray, J. and Thomas, S. and Walters, P. and Weis, P. and
                  Wentworth, P.},
	title = {Benchmarking Implementations of Functional Languages with ``Pseudoknot'', a {Float-Intensive} Benchmark},
	journal = JFP,
	volume = {6},
	number = {4},
	year = {1996},
	annote = {ftp://ftp.fwi.uva.nl/pub/computer-systems/functional/reports/JFP\_pseudoknotI.ps.Z},
        abstract = "Over 25 implementations of different functional languages are benchmarked
using the same program, a floating point intensive application taken from
molecular biology. The principal aspects studied are compile time and
execution time for the various implementations that were benchmarked. An
important consideration is how the program can be modified and tuned to
obtain maximal performance on each language implementation. With few
exceptions, the compilers take a significant amount of time to compile this
program, though most compilers were faster than the then current GNU C
compiler (GCC version 2.5.8). Compilers that generate C or Lisp are often
slower than those that generate native code directly: the cost of compiling
the intermediate form is normally a large fraction of the total compilation
time. There is no clear distinction between the runtime performance of
eager and lazy implementations when appropriate annotations are used: lazy
implementations have clearly come of age when it comes to implementing
largely strict applications, such as the Pseudoknot program. The speed of C
can be approached by some implementations, but to achieve this performance,
special measures such as strictness annotations are required by non-strict
implementations. The benchmark results have to be interpreted with
care. Firstly, a benchmark based on a single program cannot cover a wide
spectrum of `typical' applications. Secondly, the compilers vary in the
kind and level of optimisations offered, so the effort required to obtain
an optimal version of the program is similarly varied."
}


@InProceedings{FaWr87,
  author = 	 {Fairbairn, J. and Wray, S.},
  title = 	 {{TIM --- a Simple Lazy Abstract Machine to Execute
                  Supercombinators}},
  editor =	 {Kahn, G.},
  address =      {Portland, Oregon, USA, September 14--16},
  number =	 {274},
  series =	 LNCS,
  pages =	 {34--45},
  booktitle =    {{FPCA'87 ---  Conference on Functional Programming
		  Languages and Computer Architecture}}, 
  year =	 {1987},
  publisher =	 S-V,
  month =	 {September},
}

@InProceedings{Argo89,
  author = 	 {Argo, G.},
  title = 	 {{Improving the Three Instruction Machine}},
  booktitle =    {{FPCA'89 --- Conference on Functional Programming Languages and
                   Computer Architecture}}, 
  address =      {Imperial College, London, September 11--13},
  year =	 {1989},
  publisher =	 {ACM Press},
  pages =        {100--115},
}

%$%cindex G-machine

@InProceedings{John84,
  author = 	 {Johnsson, T.},
  title = 	 {{Efficient Compilation of Lazy Evaluation}},
  booktitle = 	 {CC'84 --- International Conference on Compiler Construction},
  year =	 1984,
  address =	 {Montreal, Canada},
  pages =	 {58--69},
  note =         {SIGPLAN Notices 19(6)},
  annote =	 {first reference for G-machine}
}
%$%xref{John87,G-machine,,par-others.bib,}
%$%xref{Augu87,G-machine,,par-others.bib,}

@inproceedings{LaHa92,
	author = {Langendoen, K.G. and Hartel, P.H.},
	title =  {{FCG}: a Code Generator for Lazy Functional Languages},
        booktitle = 	 {CC'92 --- International Conference on Compiler Construction},
	OPTeditor = {U. Kastens and P. Pfahler},
	publisher = {Springer-Verlag},
	address = {Paderborn, Germany},
        series =  LNCS,
        volume =  641,
	month =   oct,
	year =    {1992},
	pages =   {278--296},
	url = {ftp://ftp.fwi.uva.nl/pub/computer-systems/functional/reports/CC92\_code\_generator.ps.Z},
        abstract = "The    FCG code generator  produces portable   code that supports efficient
two-space   copying garbage collection.   The code generator transforms the
output of the  FAST compiler front end  into an abstract machine code. This
code   explicitly uses a   call stack, which  is accessible  to the garbage
collector. In contrast to other functional language compilers that generate
assembly directly, FCG  uses the C compiler  for code generation, providing
high-quality code optimisations and portability. To make  full use of the C
compiler's  capabilities,  FCG   includes  an    optimisation scheme   that
transforms the  naively generated  stack-based  code into  a register-based
equivalent form.  The results for  a benchmark  of functional programs show
that  code generated   by FCG  performs  well in   comparison with the  LML
compiler.
"
}


@InProceedings{Reid98,
  author = 	 {Reid, A.},
  title = 	 {{Putting the Spine back in the Spineless Tagless G-machine: an implementation of revertible blackholes}},
  booktitle = 	 {IFL'98 --- International Workshop on the Implementation of
                  Functional Languages},
  year = 	 1998,
  OPTseries =	 LNCS,
  address = 	 {September 9--11, University College London, UK},
  url =          {http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/reid-alastair/ifl98.ps.gz},
  publisher =	 {Draft proceedings}
}

%$%node Parallel Graph Reduction,  , Sequential Graph Reduction, Graph Reduction
%$%subsubsection Parallel Graph Reduction

% several references are in the section on Glaswegian publications

%$%xref{Peyt89,Parallel Implementations of Functional Programming Languages,,stat-an.bib,}

%$%cindex nu-G machine

@InProceedings{AuJo89,
  author =       {Augustsson, L. and Johnsson, T.},
  title =        {{Parallel Graph Reduction with the
                 $\langle{v,G}\rangle$-machine}},
  booktitle =    {{FPCA'89 --- Conference on Functional Programming Languages and
                 Computer Architecture}},
  address =      {Imperial College, London, UK, September 11--13},
  year =         {1989},
  pages =        {202--213},
  publisher =    {ACM Press},
  url =          {ftp://ftp.cs.chalmers.se/pub/cs-reports/papers/nu-G.ps.Z},
  abstract =     {We have implemented a parallel graph reducer on a
                 commercially available shared memory multiprocesor (a
                 Sequent Symmetry) that achieves real speedup compared
                 to a fast compiled implementation of the conventional
                 G-machine. Using 15 procesors, this speedup ranges
                 between 5 and 11 depending on the program. Underlying
                 the implementation is an abstract machine called the
                 $\langle{v,G}\rangle$-machine. We describe the
                 sequential and the parallel
                 $\langle{v,G}\rangle$-machine, and our implementation
                 of them. We provide performance and speedup figures and
                 graphs.},
  keywords =     {Programmed Graph Reduction},
}

@PhdThesis{Chak97,
  author = 	 {Chakravarty, M.},
  title = 	 {{On the Massively Parallel Execution of Declarative Programs}},
  school = 	 {Technical University Berlin},
  year = 	 1997,
  month =	 {February}
}

%$%cindex Ginger

@InProceedings{JoAx92,
  author = 	 {Joy, M. and Axford, T.},
  title = 	 {{A Parallel Graph Reduction Machine}},
  OPTeditor =       {Kuchen, H. and Loogen, R.},
  booktitle =    {{IFL'92 ---  International Workshop on the Parallel
		  Implementation of Functional Languages}},
  year =	 {1992},
  address =	 {RWTH Aachen, Germany},
  month =	 {September}
}

@TechReport{Joy92,
  author = 	 {Joy, M.S.},
  title = 	 {{Ginger --- A Simple Functional Language}},
  institution =  {Department of Computer Science},
  year = 	 1992,
  number =	 {CS-RR-235},
  address =	 {University of Warwick},
  month =	 {December},
  url =  {http://www.dcs.warwick.ac.uk/pub/reports/rr/235/index.html},
  abstractURL =  {http://www.dcs.warwick.ac.uk/pub/reports/rr/235.html},
  abstract =     "Ginger is a lazy functional language with simple syntax and semantics,
heavily sugared lambda-calculus spiced with primitive data types and
operators. Ginger is designed to run on a parallel machine, and operators
to control parallelism are included. Primitives for a novel
{"}divide-and-conquer{"} style list processing model are also included. This
document is the reference manual for the language."
}



@Article{KeLi84,
  author =       {Keller, R.M. and Lin, F.C.H.},
  title =        {{Simulated Performance of a Reduction-Based
                 Multiprocessor}},
  journal =      {IEEE Computer},
  volume =       {17},
  number =       {7},
  pages =        {70--82},
  month =        jul,
  year =         {1984},
}

% qaHleS remark on xref syntax (from texinfo description):
%     xref{Node name, Cross Reference Name, Particular Topic,
%     info-file-name, A Printed Manual}, for details.
%which produces

%     *Note Cross Reference Name: (info-file-name)Node name,
%     for details.

%in Info and

%     See section "Particular Topic" in A Printed Manual, for details.

%in a printed book.

%$%xref{DaRe81,ALICE,,ws-pfp.bib,}

@InProceedings{DaRe81,
  author =       {Darlington, J. and Reeve, M.},
  title =        {{ALICE --- A Multi-Processor Reduction Machine for the
                 Parallel Evaluation of Applicative Languages}},
  booktitle =    {{FPCA '81 --- Conference on  Functional Programming Languages and
                 Computer Architecture}},
  address =      {Boston, MA, USA},
  year =         {1981},
  pages =        {65--74},
  publisher =    {ACM Press},
  abstract =     "In this paper we present a scheme for the parallel
                 evaluation of a wide variety of applicative languages
                 and provide an overview of the architecture of a
                 machine on which it may be implemented.",
  keywords =     "Parallel Graph Reduction",
}

@InProceedings{HaRe86,
  author = 	 {Harrison, P.G. and Reeve, M.J.},
  title = 	 {{The Parallel Graph Reduction Machine, ALICE}},
  booktitle = 	 {{Workshop on Graph Reduction}},
  volume =	 279,
  series =	 LNCS,
  year =	 1986,
  publisher =	 S-V,
  address =	 {Santa Fe, NM, USA},
  month =	 {September/October},
  pages =	 {181--202},
  annote =	 {design of ALICE and first results of a prototype}
}

%$%cindex Flagship

@TechReport{Kean90,
  author = 	 {Keane, J.A.},
  title = 	 {{The Flagship Declarative System}},
  institution =  {Department of Computer Science},
  year = 	 1990,
  number =	 {UMCS-90-2-1},
  address =	 {University of Manchester},
  abstractURL =  {http://www.cs.man.ac.uk/csonly/cstechrep/Abstracts/UMCS-90-2-1.html},
  abstract =     "The Flagship Project was a research collaboration between the University of
Manchester, Imperial College, London and International Computers Limited. Its
aim was to produce a complete computing system based on a declarative
programming style. This survey report discusses the approaches taken in the
three major areas addressed by the project: (1) programming languages and
programming environments, (2) the machine architecture and computational
models and (3) the software environment, and, where appropriate, presents the
approaches with regard to general classifications. The influences on the
approaches taken by Flagship are considered and other similar approaches are
briefly discussed. The intention is to present the different areas of the
project both as a coherent whole and in comparison to similar activities. The
work in areas (2) and (3), which involved the groups at the University of
Manchester, is concentrated upon. Some discussion of the work in area (1), and
thus the contribution of the Imperial College part of the project, is also
included."
}

@Article{Kean94,
  author = 	 {Keane, J.A.},
  title = 	 {{An Overview of the Flagship System}},
  journal = 	 JFP,
  year = 	 1994,
  volume =	 4,
  number =	 1,
  month =	 {January},
  pages =	 {19--45},
  annote =	 {prbly std reference to Flagship},
  abstractURL =  {http://www.dcs.glasgow.ac.uk/jfp/bibliography/References/keane1994:19.html},
  abstract =     "The Flagship Project was a research collaboration between the
     University of Manchester, Imperial College London and International
     Computers Ltd. The project was unusual in that it aimed to produce a
     complete computing system based on a declarative programming
     style. Three areas of a declarative system were addressed: (1)
     Programming languages and programming environments; (2) the machine
     architecture and computational models; and (3) the software
     environment. This overview paper discusses each of these areas, the
     intention being to present the project as a coherent whole."
}

@InProceedings{KeGl89,
  author =       {Kewley, J.M. and Glynn, K.},
  title =        {{Evaluation Annotations for Hope+}},
  OPTeditor =	 {Davis, K. and Hughes, R.J.M.},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  year =	 {1989},
  publisher =	 S-V,
  address =	 {Fraserburgh, Scotland, August 21--23},
  pages =        {329--337},
  abstract =     "To fully exploit the parallelism of the Flagship
                 Machine it is desirable to be able to experiment with a
                 variety of evaluation strategies. The ability to
                 control the behavior of individual functions via
                 annotations is of enormous help in understanding the
                 behavior of a parallel system and permits the
                 comparison of ``lazy'' and ``eager'' versions of
                 languages as well as providing the opportunity to
                 intermix the two evaluation orders. The primary
                 functional language used by the Flagship project is
                 Hope+. Hope+ has a mixed evaluation strategy, with
                 strict function application combined with lazy data
                 constructors. This can result in both unnecessary
                 evaluation and loss of parallelism. Annotations enable
                 user-control of the evaluation strategy so that such
                 problems are avoided.",
  keywords =     "Para-Functional Programming, Annotations",
}

%$%xref{WaWa87,Flagship,,ws-pfp.bib,} 

@InCollection{WWW86,
  author = 	 {Watson, I. and Watson, P. and Woods, J.V.},
  title = 	 {{Parallel Data-Driven Graph Reduction}},
  booktitle = 	 {{Fifth Generation Computer Architectures}},
  publisher =	 {North Holland},
  year =	 1986,
  month =	 {April},
  annote =	 {I only have the reference (from \cite{RuSa87}) not the
  paper; seems to be general description of Flagship}
}

@Article{WSWW87,
  author = 	 {Watson, I. and Sargeant, J. and Watson, P. and Woods, V.},
  title = 	 {{Flagship Computational Models and Machine Architecture}},
  journal = 	 {ICL Technical Journal},
  year = 	 1987,
  volume =	 5,
  pages =	 {555--574},
  annote =	 {I only have the reference (from \cite{Rush95}) }
}

%$%cindex LAGER
%$%xref{Wats88b,LAGER,,man.bib,}

@TechReport{Wats90,
  author = 	 {Watson, I.},
  title = 	 {{C-LAGER Definition}},
  institution =  {University of Manchester},
  year = 	 1990,
  type =	 {Internal Document},
  number =	 {EDS.UD.3I.M0004},
  annote =	 {A std reference to LAGER from ~\cite{Rush95}},
}

@PhdThesis{Wats87,
  author = 	 {Paul Watson},
  title = 	 {{The Parallel Reduction of Lambda Calculus Expression}},
  school = 	 {Department of Computer Science},
  year = 	 1987,
  address =	 {University of Manchester},
  type =	 {UMCS-87-2-1},
  abstractURL =  {http://www.cs.man.ac.uk/csonly/cstechrep/Abstracts/UMCS-87-2-1.html},
  abstract =     "Models of computation for the evaluation of Functional Programs are based on
the rules for reducing Lambda Calculus expressions. Reduction is achieved by
the application of the b-conversion rule to suitable redexes, but few models
of computation provide a full implementation of this rule because of the
complexity of avoiding variable name clashes. Consequently, evaluation orders
are restricted to those in which name clashes cannot occur.

This thesis develops a model of computation for the parallel reduction of
Lambda Calculus expressions, represented in De Bruijn's name-free notation,
which does provide a full implementation of b-conversion, allowing expression
to be reduced by any evaluation order. The model is designed to allow
reduction to be performed on a parallel machine comprised of a set of
processor/store pairs connected by a communications network. A data- driven,
graph reduction execution mechanism is used to exploit the parallel hardware
efficiently.

A language for specifying graph reduction models of computation is proposed,
and is used to give full specification of the Lambda Calculus reduction
model. Specifications in the language can be compiled to the instruction set
of a virtual machine. The code produced can then be executed by a virtual
machine emulator, or could be recompiled to the order code of a physical
processor to allow the high performance simulation of models of
computation. The virtual machine is used as the foundation for the design of a
physical machine which would support the parallel reduction of lambda calculus
expressions.

One of the major issues in the design of graph reduction models of computation
is the removal of redundant parts of the expression graph. Neither of the two
standard Garbage Collection schemes: Reference Count, or Mark-Scan is entirely
suitable for a distributed machine, mainly because of the synchronisation they
require. A modified Reference Garbage Collection scheme is described which
removes the need for synchronisation, and enhances some of the attractive
properties of Reference Count Garbage Collection."
}

@TechReport{Wats88a,
  author = 	 {Watson, I.},
  title = 	 {{Lager: Interim Report}},
  institution =  {Department of Computer Science},
  number =       {FS/MU/IW/026-88},
  month =        {November},
  year = 	 1988,
  type =	 {Internal Report},
  address =	 {University of Manchester},
  annote =	 {Reference from \cite{Rush95}}
}

@TechReport{Wats88b,
  author = 	 {Watson, I.},
  title = 	 {{A LAGER Execution Mechanism and Store Model}},
  institution =  {Department of Computer Science},
  year = 	 1988,
  type =	 {Internal Report},
  number =	 {Version 2},
  address =	 {University of Manchester},
  annote =	 {Reference from \cite{Rush95}}
}

@TechReport{Wats93,
  author = 	 {Watson, I.},
  title = 	 {{SLAM --- Lazy Task Creation and Dynamic Load Balancing
                  with Active Messages}},
  institution =  {Department of Computer Science},
  year = 	 1993,
  type =	 {Internal Report},
  address =	 {University of Manchester},
  month =	 {May},
  annote =	 {I have a copy of that from Kevin; don't know if that has
                  ever been published}
}

@PhdThesis{Rugg87,
  author = 	 {Ruggiero, C.A.},
  title = 	 {{Throttle Mechanisms for the Manchester Dataflow Machine}},
  school = 	 {Department of Computer Science},
  year = 	 1987,
  address =	 {University of Manchester},
  type =	 {UMCS-87-8-1},
  abstractURL =  {http://www.cs.man.ac.uk/csonly/cstechrep/Abstracts/UMCS-87-8-1.html},
  abstract =     "Dataflow architectures have been studied for several years as an alternative
to the von Neumann model of computation. They have proved to be effective in
exposing and exploiting different forms of parallelism in programs. However,
it has been noted that the parallelism exposed for some programs is much more
than the parallelism that can be exploited by specific hardware
configurations. This leads to both excessive and unnecessary use of resources
(particularly storage).

In this thesis, different software methods for reducing store usage in the
Manchester Dataflow Machine by controlling parallelism are
presented. Assessment of these methods is achieved by means of simulation. A
process- based method is found to be the most suitable and its advantages and
problems are discussed in some detail. Among the problems, the necessity of
detecting termination of processes is crucial.

Three methods for detecting process termination are analysed and an
instruction reference counting method is described in detail. The way this
method detects termination for various programming constructs is explained,
and the overhead introduced by the method is assessed.

The thesis concludes that there are no fundamental reasons for store usage in
dataflow machines to be orders of magnitude higher than in von Neumann
machines."
}

@PhdThesis{Fole87,
  author = 	 {Foley, J.F.},
  title = 	 {{A Hardware Simulator for a Multi-Ring Dataflow Machine}},
  school = 	 {Department of Computer Science},
  year = 	 1987,
  address =	 {University of Manchester},
  type =	 {UMCS-87-2-2},
  abstractURL =  {http://www.cs.man.ac.uk/csonly/cstechrep/Abstracts/UMCS-87-2-2.html},
  abstract =     "The dataflow machine at Manchester has demonstrated a near linear speedup in
program execution when multiple processors are used, provided only that
sufficient parallelism is available within the problem. However, the machine
has a limited memory bandwidth and this places a ceiling on computation rates;
the matching and node stores are particular bottlenecks. One solution is a
multi-ring machines consisting of a number of dataflow machines (each with
several processors) connected by a switching network and operating in
parallel. In order to test such a system, a hardware simulator has been
built. This thesis gives details of the machine and the results of initial
tests."
}

%$%cindex GAML

@InProceedings{Mara91,
  author =       {Maranget, L.},
  title =        {{GAML: a Parallel Implementation of Lazy ML}},
  booktitle =    {{FPCA'91 --- Conference on  Functional Programming Languages and
                 Computer Architectures}},
  address =      {Harvard, Massachusetts, USA},
  month =        aug,
  year =         {1991},
  series =       LNCS,
  volume =       {523},
  OPTeditor =       {Hughes, J.},
  publisher =    S-V,
  pages =        {102--123},
  abstract =     "We present a new parallel implementation of Lazy ML.
                 Our scheme is a direct extension of the G-machine-based
                 implementation of Lazy ML. Parallelism is introduced by
                 {\em fork\/} annotations inserted by the programmer. We
                 discuss the interference of such user annotations with
                 strictness annotations generated by our compiler. The
                 system has been implemented on a Sequent Balance
                 computer. We also address the main practical issues
                 involved, including stack and heap management.",
  keywords =     "Parallel Graph Reductoin, G-Machine, Annotations",
}

%$%cindex EQUALS

@InProceedings{KPR*92,
  author = 	 {Kaser, O. and Pawagi, S. and Ramakrishnan, C.R. and
                  Ramakrishnan, I.V. and Sekar, R.C.},
  title = 	 {{Fast Parallel Implementation of Lazy Languages --- The
                   EQUALS Experience}},
  booktitle =    {{LFP'92 --- Conference on Lisp and Functional Programming}},
  year =	 1992,
  publisher =	 {ACM Press},
  address =	 {San Francisco, California, June 22--24},
  pages =	 {335--344},
  keywords =     {Automatic parallelisation, lazy functional languages}
}

%Volume 7 Number 2 (March 1997):
%   o Fast Parallel Implementation of Lazy Languages: The EQUALS Experience
%        o By Owen Kaser, S.Pawagi, C.R Ramakrishan et al

@Article{KPR*97,
  author = 	 {Kaser, O. and Pawagi, S. and Ramakrishnan, C.R. and
                  Ramakrishnan, I.V. and Sekar, R.C.},
  title = 	 {{EQUALS --- a
                   Fast Parallel Implementation of a Lazy Language}},
  journal = 	 JFP,
  year = 	 1997,
  volume =	 7,
  number =	 2,
  pages =        {183--217},
  month =	 {March},
  annote =	 {extended version of \cite{KPR*92}},
  abstractURL =  {http://www.dcs.glasgow.ac.uk/jfp/bibliography/References/kaserrrs1997:183.html},
  abstract =     "This paper describes EQUALS, a fast parallel implementation of a
     lazy functional language on a commercially available shared-memory
     parallel machine, the Sequent Symmetry. In contrast to previous
     implementations, we propagate normal form demand at compile time as
     well as run time, and detect parallelism automatically using
     strictness analysis. The EQUALS implementation indicates the
     effectiveness of NF-demand propagation in identifying significant
     parallelism and in achieving good sequential as well as parallel
     performance. Another important difference between EQUALS and
     previous implementations is the use of reference counting for memory
     management, instead of mark-and-sweep or copying garbage
     collection. Implementation results show that reference counting
     leads to very good scalability and low memory requirements, and
     offers sequential performance comparable to generational garbage
     collectors. We compare the performance of EQUALS with that of other
     parallel implementations (the $\nug$-machine and GAML) as well as
     with the performance of SML/NJ, a sequential implementation of a
     strict language."
}

%$%cindex GAML
%$%xref{Mara91,GAML,,ws-pfp.bib,}

%$%xref{Gron92,Implementation Aspects of Concurrent Clean,,ws-pfp.bib,}

%$%cindex PAM
%$%xref{LKID89,PAM,,ws-pfp.bib,}

@inproceedings{LKI89,
author =    {Loogen, R. and Kuchen, H. and Indermark, K.},
title =     {{Distributed Implementation of Programmed Graph Reduction}},
booktitle = {{PARLE 89 --- Conference on Parallel Architectures
              and Languages Europe}},
publisher = S-V,
series =    LNCS,
volume =    {365},
year =      {1989},
pages =     {136--157},
owner =     {pcl},
descr =     {plfun},
}


%$%node Dataflow/Sisal,  , Graph Reduction, Implementation of Functional Languages
%$%subsection Dataflow/Sisal

%$%cindex Dataflow

@InProceedings{Denn74,
  author = 	 {Dennis, J.B.},
  title = 	 {{First Version of a Data Flow Procedure Language}},
  booktitle = 	 {{Programming Symposium}},
  number =	 19,
  series =	 LNCS,
  year =	 1974,
  publisher =	 S-V,
  address =	 {Paris},
  annote =	 {first reference to dataflow}
}

%$%cindex Id

%CSG-Memo-313
%         The Parallel Programming Language Id and its Compilation for
%     Parallel Machines, R. S. Nikhil, July 1990, Presented at the Workshop
%     on Massive Parallelism: Hardware, Programming and Applications,
%     Amalfi, Italy, October, 1989.

@InProceedings{Nikh89,
  author = 	 {Nikhil, R.S.},
  title = 	 {{The Parallel Programming Language Id and its Compilation
                  for Parallel Machines}},
  booktitle = 	 {{Workshop on Massive Parallelism: Hardware, Programming
                  and Applications}},
  year =	 1989,
  address =	 {Amalfi, Italy},
  month =	 {October},
  note =         {CSG Memo 313},
  annote =	 {Std (?) reference for Id}
}

%CSG-Memo-284-2
%         ID Reference Manual, Version 90.1, R. S. Nikhil, July 1991, (60
%     pages).
%     [postscript 509k] - [compressed 201k] - [gzipped 151k]


@TechReport{Nikh91,
  author = 	 {Nikhil, R.S.},
  title = 	 {{ID Reference Manual}},
  institution =  {Laboratory for Computer Science},
  year = 	 1991,
  number =	 {CSG Memo 284-2},
  address =	 {M.I.T.},
  month =	 {July},
  url =  {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-284-2.ps.gz},
  abstract =     "Id is a general-purpose  parallel programming language designed by  members
of  the   Computation Structures  Group in  MIT's  Laboratory  for Computer
Science,  and    is used  for  programming   dataflow  and   other parallel
machines. The major subset of Id (syntactically  distinguishable) is a pure
functional    language  with    non-strict  semantics.  Features   include:
higher-order functions, a  Milner-style statically type-checked polymorphic
type system    with overloading, user    defined types  and patternmatching
notation, lists  and list comprehensions,  arrays and array comprehensions,
and  facilities for delayed evaluation.  The  non-functional aspects of  Id
include I-structures  and  M-structures (for  both arrays  and user-defined
types)  and input/output. With respect   to the functional subset, programs
with I-structures  remain  deterministic   but may not    be  referentially
transparent, and    programs   with  M-structures  and  i/o    may  even be
non-deterministic.   Id programs are  implicitly  parallel  to a very  fine
grain.   Some  programs  with  M-structures and   i/o  may   need explicity
sequencing, for which facilities are provided.",
}

%$%cindex pH

%CSG-Memo-369 [abstract]
%         pH Language Reference Manual, Version 1.0, R.S. Nikhil, Arvind,
%     J. Hicks, S. Aditya, L. Augustsson, J. Maessen and Y. Zhou, January
%     1995, (25 pages).
%     [postscript 215k] - [compressed 98k] - [gzipped 73k]


@TechReport{NAH*95,
  author = 	 {Nikhil, R.S. and Arvind and Hicks, J. and Aditya, S. and
                  Augustsson, L. and Maessen, J.-W. and Zhou, Y.},
  title = 	 {{pH Language Reference Manual}},
  institution =  {Laboratory for Computer Science},
  year = 	 1995,
  number =	 {CSG Memo 369},
  address =	 {M.I.T.},
  month =	 {January},
  url =  {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-369.ps.gz},
  abstractURL =  {http://www.csg.lcs.mit.edu:8001/cgi-bin/abstract.pl?/home/www/ftp/pub/papers/csgmemo/memo-369.abs},

  abstract    =  "pH is a parallel language obtained by extending
     Haskell. This document must be read in conjunction with the
     Haskell Report since it describes only the extensions."
}


@Book{NiA01,
  author =	 {Nikhil, R. and Arvind},
  title = 	 {{Implicit Parallel Programming in pH}},
  publisher = 	 MKP,
  year = 	 2001,
  month =	 may,
  note =	 {ISBN 1-55860-644-0},
  abstractURL =       {http://www.mkp.com/books_catalog/catalog.asp?ISBN=1-55860-644-0},
}

%CSG-Memo-377-1 [abstract]
%         Semantics of pH: A parallel dialect of Haskell, Shail Aditya,
%     Arvind and Jan-Willem Maessen, June 1995, (22 pages).
%     [postscript 254k] - [compressed 113k] - [gzipped 86k]


@TechReport{AAM95,
  author = 	 {Aditya, S. and Arvind and Maessen, J.-W.},
  title = 	 {{Semantics of pH: A Parallel Dialect of Haskell}},
  institution =  {Laboratory for Computer Science},
  year = 	 1995,
  number =	 {CSG Memo 377-1},
  address =	 {M.I.T.},
  month =	 {June},
  url =  {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-377-1.ps.gz},
  abstractURL =  {http://www.csg.lcs.mit.edu:8001/cgi-bin/abstract.pl?/home/www/ftp/pub/papers/csgmemo/memo-377-1.abs},
  abstract =     "The semantics of kernel pH are defined in the form of a
     parallel, normalizing interpreter. A description of I-structure
     and M-structure operators is also given within the same
     framework. Semantics of barriers in pH are presented by
     translation into the kernel language without barriers. The
     framework presented is also suitable for multithreaded
     compilation of pH."
}

%$%cindex TTDA

@Article{ArNi89,
  author = 	 {Arvind and Nikhil, R.S.},
  title = 	 {{Executing a Program on the MIT Tagged-Token Dataflow Architecture}},
  journal = 	 {IEEE Transactions on Computers},
  year = 	 {1990},
  volume = 	 {39},
  number = 	 {3},
  month =        {March},
  annote = 	 {Expanded version of the PARLE 87 paper; discusses
                  k-bounded loops}
}

@InProceedings{ArNi87,
  author =       {Arvind and Nikhil, R.S.},
  title =        {{Executing a Program on the MIT Tagged-Token Dataflow
                 Architecture}},
  booktitle =    {PARLE'87 --- Parallel Architectures and Languages
                 Europe},
  address =      {Eindhoven, The Netherlands, June 15--19},
  year =         {1987},
  volume =       {259},
  editor =       {{de Bakker}, J.W. and Nijman, A.J. and Treleaven, P.C.},
  series =       {Lecture Notes in Computer Science},
  publisher =    {Springer},
  OPTnote =         "Also: Computation Structures Group Memo 271,
                 Laboratory for Computer Science, Massachusetts
                 Institute of Technology, Cambridge, MA",
  abstract =     "The MIT Tagged-Token Dataflow project has an
                 unconventional, but integrated approach to
                 general-purpose high-performance parallel computing.
                 Rather than extending conventional sequential
                 languages, we use Id, a high-level language with
                 fine-grained parallelism and determinacy implicit in
                 its operational semantics. Id programs are compiled to
                 dynamic dataflow graphs, a parallel machine language.
                 Dataflow graphs are directly executed on the MIT
                 Tagged-Token Dataflow Architecture (TTDA), a novel
                 multiprocessor architecture. Dataflow research has
                 advanced significantly in the last few years; in this
                 paper we provide an overview of our current thinking,
                 by describing example Id programs, their compilation to
                 dataflow graphs, and their execution on the TTDA.
                 Finally, we describe related work and the status of our
                 project.",
  keywords =     "Compilation, Dataflow, Id",
}

%$%cindex Monsoon
%$%xref{PaCu90,Monsoon,,ws-pfp.bib,}

@InProceedings{PaCu90,
  author =       {Papadopoulos, G.M. and Culler, D.E.},
  title =        {{Monsoon: An Explicit Token-Store Architecture}},
  booktitle =    {{ISCA'90 --- International Symposium on Computer
                 Architecture}},
  address =      {Seattle, Washington, May 28--31},
  year =         {1990},
  pages =        {82--91},
  number =       {18(2)},
  series =       {ACM SIGARCH Computer Architecture News},
  month =        jun,
  abstract =     "Dataflow architectures tolerate long unpredictable
                 communication delays and support generation and
                 coordination of parallel activities directly in
                 hardware, rather than assuming that program mappings
                 will cause these issues to disappear. However, the
                 proposed mechanisms are complex and introduce new
                 mapping complications. This paper presents a greatly
                 simplified approach to dataflow execution, called the
                 {\em explicit token store\/} (ETS) architecture, and
                 its current realization in {\em Monsoon\/}. The essence
                 of dynamic dataflow execution is captured by a simple
                 transition on state bits associated with storage local
                 to a processor. Low-level storage management is
                 performed by the compiler in assigning nodes to slots
                 in an {\em activation frame\/}, rather than dynamically
                 in hardware. The processor is simple, highly pipelined,
                 and quite general. It may be viewed as a generalization
                 of a fairly primitive von Neumann architecture.
                 Although the addresssing capability is restrictive,
                 there is exactly one instruction executed for each
                 action on the dataflow graph. Thus, the machine
                 oriented ETS model provides new understanding of the
                 merits and the real cost of direct execution of
                 dataflow graphs.",
  keywords =     "Dataflow, Compilation, Abstract Machine",
}


@PhdThesis{Papa88,
  author =       {Papadopoulos, G.M.},
  title =        {{Implementation of a General Purpose Dataflow
                 Multiprocessor}},
  school =       {Department of Electrical Engineering and Computer
                 Science},
  address =      {Massachusetts Institute of Technology, Cambridge, MA},
  month =        aug,
  year =         {1988},
  abstract =     "General purpose multiprocessors have largely failed to
                 meet expectations for programmability and performance.
                 We blame the lack of usable parallel programming
                 languages and systems on the underlying processor
                 architecture. Machines built out of conventional
                 sequential processors simply do not support the
                 synchronization demands of parallel execution, so the
                 programmer focuses upon the dangerous and arduous task
                 of discovering a minimum set of synchronization points
                 without introducing nondeterminism. We argue that
                 processors must be fundamentally changed to execute a
                 parallel machine language, in which parallel activities
                 are coordinated as efficiently as instructions are
                 scheduled. Dataflow architectures address this
                 challenge by radically reformulating the basic
                 specification of a machine program. These machines {\em
                 directly execute\/} dataflow graphs, which specify only
                 the essential prerequisites for the execution of an
                 instruction --- the availability of operands.
                 Unfortunately, dataflow machines, including the M.I.T.
                 Tagged Token Dataflow Architecture (TTDA), have labored
                 under a number of implementation burdens, notably the
                 apparent need for a fully associative operand matching
                 store which discovers when instructions are able to
                 execute. We introduce and develop a novel dataflow
                 architecture, the Explicit Token Store (ETS), which
                 directly executes tagged-token dataflow graphs while
                 correcting a number of inherent inefficiencies of
                 previous dataflow machines. In the ETS model, operand
                 matching is performed at compiler-designated offsets
                 within an activation frame. We show that the ETS is
                 compatible with the TTDA by giving translations from
                 TTDA machine graphs to ETS machine graphs. Finally, we
                 describe an implementation of an ETS dataflow
                 multiprocessor, called Monsoon, now under
                 construction.",
  keywords =     "Dataflow, Compilation, Id",
}

%CSG-Memo-280
%         Resource Requirements of Dataflow Programs, D. E. Culler and
%     Arvind, December 1987, In Proceedings of the 15th International
%     Symposium on Computer Architectures, IEEE/ACM, Honolulu, HI, May 31 -
%     June 2, l988.

@InProceedings{CuAr88,
  author = 	 {Culler, D.E. and Arvind},
  title = 	 {{Resource Requirements of Dataflow Programs}},
  booktitle = 	 {{International Symposium on Computer Architectures}},
  year =	 1988,
  address =	 {Honolulu, HI, May 31 -- June 2},
  note =	 {Also: CSG-Memo-280},
  annote =	 {another paper with stuff about k-bounded loops}
}

%CSG-Memo-269
%         I-structures: Data Structures for Parallel Computing, Arvind,
%     R. S. Nikhil and K. Pingali, February 1987, Revised March 1989, In
%     Proceedings of the Workshop on Graph Reduction, Los Alamos, New
%     Mexico, September 28 - October 1 ,1986 and {\em ACM Transactions on
%     Programming Languages and Systems}, 11(4):598-632, October 1989.
%$%xref{ANP89,I-structures,,csg-memos.html,}

@Article{ANP89,
  author = 	 {Arvind and Nikhil, R.S. and Pingali, K.},
  title = 	 {{I-structures: Data Structures for Parallel Computing}},
  journal = 	 TOPLAS,
  year = 	 1989,
  volume =	 11,
  number =	 4,
  month =	 {October},
  pages =	 {598--632},
  annote =	 {std reference for I-structures},
  abstract =     "It is difficult simultaneously to achieve elegance,
                 efficiency and parallelism in functional programs that
                 manipulate large data structures. We demonstrate this
                 through careful analysis of program examples using
                 three common functional data-structuring approaches ---
                 lists using {\tt Cons} and arrays using {\tt Update}
                 (both fine-grained operators), and arrays using {\tt
                 make-array} (a ``bulk'' operator). We then present
                 I-structures as an alternative, defining precisely the
                 parallel operational semantics of Id, a language with
                 I-structures. We show elegant, efficient and parallel
                 solutions for the program examples in Id. I-structures
                 make the language non-functional, but do not raise
                 determinacy issues. Finally, we show that even in the
                 context of purely functional languages, I-structures
                 are invaluable for implementing functional data
                 abstractions.",
  keywords =     "Dataflow, I-structures",
}


@InProceedings{ANP87,
  author =       {Arvind and Nikhil, R.S. and Pingali, K.K.},
  title =        {{I-Structures: Data Structures for Parallel
                 Computing}},
  booktitle =    {{Graph Reduction, Proceedings of a Workshop}},
  address =      {Santa Fe, New Mexico, September 29 -- October 1},
  year =         {1986},
  volume =       {279},
  series =       {Lecture Notes in Computer Science},
  editor =       {Fasel, J.H. and Keller, R.M.},
  publisher =    {Springer},
  pages =        {336--369},
  OPTnote =         "Also: Computation Structures Group Memo 269,
                 Laboratory for Computer Science, Massachusetts
                 Institute of Technology, Cambridge, MA",
  abstract =     "It is difficult simultaneously to achieve elegance,
                 efficiency and parallelism in functional programs that
                 manipulate large data structures. We demonstrate this
                 through careful analysis of program examples using
                 three common functional data-structuring approaches ---
                 lists using {\tt Cons} and arrays using {\tt Update}
                 (both fine-grained operators), and arrays using {\tt
                 make-array} (a ``bulk'' operator). We then present
                 I-structures as an alternative, defining precisely the
                 parallel operational semantics of Id, a language with
                 I-structures. We show elegant, efficient and parallel
                 solutions for the program examples in Id. I-structures
                 make the language non-functional, but do not raise
                 determinacy issues. Finally, we show that even in the
                 context of purely functional languages, I-structures
                 are invaluable for implementing functional data
                 abstractions.",
  keywords =     "Dataflow, I-structures",
}

%CSG-Memo-327
%         M-Structures: Extending a Parallel, Non-strict, Functional
%     Language with State, P. S. Barth, R. S. Nikhil and Arvind, March 1991,
%     In Proceedings on Functional Programming and Computer Architecture,
%     Cambridge, MA, August 28-30, 1991, (35 pages).
%     [postscript 348k] - [compressed 155k] - [gzipped 108k]
% url = {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-327.ps.gz},

%$%xref{BNA91,M-structures,,ws-pfp.bib,}


@InProceedings{BNA91,
  author =       {Barth, P.S. and Nikhil, R.S. and Arvind},
  title =        {{M-Structures: Extending a Parallel, Non-strict,
                 Functional Language with State}},
  booktitle =    {{FPCA'91 --- Conference on Functional Programming Languages and Computer
                 Architectures}},
  address =      {Harvard, Massachusetts},
  month =        aug,
  year =         {1991},
  OPTeditor =       {Hughes, J.},
  series =       LNCS,
  volume =       {523},
  publisher =    S-V,
  pages =        {538--568},
  note =         {Also: CSG Memo 327},
  url =          {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-327.ps.gz},
  abstract =     "Functional programs are widely appreciated for being
                 ``declarative'' (algorithms are not {\em
                 over-specified\/}) and implicitly parallel. However,
                 when modeling state, both properties are compromised
                 because the state variables must be ``threaded''
                 through most functions. Further, state ``updates'' may
                 require much copying. A solution is to introduce
                 assignments, as in ML and Scheme; however, for
                 meaningful semantics, they resort to strict, sequential
                 evaluation. We present an alternative approach called
                 {\em M-structures\/}, which are imperative data
                 structures with implicit synchronization. M-structures
                 are added to Id, a parallel non-strict functional
                 language. We argue that some programs with M-structures
                 improve on their functional counterpart in three ways;
                 they are (1) more {\em declarative\/}; (2) more {\em
                 parallel\/}; and (3) more {\em storage efficient\/}.
                 Points (1) and (2) contradict conventional wisdom. We
                 study two problems: histogramming a tree, and graph
                 traversal, comparing functional and M-structure
                 solutions.",
  keywords =     "State, Updatable arrays, Non-determinism",
}


%CSG-Memo-371 [abstract]
%         StarT-NG: Delivering Seamless Parallel Computing, D. Chiou,
%     B.S. Ang, Arvind, M.J. Beckerle, G.A. Boughton, R. Greiner, J.E. Hicks
%     and J.C. Hoe, February 1995, Published at EURO-PAR'95 Conference,
%     August 1995, (15 pages).
%     [postscript 311k] - [compressed 140k] - [gzipped 106k]
%$%xref{CAA*95,StarT-NG,,csg-memos.html,}

%$%cindex StarT

@InProceedings{CAA*95,
  author = 	 {Chiou, D. and Ang, B.S. and Arvind and Beckerle, M.J.  and
                  Boughton, G.A.  and Greiner, R.  and  Hicks, J.E.  and
                  Hoe, J.C.},
  title = 	 {{StarT-NG: Delivering Seamless Parallel Computing}},
  booktitle = 	 {EuroPar'95 --- European
                  Conference on Parallel Processing},
  volume =	 {966},
  series =	 LNCS,
  year =	 {1995},
  address =      {Stockholm, Sweden, August 29--31},
  publisher =	 S-V,
  pages =	 {101--116},
  notes =        {Also: CSG Memo 371},
  annote =	 {std reference for StarT},
  abstract = "START-NG is a joint MIT-Motorola project to build a high-
    performance message passing machine from commercial systems. Each
    site of the machine consists of a PowerPC 620-based Motorola symmet-
    ric multiprocessor (SMP) running the AIX 4.1 operating system. Every
    processor is connected to a low-latency, high-bandwidth network that
    is directly accessible from user-level code. In addition to fast message
    passing capabilities, the machine has experimental support for cache-
    coherent shared memory across sites. When the machine requires mem-
    ory to be kept globally coherent, one processor on each site is devoted
    to supporting shared memory. When globally coherent shared memory is
    not required, that processor can be used for normal computation tasks.
    START-NG will be delivered at about the time the base SMP is introduced
    into the marketplace. The ability to be both a collection of standard SMP
    and an aggressive message passing machine with coherent shared mem-
    ory makes START-NG a good building block for incrementally expandable
    parallel machines."
}

%CSG-Memo-394 [abstract]
%         Message Passing Support for Multi-grained, Multi-threading and
%     Multi-tasking Environments, Boon S. Ang, Derek Chiou, Larry Rudolph
%     and Arvind, November 1996, (23 pages).
%     [postscript 880k] - [compressed 327k] - [gzipped 228k]

@TechReport{ACRA96,
  author = 	 {Ang, B.S. and Chiou, D. and Rudolph, L. and Arvind},
  title = 	 {{Message Passing Support for Multi-grained, Multi-threading and
     Multi-tasking Environments}},
  institution =  {Laboratory for Computer Science},
  year = 	 1996,
  number =	 {CSG Memo 394},
  address =	 {M.I.T.},
  month =	 {November},
  url =  {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-394.ps.gz},
  abstractURL =  {http://www.csg.lcs.mit.edu:8001/cgi-bin/abstract.pl?/home/www/ftp/pub/papers/csgmemo/memo-394.abs},
  abstract =     "          In order to become generally useful, message passing
     mechanisms not only need to provide high performance, but also
     the {\em three M's}: multi-granularity, multi-threading and
     multi-processing. In this paper, we discuss these requirements
     and why they are needed, and describe the message passing
     mechanisms of StarT-Voyager which provide such functionality
     while delivering high performance. We discuss how StarT-Voyager
     implements the three M's and predict its
     performance. StarT-Voyager's novel message passing mechanisms
     offer a definitive advantage in a multi-threaded environment
     without compromising the performance in a single-threaded
     environment. To our knowledge, no architecture supports this
     entire range of functionality while providing high performance."
}

%CSG-Memo-393
%         Lambda-S: an Implicitly Parallel Lambda-Calculus with Letrec,
%     Synchronization and Side-Effects, Arvind, J.W. Maessen, R.S. Nikhil
%     and Joe Stoy, November 1996, Based on paper submitted to ICFP '97, (18
%     pages).
%     [postscript 367k] - [gzipped 137k]

%$%cindex lambda-S

@InProceedings{AMNS97,
  author = 	 {Arvind and Maessen, J-W. and Nikhil, R.S. and Stoy, J.},
  title = 	 {{Lambda-S: an Implicitly Parallel Lambda-Calculus with
                  Letrec, Synchronization and Side-Effects}}, 
  booktitle = 	 {ICFP'97 ---  International Conference on Functional Programming},
  year =         1997,
  note =	 {Also: CSG Memo 383},
  annote = {The lambda-S calculus extends the lambda calculus with letrecs,
   barriers, and updatable memory cells. It is based on observational
   equivalence rather than syntactic comparison. Letrec are needed to preserve
   sharing and side-effects. Adding updatable memory cells (I-structures and
   M-structures), for synchronisation of parallel threads, yields
   non-determinism. lambda-S is the foundation for implicitly parallel
   compilation of pH but should be applicable in a more general setting for
   multi-threaded, fine-grained parallelism. A subset of lambda-S, lambda-I,
   which eliminates M-structures and only uses I-structures, is very close to
   the model of logic (single assignment) variables.},
  url = {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-393.ps.gz},
  abstract =    "Lambda-S extends the lambda-calculus with letrecs, barriers, and updateable
memory cells  with synchronized operations. The  calculus is designed to be
useful for   reasoning   about compiler optimizations  and,   thus,  allows
reductions   anywhere,  even  inside    lambda's. Despite the   presence of
side-effects, the calculus  retains fine-grained, implicit  parallelism and
non-strict functions; there  is no global, sequentializing store. Barriers,
for sequencing, capture a  robust notion of termination.  Although lambda-S
was developed as a foundation for  the parallel functional languages pH and
Id, we believe that barriers give it  wider applicability --- to sequential
and explicitly parallel  languages. In this paper  we describe the lambda-S
calculus and its   properties which are  based  on a notion of   observable
information in a term. We also describe reduction strategies to compute the
maximal observable information."
}

%CSG-Memo-374
%         Normalizing Strategies for Multithreaded Interpretation and
%     Compilation of Non-Strict Languages, Shail Aditya, May 1995, (26
%     pages).
%     [postscript 309k] - [compressed 134k] - [gzipped 98k]


@TechReport{Adit95,
  author = 	 {Aditya, S.},
  title = 	 {{Normalizing Strategies for Multithreaded Interpretation
                   and Compilation of Non-Strict Languages}}, 
  year = 	 {1995},
  number = 	 {CSG Memo 374},
  month = 	 {May},
  annote = 	 {talks about demand vs data-driven evaluation and examines
                  the whole spectrum inbetween},
  url =  {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-374.ps.gz},
  abstract =     "Execution of programs written in non-strict languages such as Haskell or
Id/pH   require the ability  to   dynamically schedule multiple threads  of
computation. This  is  because   the exact  data   dependencies   among the
sub-expressions (and hence their  ordering) cannot be completely determined
at  compile-time. Synchronizing  data  structures such  as I-structures and
M-structures    also  need  a   multithreaded   execution model  because  a
computation that gets blocked on a synchronizing data structure can only be
resumed  if another  computation produces  that   value. However, there  is
considerable flexibility in  choosing the granularity  of threads and their
dynamic scheduling strategy when implementing such a multithreaded model on
existing sequential and parallel platforms, leading to different trade-offs
between the exposed parallelism and the resource requirements.

This paper presents a formal framework for characterizing and exploring the
spectrum   of  such trade-off  points    --- from purely eager,  fine-grain
dataflow execution to purely lazy,  demand-driven evaluation. We present  a
non-strict,  kernel language and its  multithreaded  execution model in the
form  of an  abstract machine   specification. The  machine is capable   of
interpreting  non-strict  programs  using different normalizing  strategies
that expose different amounts of parallelism  and resource requirements. In
particular, we present a mixed normalizing strategy for interpreting kernel
language programs which  falls  somewhere between purely  eager  and purely
lazy evaluation. We also present   compilation rules for compiling   kernel
language  programs  into a  set of  threads  that may  be executed   on the
abstract machine following the mixed evaluation strategy."
}

%CSG-Memo-382
%         A Multithreaded Substrate and Compilation Model for the Implicitly
%     Parallel Language pH, Arvind, Alejandro Caro, Jan-Willem Maessen and
%     Shail Aditya, May 1996, Submitted to LCPC-96, (27 pages).

@InProceedings{ACMA96,
  author = 	 {Arvind and Caro, A. and Maessen, J-W. and Aditya, S.},
  title = 	 {{A Multithreaded Substrate and Compilation Model for the Implicitly Parallel Language pH}},
  booktitle = 	 {{LCPC-96}},
  year =	 1996,
  month =	 {May},
  note =	 {Also: CSG Memo 382},
  annote =	 {Describes compilation of pH via lambda-S calculus; avoids
                  dataflow representation as intermediate step},
  abstract =     "We describe the compilation of the non-strict, implicitly parallel language
pH to symmetric multiprocessors (SMPs) in several steps. We introduce the
S-calculus as a robust foundation for the semantics of pH. Next, we define
a shared-memory threaded abstract machine (SMT) that captures the essence
of our compilation target, a modern SMP. Finally, we describe a complete
syntax directed translation of S to SMT instructions. The paper makes three
important contributions: it is the first implementation of pH based on
direct semantics of barriers; second, in contrast to earlier work, the
multithreaded code generated uses suspensive threads; and third, the
compilation rules generate code from S source code directly, without
resorting to intermediate dataflow-style graphs."
}

@PhdThesis{Shaw98,
  author = 	 {Shaw, A.},
  title = 	 {{Compiling for Parallel Multithreaded Computation on Symmetric Multiprocessors}},
  school = 	 {Department of Electrical Engineering and Computer Science},
  year = 	 1998,
  address =	 {M.I.T.},
  month =	 {January},
  annote =	 {efficient compilation of Id-S, a single-assignment, non-functional dialect of Id; focus on RTS issues},
  documentURL =  {ftp://csg-ftp.lcs.mit.edu/pub/papers/theses/shaw-phd.ps.gz},
  abstractURL =  {http://www.csg.lcs.mit.edu:8001/pubs/cgi-bin/abstract.pl?/home/www/ftp/pub/papers/theses/shaw-phd.abs},
  abstract =     "
Shared-memory symmetric   multiprocessors   (SMP')s based  on  conventional
microprocessors are by far the most common parallel architecture today, and
will continue  to be so for  the  forseeable future.  This thesis describes
techniques to compile   and schedule Id-S,   a  dialect of the   implicitly
parallel language Id, for execution on SMP's.

We   show   that   previous   implementations   of   Id   for  conventional
microprocessors incurred an overhead of at  least 40-300% over an efficient
sequential implementation of Id-S. We break down this overhead into various
presence-tag checking and  scheduling  overheads. Given this overhead,   we
conclude that  a fine-grained, element-wise synchronizing implementation of
Id is not suitable for use on small-scale SMP's.

We then  describe a parallelization technique  for Id-S that discovers both
DAG  and      loop parallelism.   Our    parallelization   exploits  Id-S's
single-assignment  semantics  for data structures.  We  show that  for many
programs, our  technique can discover ample  parallelism, without  need for
Id's         traditional   nonstrict,   fine-grained,     producer-consumer
semantics. Because our parallelization eliminates the need for presence-tag
checking and creates coarser-grained units of  work, the parallelized codes
only incur a small overhead versus sequential execution.

Finally, we describe   code-generation  and   scheduling  techniques  which
produce efficient parallel executables which we run on a Sun Ultra HPC 5000
SMP. We  compare speedups  of parallelized Id-S  codes  using two different
schedulers: an SPMD scheduler, and a  more general multithreaded scheduler.
We  describe the   advantages and disadvantages    of each  scheduler,  and
quantify the  limitations in speedups for each  scheduler which  are due to
parallelization, code generation, and scheduling."
}

@Book{NiAr99,
  author =	 {Nikhil, R.S. and Arvind},
  title = 	 {{Implicit Parallel Programming in pH}},
  publisher = 	 JWS,
  year = 	 1999,
  note =	 {To appear},
  annote =	 {General introduction to implicitly parallel programming and functional programming; written as a textbook without prerequisites; focus in intro to functional programming is on data types; advanced issues cover sequencing (to reduce resource consumption), I/M-structures}
}

%CSG-Memo-381 [abstract]
%         Performance Tuning Scientific Codes for Dataflow Execution, Andrew
%     Shaw, Arvind and R. Paul Johnson, July 1996, To Appear in PACT '96,
%     (12 pages).
%     [postscript 440k]


@PhdThesis{Cull90,
  author = 	 {Culler, D.E.},
  title = 	 {{Managing Parallelism and Resources in Scientific
                  Dataflow Programs}},
  school =       {Laboratory for Computer Science},
  year = 	 1990,
  number =	 {LCS TR-446},
  address =	 {M.I.T.},
  annote =	 {includes evaluation of k-bounded loops}
}

%$%cindex Active Messages

@InProceedings{vCGS92,
  author = 	 {{von Eicken}, T. and Culler, D.E. and Goldstein, S.C. and
                  Schauser, K.E.},
  title = 	 {{Active Messages: a Mechanism for Integrated Communication
                   and Computation}},
  booktitle = 	 {{ISCA'92 --- International Symposium on Computer Architecture}},
  year =	 1992,
  publisher =	 {ACM Press},
  address =	 {Gold Coast, Australia},
  month =	 {May},
  abstractURL =  {http://http.cs.berkeley.edu/\~{}sethg/abs/isca92.html},
  url =          {http://www.cs.ucsb.edu/\~{}schauser/papers/92-isca.ps},
  abstract =     "The design  challenge for  large-scale  multiprocessors is (1)  to minimize
communication overhead, (2) allow communication to overlap computation, and
(3) coordinate the two without  sacrificing processor cost/performance.  We
show that existing message  passing multiprocessors have unnecessarily high
communication   costs.  Research  prototypes  of   message driven  machines
demonstrate    low     communication   overhead,   but    poor    processor
cost/performance.  We  introduce a  simple communication mechanism,  Active
Messages, show   that it is  intrinsic to  both architectures,  allows cost
effective use  of    the  hardware, and offers     tremendous  flexibility.
Implementations on nCUBE/2   and CM-5 are described  and  evaluated using a
split-phase  shared-memory extension to  C, Split-C.  We  further show that
active messages are   sufficient to  implement   the dynamically  scheduled
languages  for which message  driven  machines  were designed.  With   this
mechanism,  latency   tolerance becomes   a  programming/compiling concern.
Hardware support for active messages is desirable and we outline a range of
enhancements to mainstream processors."
}

%$%cindex TAM

@Article{CGSv93,
  author = 	 {Culler, D.E. and Goldstein, S.C. and
                  Schauser, K.E. and {von Eicken}, T.},
  title = 	 {{TAM --- A Compiler Controlled Threaded Abstract Machine}},
  journal = 	 JPDC,
  year = 	 1993,
  volume =	 18,
  pages =        {347--370},
  month =	 {June},
  annote = 	 {main reference for specifying TAM},
  abstractURL =  {http://http.cs.berkeley.edu/\~{}sethg/abs/jpdc93.html},
  url =          {http://www.cs.ucsb.edu/\~{}schauser/papers/93-jpdc-tr.ps},
  abstract =     "The Threaded Abstract Machine (TAM)  refines  dataflow execution models  to
address  the critical constraints  that modern parallel architectures place
on the compilation of  general-purpose parallel programming  languages. TAM
defines  a   self-scheduled machine language    of  parallel threads, which
provides a path  from dataflowgraph program representations to conventional
control flow. The  most important feature of TAM  is the way it exposes the
interaction between  the   handling  of asynchronous message    events, the
scheduling  of   computation,  and  the      utilization of  the    storage
hierarchy. This  paper provides a complete  description of TAM and codifies
the model in terms of a pseudo  machine language TL0. Issues in compilation
from a high  level parallel language to TL0   are discussed in general  and
specifically in regard to the  Id90 language. The  implementation of TL0 on
the CM-5 multiprocessor is explained  in detail. Using this implementation,
a cost model is developed for the various TAM  primitives. The TAM approach
is   evaluated on  sizable Id90  programs   on a 64   processor system. The
scheduling hierarchy of quanta and threads is  shown to provide substantial
locality while  tolerating long latencies.  This allows  the average thread
scheduling cost to be extremely low.", 
}

@InProceedings{ScGo95,
  author = 	 {Schauser, K.E. and Goldstein, S.C.},
  title = 	 {{How Much Non-strictness do Lenient Programs Require?}},
  booktitle = 	 {{FPCA'95 ---  Conference on Functional Programming
                   Languages and Computer Architecture}},
  publisher =	 {ACM Press},
  address =      {La Jolla, California, June 26--28},
  year = 	 {1995},
  abstractURL =  {http://http.cs.berkeley.edu/\~{}sethg/abs/fpca95.html},
  url =          {http://www.cs.ucsb.edu/\~{}schauser/papers/95-fpca.ps},
  abstract =     "Lenient languages, such as Id90, have been touted as among the best functional languages for massively parallel machines [AHN88]. Lenient evaluation combines non-strict semantics with eager evaluation [Tra91]. Non-strictness gives these languages more expressive power than strict semantics, while eager evaluation ensures the highest degree of parallelism. Unfortunately, non-strictness incurs a large overhead, as it requires dynamic scheduling and synchronization. As a result, many powerful program analysis techniques have been developed to statically determine when non-strictness is not required [CPJ85, Tra91, Sch94]. This paper studies a large set of lenient programs and quantifies the degree of non-strictness they require. We identify several forms of non-strictness, including functional, conditional, and data structure non-strictness. Surprisingly, most Id90 programs require neither functional nor conditional non-strictness. Many benchmark programs, however, make use of a limited form of data structure non-strictness. The paper refutes the myth that lenient programs require extensive non-strictness."
}

@Article{GSC96,
  author = 	 {Goldstein, S.C. and Schauser, K.E. and Culler, D.E.},
  title = 	 {{Lazy Threads: Implementing a Fast Parallel Call}},
  journal = 	 JPDC,
  year = 	 {1996},
  volume =	 {37},
  number =	 {1},
  pages =	 {5--20},
  keywords =     {parallel runtime-system, lazy task creation, granularity},
  url =          {http://http.cs.berkeley.edu/\~{}sethg/papers/jpdc.ps.Z},
  abstractURL =  {http://http.cs.berkeley.edu/\~{}sethg/abs/jpdc.html},
  abstract =     "In this paper we describe lazy threads, a new approach for implementing
multi-threaded execution models on conventional machines. We show how they
can implement a parallel call at nearly the efficiency of a sequential
call. The central idea is to specialize the representation of a parallel
call so that it can execute as a parallel-ready sequential call. This
allows excess parallelism to degrade into sequential calls with the
attendant efficient stack management and direct transfer of control and
data, yet a call that truly needs to execute in parallel, gets its own
thread of control. The efficiency of lazy threads is achieved through a
careful attention to storage management and a code generation strategy that
allows us to represent potential parallel work with no overhead.",
}

@PhdThesis{Gold97,
  author = 	 {Goldstein, S.C.},
  title = 	 {{Lazy Threads: Compiler and Runtime Structures for
                  Fine-Grained Parallel Programming}},
  school = 	 {University of California, Berkeley},
  year = 	 1997,
  abstract =     "Many modern parallel  languages   support dynamic creation of    threads or
require multithreading in their  implementations. The threads describe  the
logical   parallelism in  the program. For   ease  of expression and better
resource  utilization, the logical  parallelism in  a program often exceeds
the physical parallelism of the machine and leads to applications with many
fine-grained  threads. In practice,  however, most logical threads need not
be  independent threads.  Instead, they  could be  run as sequential calls,
which are  inherently cheaper than  independent  threads. The  challenge is
that one cannot generally predict  which logical threads can be implemented
as sequential   calls. In lazy multithreading   systems each logical thread
begins execution  sequentially     (with  the attendant   efficient   stack
management and direct transfer of control and data). Only if a thread truly
must execute in parallel does it get its own thread of control.

This  dissertation  presents  new   implementation  techniques   for   lazy
multithreading  systems on conventional machines  and compares a variety of
design  choices. We  develop an  abstract  machine that makes  explicit the
design   decisions for achieving  lazy   multithreading.  We introduce  new
options on each of  the four axes in the  design space: the storage  model,
the  thread   representation, the  disconnection  method,  and the queueing
mechanism.  Stacklets, our new storage   model,  allows parallel calls   to
maintain  the invariants of sequential calls.  Thread seeds, our new thread
representation, allows  threads   to  be stolen  without  requiring  thread
migration or shared  memory. Lazy-disconnect, our new disconnection method,
does not restrict the use of pointers. Implicit  and Lazy queueing, our two
new     queueing    mechanisms,    eliminate  the       need for   explicit
bookkeeping. Additionally, we develop  a core set of compilation techniques
and runtime primitives that form the basis for the efficient implementation
of any design point.

We have evaluated the  different  approaches by  incorporating them into  a
compiler for an explicitly  parallel  extension of  Split-C. We  show  that
there  exist points in  the   design space (e.g.,  stacklet, thread  seeds,
lazy-disconnect, and lazy  queueing) for which fine-grained parallelism can
be efficiently supported even on distributed memory machines, thus allowing
programmers freedom to specify the parallelism in a program without concern
for excessive overhead."
}

%$%cindex Manchester Dataflow Machine
%$%xref{GKW85,Manchester Dataflow Machine,,par-others.bib,}

%$%cindex PASTEL

@InProceedings{HoLo94a,
  author = 	 {Hogen, G. and Loogen, R.},
  title = 	 {{Efficient Organization of Control Structures in Distributed
                   Implementations}},
  booktitle = 	 {{CC'94 --- International Conference on Compiler Construction}},
  volume =	 786,
  series =	 LNCS,
  year =	 1994,
  publisher =	 {Springer Verlag},
  pages =	 {98--112},
  keywords =     {parallel runtime-systems},
  url =  {http://www-i2.informatik.rwth-aachen.de/OldStaff/hogen/PUBLICATIONS/cc94.ps.gz},
  abstractURL =  {http://www-i2.informatik.rwth-aachen.de/OldStaff/hogen/PUBLICATIONS/cc94.abs.txt},
  abstract =     "A new technique for the management of control structures in 
           distributed implementations of dynamic process systems is presented.
           Instead of storing the runtime stacks of parallel processes as linked
           lists of activation blocks in a heap structure, the local stacks of
           several parallel processes, which are executed on the same processor
           element, are stored in an interleaved manner on a single physical
           stack (within each processor element), called the {\em meshed stack}.
           The technique ensures that there is almost no overhead for the
           evaluation of single processes due to the parallel environment. In   
           principle, the meshed stack technique is independent of the 
           implemented language. We explain it for the parallel implementation 
           of functional languages."
}

@InProceedings{HoLo93,
  author = 	 {Hogen, G. and Loogen, R.},
  title = 	 {{Stack Management of Runtime Structures in Distributed
                  Implementations}},
  booktitle = 	 {PLILP'93 --- Symposium on Programming Language Implementation and Logic Programming},
  volume =	 714,
  series =	 LNCS,
  year =	 1993,
  publisher =	 {Springer Verlag},
  pages =	 {416--417},
  annote =	 {just an abstract? better reference: HoLo94a},
}


@InProceedings{HoLo95,
  author = 	 {Hogen, G. and Loogen, R.},
  title = 	 {{Parallel Functional Implementations: Graphbased
                  vs. Stackbased Reduction}}, 
  booktitle = 	 {{Fuji International Workshop on Functional and
                   Logic Programming}},
  year =	 1995,
  publisher =	 {World Scientific},
  annote =       {Similar to \cite{HoLo94} but more detailed measurements},
  abstract = "The meshed stack is a technique for the management of control structures in
distributed implementations of dynamic  process systems. In the environment
of a parallel abstract machine for the execution of functional programs, we
compare    this   approach   to   the      classical graphbased   reduction
technique. Experimental results will show, that the stackbased reduction is
more  efficient  in  space  and time, especially   in  relation  to garbage
collection. Also, the stack technique is a  natural extension of sequential
reduction machines."
}


@Article{BoWe73,
  author = 	 {Bobrow, D.G. and Wegbreit, B.},
  title = 	 {{A Model and Stack Implementation of Multiple Environments}},
  journal = 	 CACM,
  year = 	 1973,
  volume =	 16,
  pages =	 {591--602},
  annote =	 {first reference to spaghetti stacks aka meshed stacks}
}

@InProceedings{TMY94,
  author = 	 {Taura, K. and Matsuoka, S. and Yonezawa, A.},
  title = 	 {{StackThreads: An Abstract Machine for Scheduling Fine-Grain  Threads on Stock Cpus}},
  booktitle = 	 {Workshop on Theory and Practice of Parallel Programming},
  series =       LNCS,
  volume =       {907},
  publisher =    S-V,
  year =	 1995,
  url =  {ftp://ftp.yl.is.s.u-tokyo.ac.jp/pub/papers/jspp94-stackthreads-a4.ps.gz},
  abstract =     "A software scheduling method of fine-grain threads is presented. Typical
granurality of a thread is a single procedure invocation. The
implementation is described in terms of our developed abstract machine,
StackThreads. StackThreads is designed so that it runs efficiently on any
stock CPUs including one that have register windows. In the proposed
method, an asynchronous procedure invocation (a procedure call attached
with a thread creation) is done in less than 10 additional RISC
instructions to normal procedure calls in traditional sequential languages
such as C, as long as the thread terminates without blocking. StackThreads
unifies asynchronous and synchronous procedure calls, deriving synchronous
calls by combination of asynchronous calls + explicit synchronization
between the caller and the callee. Therefore, each thread does not have to
have a stack for intra-thread procedure call, simplifying the management of
activation records. Performance measurements indicate that the overhead of
a thread creation associated with a procedure call is about 50\% in a very
procedure call intensive program, and effectively negligible (6\%) in a
more typical program (a polynomial computation). StackThreads serves as a
compiler target of general purpose languages such as concurrent
object-oriented languages or functional languages on distributed memory
multicomputers where the efficiency of intra-node thread management is
crucial."
}

% File: lfp92.ps
@INPROCEEDINGS{TCS92,
  AUTHOR = {Traub, K.R. and Culler, D.E. and Schauser, K.E.},
  TITLE = {{Global Analysis for Partitioning Non-Strict Programs
            into Sequential Threads}},
  BOOKTITLE = {LFP'92 --- Conference on LISP and Functional Programming},
  YEAR = {1992},
  ADDRESS = {San Francisco, CA},
  MONTH = jun
}

%$%cindex pHluid

@InProceedings{FlNi96,
  author = 	 {Flanagan, C. and Nikhil, R.S.},
  title = 	 {{pHluid: The Design of a Parallel Functional Language
                   Implementation on Workstations}},
  booktitle = 	 {ICFP'96 --- International Conference on Functional Programming},
  year =	 1996,
  publisher =	 {ACM Press},
  address =	 {Philadelphia, Pennsylvania, May 24--26},
  pages =	 {169--179}
}

%$%cindex Cid

@InProceedings{Nikh94,
  author = 	 {Nikhil, R.S.},
  title = 	 {{Cid: A Parallel ``Shared-memory'' C for Distributed Memory Machines}},
  booktitle = 	 {{Workshop on Languages and Compilers for Parallel Computing}},
  volume =	 892,
  series =	 LNCS,
  year =	 1994,
  month =        {August},
  address =	 {Ithaca, NY},
  publisher =    S-V,
  pages =	 {376--390},
  annote =	 {most recent paper on Cid},
  url =  {http://www.research.digital.com/CRL/personal/nikhil/cid/cid.ps.Z},
  abstract =     "Cid is a parallel, ``shared-memory'' superset of C for distributed-memory
machines. A major objective is  to keep the entry  cost low. For users: the
language  should    be easily  comprehensible   to a   C   programmer.  For
implementors: it  should run  on  standard hardware  (including workstation
farms) it should not  require major new  compilation techniques  (which may
not even be widely  applicable) and it should  be compatible with  existing
code,   run-time systems  and  tools.   Cid is  implemented   with a simple
pre-processor  and   a    library,   uses    available  C  compilers    and
packet-transport primitives, and links with existing libraries.

Cid  extends  C  with  MIMD  threads  and   global objects   and  a  common
``join-variable''  mechanism  for  dealing with asynchronous  actions.  The
number of threads is arbitrary  and may vary dynamically.  Any C object can
be registered as a  global object; the resulting  ``global pointer'' may be
used  to access the  object from any PE (processing  element)  in a locked,
coherent, cached   manner.     Combining  locking  with   cacheing  reduces
communication traffic.    Being entirely in      software, there  is    the
opportunity     to     vary     coherence  protocols     to    suit     the
application. Distributed  data structures may built  by linking  across PEs
with global pointers, or by using Cid's distributed array primitives.

The  programmer   does no explicit  message-passing,   but Cid exposes some
abstractions  of the distributed memory machine.  PEs have separate address
spaces; threads may be forked to different PEs, but a  thread does not span
PE boundaries.  Threads access remote  data only  via thread arguments  and
results, and global   objects. Finally, several   Cid operations  are  made
explicitly asynchronous, in recognition of the underlying communication and
synchronization costs.

In this  paper, we describe the  language, our first, multi-threaded
implementation, some preliminary results, and compare with related systems."
}


@TechReport{NiSi94,
  author = 	 {Nikhil, R.S and Singla, A.},
  title = 	 {{Automatic Granularity Control and Load-Balancing in Cid}},
  institution =  {Cambridge Research Lab},
  year = 	 1994,
  address =	 {Digital Equipment Corp},
  month =	 {December},
  url =  {http://www.research.digital.com/CRL/personal/nikhil/cid/cid\_granularity.ps.Z},
  abstract =     "Cid  is a parallel  extension to C for  general-purpose MIMD computation on
distributed-memory     machines    such    as    workstation   farms    and
multicomputers[10]. A  Cid program may have  an arbitrary number of threads
(distributed across a  fixed number of  PEs) which may communicate  through
arguments/results and  via global   access  to  arbitrary C   objects  with
software-maintained coherent cacheing.

By its very nature, general-purpose MIMD computation involves unpredictable
thread-structure   and  unpredictable  latencies  (duration  of   a   fork,
synchronization waits). This  is compounded in distributed  memory machines
by   high    communication      overheads     and    paucity   of    global
information.   Multi-threading  is    essential   for  load-balancing   and
latency-tolerance. Thread-granularity  is   a tradeoff: fine-grain  threads
permit    better load-balancing and    latency-tolerance  but  incur higher
thread-management overheads.

In  this paper we describe several  automatic methods  available in Cid for
load-balancing  of threads across PEs   and for controlling granularity. We
describe language features,  their implementation, and their performance on
three test programs.",
}

@InProceedings{Nikh95,
  author = 	 {Nikhil, R.S.},
  title = 	 {{Parallel Symbolic Computing in Cid}},
  booktitle = 	 {{PSLS'95 --- Workshop on Parallel Symbolic Computing}},
  number =	 1068,
  series =	 LNCS,
  year =	 1995,
  publisher =	 S-V,
  address =	 {Beaune, France, October 2--4},
  pages =	 {217--242},
  url =  {http://www.research.digital.com/CRL/personal/nikhil/cid/cid\_symbolic.ps.Z},
  abstract =     "We  have designed  and  implemented  a  language  called Cid  for  parallel
applications with   recursive  linked data   structures (e.g.,lists, trees,
graphs) and complex control structures (data  dependent, recursion). Cid is
unique in that, while targeting distributed memory machines, it attempts to
preserve  the traditional ``MIMD  threads plus lock-protected shared data''
programming model that is standard on shared memory machines.

Cid is a small extension  to C because  although Lisp, functional and logic
programming  languages  are attractive  for symbolic  computing, it appears
nearly  impossible to  sway the momentum  of  C.  Cid  uses only a  simple,
platform-independent   preprocessor  for  localized expansion   of  its few
extensions; the preprocessed code is  compiled using a standard C compiler,
and linked with the Cid runtime system, which is written  in standard C. By
relying on standard  C compilers, Cid exploits  the continuing advances  in
optimizing C  compilers and  programming   tools, and  remains   completely
compatible with existing, even pre-compiled, C code.

Cid is designed for distributed address  spaces because most truly scalable
machines  are likely  to be multicomputers   (even if individual  nodes are
shared-memory   multiprocessors,    or  SMPs).  However,   Cid   is   not a
message-passing language; it adheres to  the traditional MIMD threads model
with a  shared, global, synchronized   heap of objects.  Cid  has  numerous
design  features to accomodate  the relatively  high communication costs of
multicomputers,  such  as latency-tolerant multithreading,  automatic  load
balancing  and  granularity  control,   automatic coherent  object-caching,
combined  synchronization  with  object   access, and   asynchronous object
pre-fetching, all   of  which we  believe  are  necessary  in  any symbolic
language implementation.

In this paper,  we present an  overview  of Cid with examples  using linked
recursive data structures, explaining   some  of  our design  choices   and
aspects  of  its implementation. We present    measurements of the  cost of
various    Cid operations,  and some   preliminary   observations about the
effectiveness of Cid's automatic load balancing mechanisms. We also compare
Cid with other approaches to developing parallel versions  of C and C++ for
distributed memory machines."
}


@TechReport{NRR*98,
  author = 	 {Nikhil, R.S. and Ramachandran, U. and Rehg, J.M. and {Halstead, Jr.}, R.H. and Joerg, C.F. and Kontothanassis, L.},
  title = 	 {{Stampede --- A Programming System for Emerging Scalable Interactive Multimedia Applications}},
  institution =  {Cambridge Research Laboratory},
  year = 	 {1998},
  OPTkey = 	 {},
  OPTtype = 	 {Technical Report},
  OPTnumber = 	 {CRL 98/1},
  OPTaddress = 	 {},
  OPTmonth = 	 {May},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTURL =       {},
}

@Article{Cann92,
  author = 	 {Cann, D.},
  title = 	 {{Retire Fortran? A Debate Rekindled}},
  journal = 	 CACM,
  year = 	 1992,
  volume =	 35,
  number =	 8,
  pages =	 {81--89},
  month =	 {August},
  url =          {ftp://sisal.llnl.gov/pub/sisal/publications.dir/retire.ps.Z}
}

%$%cindex SISAL

%  D.~Cann,
%  ``Retire Fortran? A Debate Rekindled'',
%  {\em Comm. ACM}, {\bf 35}(8), August 1992, pp.~81--89.


% ToDo: find better (more recent) SISAL reference
%$%xref{BCFO91,SISAL Language Description,,ws-pfp.bib,}

@TechReport{BCFO91,
  author =       {BÅˆhm, A.P.W. and Cann, D.C. and  Feo, J.T. and
                 Oldehoeft, R.R.},
  title =        {{SISAL 2.0 Reference Manual}},
  institution =  {Computer Science Department}, 
  address =      {Colorado State University},
  optaddress =      "Fort Collins, CO",
  number =       {CS-91-118},
  month =        nov,
  year =         {1991},
  abstract =     "",
  keywords =     {SISAL, Single Assignment},
}


@InProceedings{FMS*95,
  author = 	 {Feo, J. and Miller, P. and Skedziewlewski, S. and Denton, S. and Soloman, C.},
  title = 	 {{SISAL 90}},
  year =         1995,
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  pages =        {83--96},
  address =	 {Denver, Colorado, April 10--12},
}

@article{GKW85,
author = {Gurd, J.R.  and Kirkham, C.C.  and Watson, I. },
title = {{The Manchester Prototype Dataflow Computer}},
journal = {Communications of the ACM},
volume = {28},
number = {1},
month = {January},
year = {1985},
pages = {34--52},
descr = {PARHW},
}


@InProceedings{FrAn95,
  author = 	 {Freeh, V.W. and Andrews, G.R.},
  title = 	 {{{\tt fsc}: A Sisal Compiler for Both Distributed- and
                   Shared-Memory Machines}},
  year =         {1995},
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  address =	 {Denver, Colorado, April 10--12},
  url =          {ftp://sisal.llnl.gov/pub/hpfc/papers95/paper20.ps},
  abstract =     "This paper describes a prototype Sisal  compiler that supports distributed-
as  well as shared-memory  machines.  The compiler,  fsc, modifies the code
generation  phase  of the  optimizing Sisal   compiler,   osc, to  use  the
Filaments library  as  a  run-time system.  Filaments  efficiently supports
fine-grain parallelism   and  a  shared-memory  programming  model.   Using
fine-grain threads makes it possible to implement recursive as well as loop
parallelism;   it   also  facilitates   dynamic  load  balancing.   Using a
distributed implementation of shared memory (a DSM) simplifies the compiler
by obviating the need for explicit message passing.
"
}

@TechReport{HaBo92,
  author = 	 {Haines, M. and Bohm, W.},
  title = 	 {{Software Multithreading in a Conventional Distributed
                   Memory Multiprocessor}},
  institution =  {Colorado State University},
  year = 	 1992,
  number =	 {CS-92-126},
  month =	 {September}
}

@Article{PAM94,
  author = 	 {Pande, S.S. and Agrawal, D.P. and Mauney, J.},
  title = 	 {{Compiling Functional Parallelism on Distributed-Memory
                   Systems}}, 
  journal = 	 {IEEE Parallel and Distributed Technology},
  year = 	 1994,
  pages =	 {64--76}
}

@Article{OlCa88,
  author =       "R. R. Oldehoeft and D. C. Cann",
  title =        "{Applicative Parallelism on a Shared-Memory
                 Multiprocessor}",
  journal =      "IEEE Software",
  volume =       "5",
  pages =        "62--70",
  number =       "1",
  month =        jan,
  year =         "1988",
  keywords =     "Parallel Functional Programming",
}

@InProceedings{SSM88,
  author =       "V. Sarkar and S. Skedzielewski and P. Miller",
  title =        "{An Automatically Partitioning Compiler for Sisal}",
  booktitle =    "CONPAR 88",
  address =      "Manchester, UK, September 12--16",
  year =         "1988",
  pages =        "26--33",
  volume =       "B",
  publisher =    "British Computer Society",
  abstract =     "In this paper, we describe a compiler that
                 automatically repackages Sisal programs to achieve the
                 grain size needed to run efficiently on different
                 multiprocessors. The compiler is based on an existing
                 implementation for Sisal on the Sequent Balance
                 multiprocessor, and on previous work on automatic
                 partitioning of Sisal programs.",
  keywords =     "Partitioning, Compilation",
}


@InProceedings{SaHe86,
  author = 	 {Sarkar, V. and Hennessy, J.},
  title = 	 {{Partitioning Parallel Programs for Macro-Dataflow}},
  pages =	 {202--211},
  booktitle =    {{LFP'86 --- Conference on Lisp and Functional Programming}},
  year =	 {1986},
  publisher =	 {ACM Press},
  month =	 {August}
}

@Book{Sark89,
  author = 	 {Sarkar, V.},
  title = 	 {{Partitioning and Scheduling Parallel Programs for
                   Multiprocessors}},
  publisher = 	 MIT,
  year = 	 {1989},
  series =	 {Research Monographs in Parallel and Distributed Computing},
  keywords =	 {dataflow}
}

@InBook{Sked91,
  author =	 {Skedzielewski, S.},
  title = 	 {{Parallel Functional Languages and Compilers}},
  chapter = 	 {{Sisal}},
  pages =        {105--158},
  publisher = 	 {ACM Press},
  year = 	 1991,
  series =	 {Frontier Series}
}

@InProceedings{GaLe87,
  author =       {Gaudiot, J.L. and Lee, L.T.},
  title =        {{Multiprocessor Systems Programming in a High-Level
                 Data-Flow Language}},
  booktitle =    {{PARLE'87 --- Parallel Architectures and Languages
                 Europe}},
  opteditor =       {J. W. de Bakker and A. J. Nijman and P. C. Treleaven},
  address =      {Eindhoven, The Netherlands, June 15--19},
  year =         {1987},
  pages =        {134--151},
  series =       LNCS,
  volume =       {258},
  publisher =    S-V,
  abstract =     "The data-flow model of computation is an attractive
                 methodology for multiprocessor programming for it
                 offers the potential for unlimited parallelism
                 detection at no programmer's expense. It is here
                 applied to a distributed architecture based on a
                 commercially available microprocessor (the Inmos
                 Transputer). In this project, we have integrated the
                 high-level data driven principles of scheduling within
                 the Transputer architecture so as to provide high
                 programmability of our multicomputer system. A
                 completet programming environment which translates a
                 complex data-flow program graph into occam has been
                 developed and is presented in this paper. We here
                 describe in detail the mapping from the SISAL
                 high-level constructs into the low-level mechanisms of
                 the Transputer (synchronization, structure
                 representation, etc.). The partitioning issues
                 (granularity of the graph) are presented and several
                 solutions based upon both data-flow analysis
                 (communication costs) and program syntax (program
                 structure) are proposed and have been implemented in
                 our programming environment. Finally, we present and
                 analyze graph allocation and optimization schemes to
                 improve the performance of the resulting occam
                 program.",
  keywords =     "SISAL, Dataflow",
}
 
@Article{McGi89,
  author = 	 {McGreary, C. and Gill, H.},
  title = 	 {{Automatic Determination of Grain Size for Efficient
		  Parallel Processing}},
  journal =	 {Communications of the ACM},
  year =	 {1989},
  volume =	 {32},
  annote =       {general dataflow}
}

@Article{KuLe88,
  author = 	 {Kuatrachue, B. and Lewis, T.},
  title = 	 {{Grain Size Determination for Parallel Processing}},
  journal =	 i3es,
  year =	 {1988},
  volume =	 {5},
  number =	 {1},
  pages =	 {23--32},
  month =	 {January},
  annote =       {general dataflow}
}

%$%cindex Sigma-1

@InProceedings{Shim86,
  author = 	 {Shimada, T. and Hiraki, K. and Nishida, K. and Sekigushi, S.},
  title = 	 {{Evaluation of a Prototype Data Flow Processor of the
                  Sigma-1 for Scientific Computations}},
  booktitle = 	 {International Symposium on Computer Architecture},
  year =	 1986,
  month =	 {June},
  annote =	 {Reference taken from \cite{RuSa87}}
}

@InProceedings{Amam86,
  author = 	 {Amamiya et al},
  title = 	 {{Implementation and Evaluation of a
                   List-Processing-Oriented Data Flow Machine}},
  booktitle = 	 {International Symposium on Computer Architecture},
  year =	 1986,
  month =	 {June},
  annote =	 {Reference taken from \cite{RuSa87}}
}

@InProceedings{ISK*86,
  author = 	 {Ito, N. and Sato, M. and Kishi, A. and Kuno, E. and
                  Rokusawa, K.},
  title = 	 {{The Architecture and Preliminary Evaluation Results of
                   the Experimental Parallel Inference Machine PIM-D }},
  booktitle = 	 {International Symposium on Computer Architecture},
  year =	 1986,
  month =	 {June},
  keyword =      {Dataflow Machine},
  annote =	 {Reference taken from \cite{RuSa87}},
}


@InProceedings{Ito96,
  author = 	 {Ito, T.},
  title = 	 {{Efficient Evaluation Strategies for Structured Concurrency Constructs in Parallel Scheme Systems}}, 
  booktitle = 	 {{PSLS'95 --- International Workshop on Parallel Symbolic Languages
                  and Systems}},
  pages = 	 {22--52},
  year = 	 {1995},
  volume = 	 {1068},
  series = 	 LNCS,
  publisher =    S-V,
}

%$%cindex Tuki

@InProceedings{CGR89,
  author = 	 {Cox, S. and Glaser, H. and Reeve, M.},
  title = 	 {{Implementing Functional Languages on the Transputer}},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  year =	 1989,
  publisher =	 S-V,
  address =	 {Fraserburgh, Scotland, August 21--23},
  pages =	 {287--295},
  annote =	 {uses Tuki code as intermediate language (data-flow graph)}
}

% ---------------------------------------------------------------------------
%$%node Parallel Garbage Collection, Applications, Implementation of Functional Languages, Top
%$%section Parallel Garbage Collection
% Warning: this is totally incomplete; just a few references I picked up
% along the way
% ---------------------------------------------------------------------------

%$%cindex weighted reference counting

@InProceedings{Beva87,
  author = 	 {Bevan, D.I.},
  title = 	 {{Distributed Garbage Collection Using Reference Counting}},
  booktitle =	 {PARLE'87 --- Parallel Architectures and Languages Europe},
  series =	 LNCS,
  number =	 259,
  pages =        {176--187},
  year =	 1987,
  address =	 {Eindhoven, The Netherlands, June 12--16},
  publisher =	 S-V,
  keywords =     {Parallel garbage collection},
  annote =	 {main reference for weighted reference counting}
}


@InProceedings{WaWa87,
  author = 	 {Watson, P. and Watson, I.},
  title = 	 {{An Efficient Garbage Collection Scheme for Parallel
                   Computer Architectures}},
  booktitle =	 {PARLE'87 --- Parallel Architectures and Languages Europe},
  series =	 LNCS,
  number =	 259,
  pages =        {432--443},
  year =	 1987,
  address =	 {Eindhoven, The Netherlands, June 12--16},
  publisher =	 S-V,
  keywords =     {Parallel garbage collection},
  annote =	 {independent devolpment of weighted reference counting}
}

@InProceedings{Gold89,
  author = 	 {Goldberg, B.},
  title = 	 {{Generational Reference Counting: A reduced-communication
                   storage reclamation scheme}},
  booktitle =    {{PLDI'89 --- Programming Languages Design and
                   Implementation}}, 
  series =       {SIGPLAN Notices},
  year =	 {1989},
  month =	 {June},
  keywords =     {Parallel garbage collection},
  publisher =    {ACM Press},
}

% From Simon's survey paper:

@InProceedings{AEL88,
  author = 	 {Appel, A.W. and Ellis, J.R. and Li, K.},
  title = 	 {{Real-Time Concurrent Collection on Stock Multiprocessors}},
  booktitle =    {{PLDI'88 --- Programming Languages Design and
                   Implementation}}, 
  series =       {SIGPLAN Notices},
  year =	 1988,
  month =	 {June},
  publisher =    {ACM Press},
  pages =        {11--20},
  keywords =     {Parallel garbage collection},
  annote =	 {parallel GC for closely-coupled machines},
  abstract =     " We have designed   and implemented a copying  garbage-collection  algorithm
 that is efficient,  real-time, concurrent, runs on commerial uniprocessors
 and   shared-memory    multiprocessors,   and  requires    no   change  to
 compilers. The algorithm uses  standard virtual-memory hardware  to detect
 references to {"}from space{"} objects and  to synchronize the collector and
 mutator  threads. We have implemented  and measured a prototype running on
 SRC's 5-processor  Firefly.    It  will be straightforward  to   merge our
 techniques with generational  collection.  An  incremental, non-concurrent
 version could be implemented easily on many versions of Unix."
}

@InCollection{Deb87,
  author = 	 {Deb, A.},
  title = 	 {{Parallel Garbage Collection for Graph Machines}},
  booktitle =    {{Workshop on Graph Reduction and Techniques}},
  year =         {1987},
  pages =        {252--264},
  series =       LNCS,
  volume =       {279},
  publisher =    S-V,
  keywords =     {Parallel garbage collection},
  annote =	 {parallel GC for distributed memory machines}
}

@InProceedings{Ruda86,
  author = 	 {Rudalics, M.},
  title = 	 {{Distributed Copying Garbage Collection}},
  booktitle =    {{LFP'86 --- Conference on Lisp and Functional Programming}},
  year =	 {1986},
  publisher =	 {ACM Press},
  month =	 {August}
}

@InProceedings{Lest89a,
  author = 	 {Lester, D.R.},
  title = 	 {{An Efficient Distributed Garbage Collection Algorithm}},
  booktitle = 	 {PARLE'89 --- Parallel Architectures and Languages Europe},
  number = 	 {365/366},
  series = 	 LNCS,
  year = 	 1989,
  publisher =    S-V,
  address = 	 {Eindhoven, The Netherlands},
  month = 	 {June},
  pages = 	 {207--223},
  annote =	 {local compacting, global eeighted reference count; used in the HDG machine},
}

% Seems to be a good survey from some persitent person
@InProceedings{PlSh95,
  author = 	 {Plainfoss\'{e}, D. and Shapiro, M.},
  title = 	 {{A Survey of Distributed Garbage Collection Techniques}},
  booktitle = 	 {IWMM'95 --- International Workshop on Memory Management},
  year = 	 1995,
  address =	 {Kinross, Scotland, U.K.},
  month =	 {September},
  annote =       {survey; oriented toward persistence},
  keywords =     {Parallel garbage collection},
  url =  {ftp://ftp.inria.fr/INRIA/Projects/SOR/SDGC\_iwmm95.ps.gz},
  abstractURL =  {http://www-sor.inria.fr/SOR/docs/SDGC\_iwmm95.html},
  abstract =     "We present the spectrum of distributed garbage collection techniques. We
first describe those reference counting-based techniques and compare them,
in particular with respect to resilience to message failures. Reference
counting-based techniques are acyclic since they are unable to collect
cyclic data structures. We then describe a number of hybrid schemes
improving over distributed reference counting algorithms in order to
collect cyclic garbage. We then describe tracing-based techniques derived
from uniprocessor tracing-based techniques. Finally, we discuss the pros
and cons of each technique."
}


@Misc{JoneON,
  author =	 {Richard Jones},
  title =	 {{The Garbage Collection Bibliography}},
  howpublished = {on-line bibtex library},
  annote =	 {Part of the Collection of CS BIBS in Karlsruhe},
  url =  {http://liinwww.ira.uka.de/bibliography/Compiler/gc.html},
  abstract =     "A comprehensive collection of references on Garbage Collection. It
     also includes some references on explicit heap management, cache
     behaviour, etc."
}

% ---------------------------------------------------------------------------
%$%node Applications, Symbolic Computation, Parallel Garbage Collection, Top
%$%section Applications
% ---------------------------------------------------------------------------

%$%menu
%* Parallel Functional Programming::  
%* Symbolic Computation::	
%$%end menu

%$%node Parallel Functional Programming, Symbolic Computation, Applications, Applications
%$%subsection Parallel Functional Programming

@Book{RuWa95,
  author = 	 {Runciman, C. and Wakeling, D.},
  title = 	 {{Applications of Functional Programming}},
  publisher = 	 {UCL Press},
  year = 	 1995,
  annote =	 {FLARE book}
}

@Article{WuHa96,
  author = 	 {Wu, J. and Harbird, L.},
  title = 	 {{A Functional Database System for Road Accident Analysis}},
  journal = 	 {Advances in Engineering Software},
  year = 	 1996,
  volume =	 26,
  number =	 1,
  pages =	 {29--43}
}

@InProceedings{PoSm93,
  author = 	 {Poulovassilis, A.P. and Small, C.},
  title = 	 {{A Domain-Theoretic Approach to Logic and Functional Databases}},
  booktitle = 	 {VLDB'93 --- International Conference on Very Large Databases},
  year =	 1993,
  pages =	 {415--426},
  annote =	 {from one of Phil's papers}
}

@InProceedings{SAJ96,
  author = 	 {Shaw, A. and Arvind and Johnson, R.P.},
  title = 	 {{Performance Tuning Scientific Codes for Dataflow Execution}},
  booktitle = 	 {PACT'96},
  year =	 1996,
  month =	 {July},
  keywords =     {Large-scale programming, performance tuning, dataflow
                  computation, implicit parallelism},
  annote =	 {case study in large-scale parallel programming and
                  performance tuning; uses a global ocean modelling
                  program (originally written in CM Fortran on a CM-5)
                  and runs it on Monsoon; k-bounded loops prove to be
                  difficult to use but offer large potential of
                  improvement},
  url =  {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-381.ps},
  abstractURL =  {http://www.csg.lcs.mit.edu:8001/cgi-bin/abstract.pl?/home/www/ftp/pub/papers/csgmemo/memo-381.abs},  
  abstract =     "Performance tuning programs for dataflow execution involves
     tradeoffs and optimizations which may be significantly different
     than for execution on conventional machines. We examine some
     tuning techniques for scientific programs with regular control
     but irregular geometry. We use as an example the core of an ocean
     modeling code developed in the implicitly parallel language Id
     for the Monsoon dataflow machine. Dataflow implementations of
     loops, in particular, require careful examination by the compiler
     or programmer to attain high performance because of overheads due
     to fine-grained synchronization, control, and operand
     distribution."
}

%CSG-Memo-364 [abstract]
%          A Comparison of Implicitly Parallel Multithreaded and Data Parallel
%     Implementations of an Ocean Model based on the Navier-Stokes Equations,
%     Andrew Shaw, Arvind, Kyoo-Chan Cho, Christoper Hill, R. Paul Johnson and
%     John Marshall , September 1997, Accepted to Journal of Parallel and
%     Distributed Computing, (35 pages).
%     [postscript 846k] - [compressed 309k] - [gzipped 213k]

@Article{SAC*97,
  author = 	 {Shaw, A. and Arvind and Cho, K.-C. and Hill, C. and
                  Johnson, R.P. and Marshall. J.},
  title = 	 {{A Comparison of Implicitly Parallel Multithreaded and Data Parallel Implementations of an Ocean Model based on the Navier-Stokes Equations}},
  journal = 	 JPDC,
  year = 	 1997,
  month =	 sep,
  annote =	 {Discusses ocean modelling alg in more detail and compares
                  it with other implementations},
  url =          {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-364.ps.Z},
  abstract =     "Two parallel implementations of a state-of-the-art ocean model
     are described and analyzed: one is written in the implicitly
     parallel language Id for the Monsoon multithreaded dataflow
     architecture, and the other in data parallel CM Fortran for the
     CM-5. The multithreaded programming model is inherently more
     expressive than the data parallel model. One goal of this study is
     to understand what, if any, are the performance penalties of
     multithreaded execution when implementing a program that is well
     suited for data parallel execution. To avoid technology and machine
     configuration issues, the two implementations are compared in terms
     of overhead cycles per {\it required} floating point operation. When
     simulating flows in complex geometries typical of ocean basins, the
     data parallel model only remains efficient if redundant computations
     are performed over land. The generality of the Id programming model
     allows one to easily and transparently implement a parallel code
     that computes only in the ocean. When simulating ocean basins with
     complex and irregular geometry the normalised performance on Monsoon
     is comparable with that of the CM-5. For more regular geometries,
     that map well to the computational domain, the data-parallel
     approach proves to be a better match. We conclude by examining the
     extent to which clusters of mainstream symmetric multiprocessor
     (SMP) systems offer a scientific computing environment which can
     capitalize on and combine the strengths of the two paradigms."
}



@Article{SAC*98,
  author = 	 {Shaw, A. and Arvind and Cho, K.-C. and Hill, C. and Johnson, R.P. and Marshall, J.},
  title = 	 {{A Comparison of Implicitly Parallel Multithreaded and Data Parallel Implementations of an Ocean Model}},
  journal = 	 JPDC,
  year = 	 1998,
  volume =	 48,
  number =	 1,
  pages =	 {1--51},
  month =	 jan,
  url =          {ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-364.ps.Z}
}

%CSG-Memo-345-3 [abstract]
%          Performance Studies of the Monsoon Dataflow Processor, J. Hicks,
%     D. Chiou, B.S. Ang and Arvind, May 1993, In Journal of Parallel and
%     Distributed Computing, July, 1993, (54 pages).
%     [postscript 503k] - [compressed 207k] - [gzipped 155k] - HTML document

@Article{HCAA93,
  author = 	 {Hicks, J. and Chiou, D. and Ang, B.S. and Arvind},
  title = 	 {{Performance Studies of the Monsoon Dataflow Processor}},
  journal = 	 JPDC,
  year = 	 1993,
  month =	 {July},
  url =          {http://www.csg.lcs.mit.edu/monsoon/monsoon-performance/monsoon-performance.html},
  abstract =     "In this paper, we examine the performance of Id, an implicitly
     parallel language, on Monsoon, an experimental dataflow machine. One
     of the precepts of our work is that the Id run-time system and
     compiled Id programs should run on any number of Monsoon processors
     without change. Our experiments running Id programs on Monsoon show
     that speedups of more than seven are easily achieved on eight
     processors for most of the applications that we studied. We explain
     the sources of overhead that limit the speedup of each of our
     benchmark programs. We also compare the performance of Id on a
     single Monsoon processor with C/Fortran on a DEC Station 5000 (MIPS
     R3000 processor), to establish a baseline for the efficiency of Id
     execution on Monsoon. We find that the execution of Id programs on
     one Monsoon processor takes up to three times as many cycles as the
     corresponding C or Fortran programs executing on a MIPS R3000
     processor. We identify the sources of inefficiency on Monsoon and
     suggest improvements, where possible. In many cases, however,
     improving single processor performance will reduce parallel
     processor performance."
}

%   o Functional, I-Structure, and M-Structure Implementations of NAS Benchmark
%     FT Sumit Sur, Wim BÅˆhm Proc. of the Int'l. Conference on Parallel
%     Architecture and Compilation Techniques (PACT'94) Montreal, Canada Aug.,
%     1994.
% http://www.cs.colostate.edu/~dataflow/papers/pact94b.ps.gz

@InProceedings{SuBo94,
  author = 	 {Sur, S. and BÅˆhm, W.},
  title = 	 {{Functional, I-Structure, and M-Structure Implementations of NAS Benchmark FT}},
  booktitle = 	 {{PACT'94 --- International Conference on Parallel Architecture and Compilation Techniques}},
  year =	 1994,
  address =	 {Montreal, Canada},
  month =	 aug,
  pages =	 {47--56},
  annote = 	 {Implementation of FFT based PDE solver in Id; studies
                  features of fctal languages for parallel execution; I-and
                  M-structures are used to reduce space consumption},
  keywords =     {Applicative Programming, Language Constructs and
                  Features, Data Storage Representations, Id, I-structures,
                  M-structures},
  url =          {http://www.cs.colostate.edu/\~{}dataflow/papers/pact94b.ps.gz},
  abstract = "We implement the NAS parallel benchmark FT, which numerically
solves a three dimensional partial differential equation using forward and
inverse FFTs, in the dataflow language Id and run it on a one node monsoon
machine. Id is a layered language with a purely functional kernel, a
deterministic layer with I-structures, and a non-deterministic layer with
M-structures. We compare the performance of versions of our code written in
these three layers of Id. We measure instruction counts and critical path
length using the Monsoon Interpreter Mint. We measure the space
requirements of our codes by determining the largest possible problem size
fitting on a one node monsoon machine. The purely functional code provides
the highest average parallelism, but this parallelism turns out to be
superfluous. The I-structure code executes the minimal number of
instructions and as it has a similar critical path length as the functional
code, runs the fastest. The M-structure code allows the largest problem
sizes to be run at the cost of about 20\% increase in instruction count, and
75\% to 100\% increase in critical path length, compared to the I-structure
code."
}

%   o Analysis of Non-Strict Functional Implementations of the
%     Dongarra-Sorensen Eigensolver Sumit Sur, Wim BÅˆhm Proc. Int. Conference on
%     Supercomputing (ICS) Manchester, UK June, 1994

@InProceedings{SuBo94a,
  author = 	 {Sur, S. and BÅˆhm, W.},
  title = 	 {{Analysis of Non-Strict Functional Implementations of the Dongarra-Sorensen Eigensolver}},
  booktitle = 	 {{ICS'94 --- International Conference on Supercomputing}},
  year =	 1994,
  address =	 {Manchester, U.K.},
  month =	 {June},
  annote =	 {shows that modularity via non-strict languages makes the
                  expression of producer-consumer parallelism easy},
  url =          {http://www.cs.colostate.edu/\~{}dataflow/papers/ics94b.ps.gz},
  abstract =     "We study the producer-consumer parallelism of Eigensolvers composed of a
tridiagonalization function, a tridiagonal solver, and a matrix
multiplication, written in the non-strict functional programming language
Id. We verify the claim that non-strict functional languages allow the
natural exploitation of this type of parallelism, in the framework of
realistic numerical codes. We compare the standard top-down
Dongarra-Sorensen solver with a new, bottom-up version. We show that this
bottom-up implementation is much more space efficient than the top-down
version. Also, we compare both versions of the Dongarra-Sorensen solver
with the more traditional QL algorithm, and verify that the
Dongarra-Sorensen solver is much more efficient, even when run in a serial
mode. We show that in a non-strict functional execution model, the
Dongarra-Sorensen algorithm can run completely in parallel with the
Householder function. Moreover, this can be achieved without any change in
the code components. We also indicate how the critical path of the complete
Eigensolver can be improved."
}

@Article{DoSo87,
  author = 	 {Dongarra, J.J. and Sorensen, D.C.},
  title = 	 {{A Fully Parallel Algorithm for the Symmetric Eigenvalue Problem}},
  journal = 	 {SIAM Journal of Scientific and Statistical Computation},
  year = 	 1987,
  volume =	 8,
  pages =	 {139--154},
  annote =	 {cited in \cite{SuBo94a} on difficulty of achieving producer-consumer parallelism in the Eigensolver}
}

%   o Functional Implementations of the Jacobi Eigen-Solver Wim BÅˆhm, Bob
%     Hiromoto Proc. of the High Performance Functional Computing
%     Conf. (HPFC'95) Denver, CO April, 1995 LLNL CONF-9504126


@InProceedings{BoHi95,
  author = 	 {BÅˆhm, W. and Hiromoto, B.},
  title = 	 {{Functional Implementations of the Jacobi Eigen-Solver}},
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  address =	 {Denver, Colorado, April 10--12},
  year =         {1995},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTyear = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTpages = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  url =          {http://www.cs.colostate.edu/\~{}dataflow/papers/sp95.ps.gz},
  abstract =     "In this paper we describe the systematic development of two implementations
of the Jacobi Eigensolver and give their initial performance results for
the MIT/Motorola Monsoon dataflow machine. Our study is carried out using
MINT, the MIT Monsoon simulator. The functional semantics with respect to
array updates, which cause excessive array copying, has lead us to an
implementation of a parallel {"}group rotations{"} algorithm first
described by Sameh. Our version of this algorithm requires O($n^3$)
operations, whereas Sameh's original version requires O($n^4$)
operations. The convergence of the group alogorithm is briefly treated. We
discuss the issues involved in rewriting the algorithm in Sisal, a strict,
purely functional language without explicit I-structures.",
}

%$%cindex Erlang

@InProceedings{Arms96,
  author = 	 {Armstrong, J.},
  title = 	 {{Erlang -- a Survey of the Language and its Industrial Applications}},
  booktitle = 	 {INAP'96 --- Exhibitions and Symposium on Industrial Applications of Prolog},
  pages =	 {16--18},
  year =	 1996,
  address =	 {Hino, Tokyo, Japan},
  month =	 {October}
}

@Book{AWWV96,
  author =	 {Armstrong, J. and Williams, M. and Wikstrom, C. and Virding, R.},
  title = 	 {{Concurrent Programming in Erlang}},
  publisher = 	 P-H,
  year = 	 1996
}

@InProceedings{BoFe94,
  author = 	 {Boucher, D. and Feeley, M.},
  title = 	 {{Construction Parall\`{e}le de l'Automate LR(0): Une Application de MultiLisp \`{a} la Compilation}},
  booktitle = 	 {6i\`{e}me Rencontres Francophones du Parall\'{e}lisme},
  year =	 1994,
  address =	 {Universit\'{e} de Montr\'{e}al},
  month =	 {June},
  url =          {http://www.iro.umontreal.ca/\~{}feeley/papers/renpar6.ps.gz}
}


%   o Comparing Id and Haskell in a Monte Carlo photon transport code Jeff
%     Hammes, Sumit Sur, Wim BÅˆhm Journal of Functional Programming, (to
%     appear)

@Article{HLB95,
  title=  {Comparing {Id} and {Haskell} in a {Monte} {Carlo} Photon
           Transport Code},
  author= {Hammes, J. and Lubeck, O. and BÅˆhm, W.},
  pages=  {283--316},
  journal=jfp,
  month=  jul,
  year=   1995,
  volume= 5,
  number= 3,
  url =   {http://www.cs.colostate.edu/\~{}hammes/documents/final1.ps.Z},
  abstract = "In this paper we present functional Id and Haskell versions of a large
Monte Carlo radiation transport code, and compare the two languages with
respect to their expressiveness. Monte Carlo transport simulation exercises
such abilities as parsing, input/output, recursive data structures and
traditional number crunching, which makes it a good test problem for
languages and compilers. Using some code examples, we compare the
programming styles encouraged by the two languages. In particular, we
discuss the effect of laziness on programming style. We point out that
resource management problems currently prevent running realistically large
problem sizes in the functional versions of the code."
}


@InProceedings{HaBo97,
  author = 	 {Hammes, J. and B\"{o}hm, W.},
  title = 	 {{On the Performance of Functional Programming Languages on Realistic Benchmarks}},
  booktitle = 	 {PDPTA'97 --- International Conference on Parallel and Distributed Processing Techniques},
  year =	 1997,
  url =          {http://www.cs.colostate.edu/\~{}hammes/documents/PDPTA.ps.Z}
}

@incollection{BlNa94,
 title =      {{A Practical Comparison of $N$-Body Algorithms}},
 author =     {Blelloch, G. and Narlikar, G.},
 booktitle =  {{Parallel Algorithms}},
 series =     {Series in Discrete Mathematics and Theoretical Computer
                  Science},
 publisher=   {American Mathematical Society},
 year =       1997,
 annote =       {mainly on sequential complexity of the programs; the
                  parallel versions had problems because of high memory
                  consumption.},
 url =          {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/dimacs-nbody.ps.gz},
  abstractURL =  {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/dimacs-nbody.html},
  abstract =     "This work compares three algorithms for the three dimensional N-body
problem, the Barnes-Hut algorithm, Greengard's Fast Multipole Method(FMM), and
the Parallel Multipole Tree Algorithm (PMTA) to determine which of the
algorithms performs best in practice. Although FMM has a better asymptotic
running time (O(N) instead of O(N log N) for uniform distributions), the
algorithm is more complicated and it is not immediately clear above what
values of N it performs better in practice. We studied the dependence of
accuracy on the variable parameters theta, p and alpha, and then compared the
floating point operation counts of the three algorithms at similar levels of
accuracy, for both charged and uncharged random distributions. At a high level
of accuracy (RMS-error \(\approx\ 10^{-5}\)), the FMM did the least number of operations
for \(\approx\ N > 10^4\), assuming both charged and uncharged distributions of points. At
a lower level of accuracy, (RMS-error \(\approx\ 10^{-3}\)) for uncharged distributions,
the FMM did not outperform Barnes-Hut even for \(N > 10^8\). For charged
distributions of particles, both the FMM and PMTA were comparable at low
accuracy. The algorithms were implemented in the parallel language NESL."
}

%InProceedings{BlNa94,
%  author = 	 {Blelloch, G. and Narlikar, G.},
%  title = 	 {{A Practical Comparison of $N$-Body Algorithms}},
%  booktitle = 	 {{Dimacs implementation challenge workshop}},
%  year =	 1994,
%  month =	 oct,
%}

@article{BCH*94,
        author = {Blelloch, G.E. and Chatterjee, S. and Hardwick, J.C. and Sipelstein, J. and Zagha, M.},
        title = {{Implementation of a Portable Nested Data-Parallel Language}},
        journal = JPDC,
        volume = 21,
        number = 1,
        pages = {4--14},
        month = apr,
        year = 1994,
        documentURL = {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/nesl-ppopp93.ps.gz},
        abstract = "
This  paper gives an  overview of  the implementation  of NESL,  a portable
nested data-parallel language. This language and its implementation are the
first  to  fully   support  nested  data  structures  as   well  as  nested
data-parallel function calls. These  features allow the concise description
of parallel  algorithms on irregular  data. In addition, they  maintain the
advantages  of  data-parallel languages:  a  simple  programming model  and
portability. The  current NESL implementation  is based on  an intermediate
language called VCODE and a library  of vector routines called CVL. It runs
on the Connection Machines CM-2, the Cray Y-MP C90, and serial machines. We
compare initial  benchmark results of  NESL with those  of machine-specific
code on  these machines  for three algorithms:  least-squares line-fitting,
median finding,  and a  sparse- matrix vector  product. These  results show
that NESL's  performance is competitive with that  of machine-specific code
for regular dense data, and is often superior for irregular data.
"
}

@article        {NaBe99,
author          = {Narlikar, G.J. and Blelloch, G.E.},
title           = {{Space-Efficient Scheduling of Nested Parallelism}},
journal         = TOPLAS,
volume          = 21,
number          = 1,
year            = 1999,
month           = jan,
pages           = {138--173},
keywords        = {Space efficiency, dynamic scheduling, nested parallelism,
multithreading, scalable runtime systems, parallel language implementation},
documentURL     = {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/toplas.ps.gz},
abstract        = "
Many of today's high level parallel languages support dynamic, fine-grained
parallelism. These languages  allow the user to expose  all the parallelism
in the program, which is typically  of a much higher degree than the number
of  processors. Hence  an  efficient scheduling  algorithm  is required  to
assign computations to processors  at runtime. Besides having low overheads
and good  load balancing, it is  important for the  scheduling algorithm to
minimize the space usage of the parallel program.
This  paper  presents an  on-line  scheduling  algorithm  that is  provably
space-efficient  and time-efficient  for nested  parallel languages.  For a
computation  with depth  D and  serial space  requirement S1  the algorithm
generates  a schedule that  requires at  most S1  + O(pD)  space (including
scheduler space) on p processors. To  allow the scheduler to scale with the
number of  processors, we  also parallelize the  scheduler and  analyze the
space and  time bounds of the  computation to include  scheduling costs. In
addition  to  showing that  the  scheduling  algorithm  is space  and  time
efficient in  theory, we demonstrate that  it is effective  in practice. We
have  implemented a  runtime system  that  uses our  algorithm to  schedule
lightweight parallel threads. The results of executing parallel programs on
this system show that our scheduling algorithm significantly reduces memory
usage compared to previous techniques, without compromising performance."
}

@TechReport{GMZ94,
  author = 	 {Gremban, K.D. and Miller, G.L. and Zagha, M.},
  title = 	 {{Performance Evaluation of a New Parallel Preconditioner}},
  institution =  {School of Computer Science},
  year = 	 1994,
  number =	 {CMU-CS-94-205},
  address =	 {Carnegie Mellon University},
  month =	 oct,
  keywords =     {linear systems, iterative methods, preconditioners,
                  sparse and very large systems, parallel algorithms,
                  parallel processors, vector processors, NESL},
  url =          {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-94-205.ps.gz},
  abstract =     "Solution of partial differential equations by either the finite element or
the finite difference methods often requires the solution of large, sparse
linear systems. When the coefficient matrices associated with these linear
systems are symmetric and positive definite, the systems are often solved
iteratively using the preconditioned conjugate gradient method. We have
developed a new class of preconditioners, which we call support tree
preconditioners, that are based on the connectivity of the graphs
corresponding to the coefficient matrices of the linear systems. These new
preconditioners have the advantage of being well-structured for parallel
implementation, both in construction and in evaluation. In this paper, we
evaluate the performance of support tree preconditioners by comparing them
against two common types of preconditioners: those arising from diagonal
scaling, and from the incomplete Cholesky decomposition. We solved linear
systems corresponding to both regular and irregular meshes on the Cray C-90
using all three preconditioners and monitored the number of iterations
required to converge, and the total time taken by the iterative
processes. We show empirically that the convergence properties of support
tree preconditioners are similar, and superior in many cases, to those of
incomplete Cholesky preconditioners, which in turn are superior to those of
diagonal scaling. Support tree preconditioners require less overall
storage, less work per iteration, and yield better parallel performance
than incomplete Cholesky preconditioners. In terms of total execution time,
support tree preconditioners outperform both diagonal scaling and
incomplete Cholesky preconditioners. Hence, support tree preconditioners
provide a powerful, practical tool for the solution of large sparse systems
of equations on vector and parallel machines."
}


@InProceedings{Grei94,
  author = 	 {Greiner, J.},
  title = 	 {{A Comparison of Data-Parallel Algorithms for Connected Components}},
  booktitle = 	 {{SPAA'94 --- Symposium on Parallel Algorithms and Architectures}},
  year =	 1994,
  address =      {Cape May, NJ, U.S.A.},
  pages =        {16--25},
  month =	 jun,
  note =	 {Also: Technical Report CMU-CS-93-191},
  url =          {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/concomp-spaa94.ps.gz},
  abstract =     "This paper presents a comparison of the pragmatic aspects of some
parallel algorithms for finding connected components, together with
optimizations on these algorithms. The algorithms being compared are two
similar algorithms by Shiloach-Vishkin and Awerbuch-Shiloach, a randomized
contraction algorithm based on algorithms by Reif and Phillips, and a hybrid
algorithm. Improvements are given for the first two to improve performance
significantly, although without improving their asymptotic complexity. The
hybrid combines features of the others and is generally the fastest of those
tested. Timings were made using NESL code as executed on a Connection
Machine~2 and Cray Y-MP/C90."
}

@InProceedings{YeAg93,
  author = 	 {Yeung, D. and Agarwal, A.},
  title = 	 {{Experience with Fine-Grain Synchronization in MIMD Machines for Preconditioned Conjugate Gradient}},
  booktitle = 	 {{PPoPP'93 --- Symposium on Principles and Practice of Parallel Programming}},
  year =	 1993,
  month =	 may,
  url =          {ftp://cag.lcs.mit.edu/pub/papers/fine-grain.ps.Z},
  abstract =     "This paper discusses our experience with fine-grain synchronization for a
variant of the preconditioned conjugate gradient method. This algorithm
represents a large class of algorithms that have been widely used but
traditionally difficult to implement efficiently on vector and parallel
machines. Through a series of experiments conducted using a simulator of a
distributed shared-memory multiprocessor, this paper addresses two major
questions related to fine-grain synchronization in the context of this
application. First, what is the overall impact of fine-grain
synchronization on performance? Second, what are the individual
contributions of the following three mechanisms typically provided to
support fine-grain synchronization: language-level support, full-empty bits
for compact storage and communication of synchronization state, and
efficient processor operations on the state bits? Our experiments indicate
that fine-grain synchronization improves overall performance by a factor of
3.7 on 16 processors using the largest problem size we could simulate; we
project that a significant performance advantage will be sustained for
larger problem sizes. We also show that the bulk of the performance
advantage for this application can be attributed to exposing increased
parallelism through language-level expression of fine-grain
synchronization. A smaller fraction relies on a compact implementation of
synchronization state, while an even smaller fraction results from
efficient full-empty bit operations."
}

@InProceedings{ABC*95,
  author = 	 {Agarwal, A. and Bianchini, R. and Chaiken, D. and
                  Johnson, K.L. and Kranz, D. and Kubiatowicz, J. and Lim,
                  B-H. and  Mackenzie, K. and Yeung, D.},
  title = 	 {{The MIT Alewife Machine: Architecture and Performance}},
  booktitle = 	 {{ISCA'95 --- International Symposium on Computer Architecture}},
  year =	 1995,
  url =          {ftp://cag.lcs.mit.edu/pub/papers/isca95.ps.Z},
  abstract =     "Alewife is a multiprocessor architecture that supports up to 512 processing
nodes connected over a scalable and cost-effective mesh network at a
constant cost per node. The MIT Alewife machine, a prototype implementation
of the architecture, demonstrates that a parallel system can be both
scalable and programmable. Four mechanisms combine to achieve these goals:
software-extended coherent shared memory provides a global, linear address
space; integrated message passing allows compiler and operating system
designers to provide efficient communication and synchronization; support
for fine-grain computation allows many processors to cooperate on small
problem sizes; and latency tolerance mechanisms --- including block
multithreading and prefetching --- mask unavoidable delays due to
communication. Microbenchmarks, together with over a dozen complete
applications running on the 32-node prototype, help to analyze the behavior
of the system. Analysis shows that integrating message passing with shared
memory enables a cost-efficient solution to the cache coherence problem and
provides a rich set of programming primitives. Block multithreading and
prefetching improve performance by up to 25\% individually, and 35\%
together. Finally, language constructs that allow programmers to express
fine-grain synchronization can improve performance by over a factor of two."
}

@Article{GSWZ95,
  author = 	 {Grant, P.W. and Sharp, J.A. and Webster, M.F. and Zhang, X.},
  title = 	 {{Experiences of Parallelizing Finite-Element Problems in a Functional Style}},
  journal = 	 SPE,
  month =        sep,
  year = 	 1995,
  volume =	 25,
  number =	 9,
  pages =	 {947--974},
  abstract =     "Experiences are described of parallelizing a functional finite-
    element program (written in Haskell) for the solution of 
    computational fluid-dynamics problems. A transformation prototyping 
    approach using a system developed at the University of York, which 
    simulates idealised parallel machines on a conventional sequential 
    workstation, is presented. In this paper we demonstrate: (a) the 
    relative simplicity of the functional approach for parallelizing a 
    complex program compared with the conventional procedural approach; 
    (b) the suitability of functional languages for prototyping parallel 
    algorithms to improve an implementation; and (c) the considerable 
    assistance provided by the simulator."
}

@Article{MiSc95,
  title =   {{Prototyping a Parallel Vision System in {Standard} {ML}}},
  author =  {Michaelson, G. and Scaife, N.},
  pages =   {345--382},
  journal = JFP,
  month =   jul,
  year =    1995,
  volume =  5,
  number =  3,
  url =     {ftp://ftp.cee.hw.ac.uk/pub/funcprog/ms.jfp95.ps.Z},
  abstract = "The construction of a parallel vision system from Standard ML prototypes is
presented. The system recognises 3D objects from 2D scenes through edge
detection, grouping of edges into straight lines and line junction based
model matching. Functional prototyping for parallelism is illustrated
through the development of the straight line detection component. The
assemblage of the whole system from prototyped components is then
considered and its performance is discussed."
}

@InProceedings{MiTr93,
  author = 	 {Mitrovic, S. and Trobina, M.},
  title = 	 {{Computer Vision Algorithms in Sisal}},
  booktitle = 	 {SISAL'93},
  pages =	 {114--119},
  year =	 1993,
  address =	 {San Diego, CA, USA},
  month =	 oct
}

%$%cindex Lolita

@InProceedings{MSS94,
  author = 	 {Morgan, R.G. and Smith, M.H. and Short, S.},
  title = 	 {{Translation by Meaning and Style in Lolita}},
  booktitle = 	 {International BCS Conference --- Machine Translation Ten Years On},
  year =	 1994,
  address =	 {Cranfield University},
  month =	 {November}
}

@InProceedings{MoJa95,
  author = 	 {Morgan, R.G. and Jarvis, S.A.},
  title = 	 {{Profiling Large-Scale Lazy Functional Programs}},
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  pages =        {182--192},
  address =	 {Denver, Colorado, April 10--12},
  year =         1995,
  pages =	 {222--234},
  annote =	 {List of Publications: http://www.dur.ac.uk/\~{}dcs0www3/lnle/publications.html},
  url =          {ftp://sisal.llnl.gov/pub/hpfc/papers95/paper05.ps},
  abstract =      "The LOLITA natural language processing system is an example of one of the
ever increasing number of large scale systems written entirely in a
functional programming language. The system consists of over 35,000 lines
of Haskell code (excluding comments) and is able to perform a number of
tasks such as semantic and pragmatic analysis of text, context scanning and
query analysis. Such a system is only useful if the results are calculated
in real-time, therefore the efficiency of such a system is paramount. For
the past year and a half we have used profiling tools supplied with GHC and
HBC to analyse and reason about our programming solutions and achieved good
results; however, our experience has shown that the profiling life-cycle is
often too long to make detailed analysis possible in a constantly changing
system, and the results are often misleading. A system is currently being
developed at Durham to produce a profiling PostProcessor which is based
upon Sansom's Cost Centre profiler. Post-processing allows results which
have previously been collected from a program to be analysed by the
programmer after execution time, no further compilation is needed, thus
greatly reducing the analysis time. This paper demonstrates how such a
system has been developed. It also considers an extension to the current
cost centre approach using Cost Stacks, leading to a Post-Processor which
produces results based on a more accurate inheritance approach. The
modifications to the Haskell compiler based upon detailed cost semantics
and an implementation scheme are also discussed. The result is a
programming tool which produces more accurate profiling results to large
lazy functional programs in real-time."
}

@InProceedings{JPM95,
  author = 	 {Jarvis, S.A. and Poria, S. and Morgan, R.G.},
  title = 	 {{Understanding LOLITA: Experiences in Teaching Large
                  Scale Functional Programming}},
  booktitle = 	 {{FPLE'95 --- Functional Programming Languages in Education}},
  editor =	 {Hartel, P.H. and Plasmeijer, M.J.},
  volume =	 1022,
  series =	 LNCS,
  year =	 1995,
  publisher =	 S-V,
  pages =	 {103--120},
  annote =	 {List of Publications: http://www.dur.ac.uk/\~{}dcs0www3/lnle/publications.html}
}

@PhdThesis{Clay93,
  author = 	 {Clayman, S.},
  title = 	 {{Developing and Measuring Parallel Rule-Based Systems in a
Functional Programming Environment}},
  school = 	 {Department of Computer Science},
  year = 	 1993,
  address =	 {University College London, UK}
}

%$%node Symbolic Computation,  , Parallel Functional Programming, Applications
%$%subsection Symbolic Computation


@InProceedings{Buch87,
  author = 	 {Buchberger, B.},
  title = 	 {{Applications of GrÅˆbner Bases in Non-Linear
                  Computational Geometry}}, 
  booktitle = 	 {{Workshop on Scientific Software}},
  year =	 1987,
  publisher =	 S-V,
  pages =	 {59--88}
}


@InProceedings{GVH92,
  author = 	 {Ganzha, V.G. and Vorozhtsov, E.V.  and Hulzen, J.A.},
  title = 	 {{A New Symbolic-Numeric Approach to Stability Analysis of
                  Difference System}},
  booktitle = 	 {{ISSAC'92 --- International Symposium on Symbolic and
                  Algebraic Computation}},
  year =	 1992,
  address =	 {Berkeley, CA, USA, July 27--29}
}


@TechReport{Hoff92,
  author = 	 {Hoffmann, C.M.},
  title = 	 {{Implicit Curves and Surfaces in Computer Aided Geometric
                  Design}}, 
  institution =  {Purdue University},
  year = 	 1992,
  type =	 {Technical Report},
  number =	 {CER-92-002}
}


@Article{L*85,
  author = 	 {Leslie, L.M. et al.},
  title = 	 {{A High Resolution Primitive Equations NWP Model for Operations and Research}}, 
  journal = 	 {Australian Metereological Magazine},
  year = 	 1985,
  volume =	 33,
  pages =	 {11--35},
  annote =	 {large parallel functional application}
}


@InProceedings{Egan93,
  author = 	 {Egan, G.},
  title = 	 {{Implementing the Kernel of the Australian Weather Prediction Model in Sisal}},
  booktitle = 	 {SISAL'93},
  pages =	 {11--17},
  year =	 1993,
  address =	 {San Diego, CA, USA},
  month =	 {October}
}


@Article{FTL94,
  author = 	 {Feeley, M. and Turcotte, M. and LaPalme, G.},
  title = 	 {{Using MultiLisp for Solving Constraint Satisfaction Problems:
an Application to Nucleic Acid 3D Structure Determination}},
  journal = 	 LSC,
  year = 	 1994,
  volume =	 7,
  pages =	 {231--247},
  url =          {http://www.iro.umontreal.ca/\~{}feeley/papers/nucleic.ps.gz},
}


@InProceedings{PRR96,
  author = 	 {Podehl, A. and Rauber, T. and R\"unger, G.},
  title = 	 {{Scalability and Granularity Issues of the Hierarchical Radiosity Method}},
  booktitle = 	 {{EuroPar'96 --- European Conference on Parallel Processing}},
  address =      {Lyon, France, August 26--29},
  volume =	 1123,
  series =	 LNCS,
  year =	 1996,
  publisher =	 S-V,
  pages = 	 {789--798},
}

% ---------------------------------------------------------------------------
%$%node Symbolic Computation, Misc, Applications, Top
%$%section Symbolic Computation
% ---------------------------------------------------------------------------
                                                                            
@Book{CGG*91a,
  author = 	 {Char, B.W. and Geddes, K.O. and Gonnet, G.H.
		  and Leong, B.L. and Monagan, M.B. and Watt,
		  S.M.},
  title = 	 {{Maple V --- Library Reference Manual}},
  publisher = 	 S-V,
  OPTaddress =      {New York},
  year = 	 {1991}
}

@Book{Wolf88,
  author = 	 {Wolfram, S.},
  title = 	 {{Mathematica --- A System for Doing Mathematics by
		  Computer}}, 
  publisher = 	 {Addison-Wesley},
  year = 	 {1988},
  owner =        {risc},
}

@InBook{Laue82,
  title = 	 {{Computing by Homomorphic Images}},
  booktitle =    {{Computer Algebra --- Symbolic and Algebraic Computation}},
  publisher =	 {Springer},
  year =	 {1982},
  OPTeditor =	 {Buchberger, B. and Collins, G. E. and Loos, R. and
                  Albrecht, R.},
  author =       {Lauer, M.},
  pages =	 {139--168},
  OPTedition =	 {Second},
  descr =        {paca},
  owner =        {risc},
}

@InProceedings{Lips71,
  author = 	 {Lipson, J. D.},
  title = 	 {{Chinese Remainder and Interpolation Algorithms}},
  pages =	 {372--391},
  booktitle =	 {SYMSAM'71 --- Symposium on Symbolic and Algebraic Manipulation},
  publisher =    {Academic Press},
  year =	 {1971},
  descr =        {paca},
}

@Book{Lips81,
  author = 	 {Lipson, J.D.},
  title = 	 {{Elements of Algebra and Algebraic Computation}},
  publisher = 	 {Addison-Wesley},
  year = 	 {1981},
  descr =        {paca},
  owner =        {risc},
}

@techreport{Wink88,
  author =       {Winkler, F.},
  title =        {{Computer Algebra I (in German)}},
  number =       {88-88},
  year =         {1988},
  type =         {Lecture Notes},
  institution =  {RISC-Linz},
  address =      {Johannes Kepler University, Linz, Austria},
}

@Book{Wink96,
  author =	 {Winkler, Franz},
  title = 	 {{Polynomial Algorithms in Computer Algebra}},
  publisher = 	 {Springer-Verlag},
  year = 	 1996,
  note =	 {ISBN 3-211-82759-5},
  annote =	 {chapter 3 talks about computing in hom ims but no details on the bound for the modules}
}

@Article{Pugh92,
  author = 	 {Pugh, W.},
  title = 	 {{The Omega test: A Fast and Practical Integer Programming
                  Algorithm for Dependence Analysis}},
  journal =	 CACM,
  year =	 {1992},
  volume =	 {8},
  pages =	 {102--114},
  annote =	 {Citation from Hughes' sized types paper.},
  url =  {ftp://ftp.cs.umd.edu/pub/omega/techReports/non-TRs/omega/omega.ps.Z},
  abstract =     "The  Omega test is  an  integer  programming algorithm  that  can determine
whether a dependence exists between two  array references, and if so, under
what   conditions.  Conventional  wisdom  holds  that  integer  programming
techniques are far too expensive to be used for dependence analysis, except
as a method of last resort for situations that cannot be decided by simpler
methods. We  present evidence that suggests this  wisdom is wrong, and that
the Omega test is competitive with approximate  algorithms used in practice
and suitable for use in production compilers. Experiments suggest that, for
almost all programs, the   average  time required  by  the Omega   test  to
determine  the  direction vectors  for  an  array  pair  is  less than  500
micro-seconds on a 12 MIPS workstation.

The Omega  test   is based on    an extension of  Fourier-Motzkin  variable
elimination  (a linear programming method) to  integer programming, and has
worst-case exponential  time complexity.  However,  we show that  for  many
situations in which other (polynomial) methods are accurate, the Omega test
has low order polynomial time complexity.

The Omega test can  be used to  project integer programming problems onto a
subset  of the variables,  rather than  just deciding  them. This has  many
applications,  including  accurately  and efficiently computing  dependence
direction and distance vectors."
}

@TechReport{MaPu94,
  author = 	 {Maslov, V. and Pugh, W.},
  title = 	 {{Simplifying Polynomial Constraints over Integers to Make
                   Dependence Analysis More Precise}}, 
  institution =  {Department of Computer Science},
  year = 	 1994,
  number =	 {CS-TR-3109.1},
  address =	 {University of Maryland},
  month =	 {February},
  annote =	 {check how large this class is; useful for subtype
                  inference based constraint checking?}
}

@Book{Knut68,
  author = 	 {Knuth, D.E.},
  title = 	 {{The Art of Computer Programming}},
  publisher = 	 A-W,
  year = 	 {1968}
}

@Book{Knut81,
  author = 	 {Knuth, D.E.},
  title = 	 {{The Art of Computer Programming}},
  volume =       {{Vol.\ II: Seminumerical Algorithms}},
  publisher = 	 A-W,
  edition =      {2nd},
  year = 	 {1981}
}

@Book{GKP89,
  author = 	 {Graham, R.L. and Knuth, D.E. and Patashnik, O.},
  title = 	 {{Concrete Mathematics}},
  publisher = 	 {Addison-Wesley},
  year = 	 {1989},
  owner =        {risc},
}

@Book{PrSh85,
  author =	 {Preparata, F.P. and Shamos, F.I.},
  title = 	 {{Computational Geometry: An Introduction}},
  publisher = 	 S-V,
  year = 	 1985,
  note =	 {ISBN 0387961313},
  annote =	 {Std textbook on computation geometry}
}

@Book{BKS00,
  author =	 {{de Berg}, M. and {van Kreveld}, M. and Overmars, M.},
  title = 	 {{Computational Geometry: Algorithms and Applications}},
  publisher = 	 S-V,
  year = 	 2000,
  edition =	 {second},
  note =	 {ISBN 3-540-65620-0},
  annote =	 {Chapter on Voronoi diagrams is available on-line: http://www.cs.ruu.nl/geobook/voronoi.ps}
}

% ---------------------------------------------------------------------------
%$%node Misc, Our Own Stuff, Symbolic Computation, Top
%$%section Misc
% ---------------------------------------------------------------------------

@Article{ToDi94,
  author = 	 {Toyn, I. and Dix, A.J.},
  title = 	 {{Efficient Binary Transfer of Pointer Structures}},
  journal = 	 SPE,
  year = 	 1994,
  volume =	 24,
  number =	 11,
  month =	 {November},
  pages =	 {1001--1023},
  annote =	 {About graph packing and transfer}
}

@Article{Tarj75,
  author = 	 {Tarjan, R.E.},
  title = 	 {{Efficiency of a Good, but not Linear Set Union Algorithm}},
  journal = 	 JACM,
  year = 	 1975,
  volume =	 22,
  pages =	 {215--225}
}

@InProceedings{PoSm93,
  author = 	 {Poulovassilis, A.P. and Small, C.},
  title = 	 {{A Domain-Theoretic Approach to Logic and Functional
                  Databases}}, 
  booktitle = 	 {{VLDB'93 --- International Conference on Very Large Databases}},
  year =	 1993,
  pages =	 {415--426}
}

% move that to a better place

@TechReport{Warr83,
  author = 	 {Warren, D.H.D.},
  title = 	 {{An Abstract Prolog Instruction Set}},
  institution =  {SRI International},
  year = 	 1983,
  type =	 {Technical report},
  number =	 309,
  annote =	 {std reference for WAM}
}


@Article{Burt84,
  author = 	 {Burton, F.W.},
  title = 	 {{Annotations to Control Parallelism and Reduction Order
                  in the Distributed Evaluation of Functional Programs}},
  journal = 	 TOPLAS,
  year = 	 1984,
  volume =	 6,
  number =	 2,
  month =	 {April},
  pages =	 {159--174},
  annote =	 {std reference for annotations for parallelism}
}


@Article{Robi65,
  author = 	 {Robinson, J.},
  title = 	 {{A Machine Oriented Logic Based on the Resolution Principle}},
  journal = 	 CACM,
  year = 	 1965,
  volume =	 12,
  number =	 1,
  pages =	 {23--41},
  annote =	 {std reference for unification}
}


@InProceedings{Schr95,
  author = 	 {Schreiner, W.},
  title = 	 {{Application of a Para-Functional Language to Problems in
Computer Algebra}},
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  pages =        {182--192},
  address =	 {Denver, Colorado, April 10--12},
  year =         1995,
  url =          {ftp://sisal.llnl.gov/pub/hpfc/papers95/paper1.ps},
  pages =	 {10--24}
}

@TechReport{Schr93,
  author = 	 {Schreiner, W.},
  title = 	 {{Parallel Functional Programming (An Annotated Bibliography)}},
  institution =  {RISC-Linz},
  year = 	 1993,
  type =	 {Technical Report},
  number =	 {93-24},
  address =	 {Johannes Kepler University, Linz},
  url =          {ftp://ftp.risc.uni-linz.ac.at/pub/techreports/1993/93-24.ps.gz},
  abstract =     "This bibliography cites and comments more than 350 publications  
on the parallel functional programming research of the last 15 years. It   
focuses oin the software aspect of this area, i.e. on languages,           
compile-time analysis techniques (in particular for strictness and weight  
analysis), code generation, and runtime systems. Excluded from this        
bibliography are publications on special architectures and on garbage      
collection unless they contain aspects interesting for above areas. Most   
bibliographic items are listed inclusive their full abstracts.             "
}

@article{Turn79,
author = {Turner, D.A.},
title = {{A New Implementation Technique for Applicative Languages}},
journal = SPE,
volume = {9},
year = {1979},
pages = {31--49},
}

@phdthesis{Hugh84,
author = {Hughes, R.J.M.},
title = {{The Design and Implementation of Programming Languages}},
school = {Programming Research Group},
address = {University of Oxford},
year = {1984},
}

@article{Land64,
author = {Landin, P. J.},
title = {{The Mechanical Evaluation of Expressions}},
journal = compj,
volume = {6},
pages = {308--320},
year = {1964},
}

@book{Burg75,
author = {Burge, W. H.},
title = {{Recursive Programming Techniques}},
publisher = {Addison-Wesley},
address = {Reading, Mass.},
year = {1975},
}

% ---------------------------------------------------------------------------
%$%node Our Own Stuff, References To Check, Misc, Top
%$%section Our Own Stuff
% ---------------------------------------------------------------------------

%$%menu
%* My own publications at Glasgow::  
%* My own publications at RISC::  
%* Glaswegian publications::	
%$%end menu

%$%node  My own publications at Heriot-Watt
%$%subsection My own publications at Heriot-Watt

%$%cindex Our book chapter

@InBook{TLH99,
  author =	 {Trinder, P.W. and Loidl, H-W. and Hammond, K.},
  editor =	 {Hammond, K. and Michaelson, G.},
  title = 	 {{Research Directions in Parallel Functional Programming}},
  chapter = 	 {{Large-Scale Functional Applications}},
  publisher = 	 S-V,
  year = 	 1999,
  month =	 oct,
  pages =	 {399--426}
}

%$%cindex Our TSE paper

@Article{TBD*99,
  author = 	 {Trinder, P.W. and {Barry Jr.}, E. and Davis, M.K. and Hammond, K. and Junaidu, S.B. and Klusik, U. and Loidl, H-W. and {Peyton Jones}, S.L.},
  title = 	 {{GPH: An Architecture-Independent Functional Language}},
  journal = 	 {IEEE Transactions on Software Engineering},
  year = 	 1999,
  note =	 {Submitted for publication},
  annote =       {Unpublished},
}

@InProceedings{TLB*00,
  author = 	 {Trinder, P.W. and Loidl, H-W. and {Barry Jr.}, E. and Hammond, K. and Klusik, U. and {Peyton Jones}, S.L. and {RebÅÛn Portillo}, Å¡.J.},
  title = 	 {{The Multi-Architecture Performance of the Parallel Functional Language GPH}},
  booktitle =    {{Euro-Par 2000 --- Parallel Processing}},
  OPTeditor =    {Bode, A. and Ludwig, T. and WismÅ¸ller, R.},
  OPTvolume =    {},
  series =       LNCS,
  year =         {2000},
  publisher =    S-V,
  month =        {August},
  OPTpages =        {},
  note =	 {To appear},
  OPTcontentsURL =       {http://link.springer.de/link/service/series/0558/tocs/t1900.htm},
  abstractURL =  {http://www.cee.hw.ac.uk/~dsg/gph/papers/abstracts/europar00.html},
  url =          {http://www.cee.hw.ac.uk/~dsg/gph/papers/europar00.ps.gz},
  abstract =     "In principle, functional languages promise straightforward
architecture-independent parallelism, because of their high level
description of parallelism, dynamic management of parallelism and
deterministic semantics.  However, these language features come at the
expense of a sophisticated compiler and/or runtime-system. The problem
we address is whether such an elaborate  system can deliver
acceptable performance on a variety of parallel architectures.  In
particular we report performance measurements for the GUM
runtime-system on eight parallel architectures, including massively
parallel, distributed-memory, shared-memory and workstation networks.",
}

%$%cindex Our CPE paper

@Article{LTH*98,
  author = 	 {Loidl, H-W. and Trinder, P.W. and Hammond, K. and Junaidu, S.B. and Morgan, R.G. and {Peyton Jones}, S.L.},
  title = 	 {{Engineering Parallel Symbolic Programs in GPH}},
  journal = 	 CPE,
  year = 	 1999,
  volume =       {11},
  issue =        {12},
  pages =        {701--752},
  OPTnote =	 {To appear},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/cpe-gph.ps.gz},
  annote =       {ToDo: put the paper on-line},
  abstract =     "We investigate the claim that functional languages offer low-cost
parallelism in the context of symbolic programs on modest parallel
architectures. In our investigation we present the first comparative study
of the construction of large applications in a parallel functional
language, in our case in Glasgow Parallel Haskell (GPH). The applications
cover a range of application areas, use several parallel programming
paradigms, and are measured on two very different parallel architectures.

On the applications level the most significant result is that we are able
to achieve modest wall-clock speedups (between factors of 2 and 10) over
the optimised sequential versions for all but one of the programs. Speedups
are obtained even for programs that were not written with the intention of
being parallelised. These gains are achieved with a relatively small
programmer-effort. One reason for the relative ease of parallelisation is
the use of evaluation strategies, a new parallel programming technique that
separates the algorithm from the coordination of parallel behaviour.

On the language level we show that the combination of lazy and parallel
evaluation is useful for achieving a high level of abstraction. In
particular we can describe top-level parallelism, and also preserve module
abstraction by describing parallelism over the data structures provided at
the module interface (``data-oriented parallelism'').  Furthermore, we find
that the determinism of the language is helpful, as is the largely-implicit
nature of parallelism in GPH."
}

%$%cindex Our HasPar paper

@Article{HKL*00,
  author = 	 {Hammond, K. and King, D.J. and Loidl, H-W. and {Reb\'{o}n Portillo}, \'{A}.J. and Trinder, P.W.},
  title = 	 {{The HasPar Performance Evaluation Suite for {\sc GpH}: a Parallel Non-Strict Functional Language}},
  journal = 	 SPE,
  month =        feb,
  year = 	 2000,
  note =	 {Submitted for publication},
  abstract =     "The ultimate purpose of parallel computation is to improve performance by
exploiting hardware duplication.  In order to achieve this improvement,
it is essential to have a good understanding of real parallel behaviour.
This paper introduces the HasPar integrated suite of
performance evaluation tools for Glasgow Parallel Haskell (GpH), a
high-performance parallel non-strict functional language.  This suite provides a
framework for assessing and improving parallel program performance that
has been used successfully on a number of large functional programs.

The HasPar suite includes both idealised and realistic simulators
for GpH.  It also incorporates an instrumented parallel
implementation that can be used on a range of architectures including
both tightly-coupled multiprocessors and loosely-coupled networks of
workstations. An important feature of the tools is that they allow
costs to be attributed to the parallel program source
using either static or dynamic cost attribution mechanisms, as
appropriate.  The resulting performance profiles can be visualised in
a number of different ways, as illustrated in this paper.",
}

@InProceedings{Loid99a,
  author = 	 {Loidl, H-W.},
  title = 	 {{Making GUM more Flexible}},
  booktitle = 	 {SFP99 --- Scottish Functional Programming Workshop, University of Stirling, Aug 29 -- Sep 1},
  year =	 1999,
  annote =	 {Draft Proceedings},
}

@inproceedings{mhaskell,
author       = "Andre R. Du Bois and Phil Trinder and Hans-Wolfgang Loidl",
title        = "Towards a {M}obile {H}askell",
year         = "2003",
address      = "Valencia (Spain)",
booktitle    = "Proc.\ of the 12th International Workshop on Functional and
                (Constraint) Logic Programming (WFLP 2003)",
pages = "113-116"
}

@InProceedings{TM,
  author = 	 {{Du Bois}, A.R. and Loidl, H-W. and Trinder, P.},
  title = 	 {{Thread Migration in a Parallel Graph Reducer}},
  booktitle =	 {{IFL'02 ---  Intl.\ Workshop on the Implementation of Functional Languages}},
  year =	 2002,
  series =	 {LNCS~2670},
  volume =       {2670},
  pages =        {199--214},
  address =	 {Madrid, Spain},
  month =	 sep,
  publisher =	 S-V,
  url =	         {http://www.macs.hw.ac.uk/\~{}dsg/gph/papers/ps/Migration-IFL02.ps.gz}
}

@InProceedings{RHLV02,
  author = 	 {{Rebon Portillo}, A.J. and  Hammond, K. and Loidl, H-W. and Vasconcelos, P.},
  title = 	 {{Cost Analysis using Automatic Size and Time Inference}},
  booktitle =	 {{IFL'02 --- Intl.\ Workshop on the Implementation of Functional Languages}},
  pages =	 {232--247},
  year =	 2002,
  volume =	 2670,
  series =	 {LNCS},
  address =	 {Madrid, Spain, September 16-18},
  publisher =	 {Springer-Verlag},
  url =          {http://www.macs.hw.ac.uk/\~{}dsg/gph/papers/ps/Analysis-IFL02.ps.gz},
}


%$%cindex Our JFP survey paper

@Article{TLP01,
  author = 	 {Trinder, P.W. and Loidl, H-W. and Pointon, R.F.},
  title = 	 {{Parallel and Distributed Haskells}},
  journal = 	 JFP,
  year = 	 2002,
  month =      jul,
  volume =     12,
  number =     {4\&5},
  pages =      {469--510},
  abstractURL = {http://www.dcs.glasgow.ac.uk/jfp/bibliography/References/trinderlp2002:469.html},
}

%$%cindex Our HLPP paper

@Article{LTB01,
  author = 	 {Loidl, H-W. and Trinder, P.W. and Butz, C.},
  title = 	 {{Tuning Task Granularity and Data Locality of Data Parallel GpH Programs}},
  journal = 	 {Parallel Processing Letters},
  year = 	 2001,
  month =      dec,
  volume =     11,
  number =     4,
  note =	 {{Selected papers from HLPP'01 --- International Workshop on
High-level Parallel Programming and Applications, Orleans, France, 26-27 March, 2001}}
}

%$%cindex Gentle Intro

@Misc{LoTr??,
  author =	 {Loidl, H-W. and Trinder, P.W.},
  title =	 {{A Gentle Introduction to GPH}},
  note =	 {In preparation},
  annote =	 {Should become std intro to GPH, once it is written}
}

@Misc{Loid01z,
  author =	 {Loidl, H-W.},
  title =	 {{Stg Survival Sheet}},
  month =	 {April},
  year =	 2001,
  note =	 {A Guide to RTS Hackers}
}

%$%node  My own publications at Glasgow, My own publications at RISC, Our Own Stuff, Our Own Stuff
%$%subsection My own publications at Glasgow
% i.e. papers where I am at least co-author

@InProceedings{LoHa95,
  author =       {Loidl, H-W. and Hammond, K.},
  title =        {{On the Granularity of Divide-and-Conquer Parallelism}},
  year =         {1995},
  series =	 {Workshops in Computing},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  publisher =	 S-V,
  address =	 {Ullapool, Scotland, July 8--10},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/div-conc.ps.gz},
  ALTurl =          {http://www.dcs.gla.ac.uk/\~{}hwloidl/publications/GlaFp95.ps.gz},
  abstract =     "This paper studies the runtime behaviour of various parallel
divide-and-conquer algorithms written in a non-strict          
functional language, when three common granularity control      
mechanisms are used: a simple cut-off, a priority thread       
creation and a priority scheduling mechanism. These mechanisms  
use granularity information that is currently provided via      
annotations to improve the performance of the parallel          
programs.                                                       
                                                                
The programs we examine are several variants of a generic       
divide-and-conquer program, an unbalanced divide-and-conquer   
algorithm and a parallel determinant computation. Our results   
indicate that for balanced computation trees a simple,          
low-overhead mechanism performs well whereas the more complex   
mechanisms offer further improvements for unbalanced            
computation trees."
}

@InProceedings{HLP94,
  author =       {Hammond, K. and Loidl, H-W. and Partridge, A.},
  title =        {{Improving Granularity in Parallel Functional Programs: A
                   Graphical Winnowing System for Haskell}},
  year =         {1994},
  series =	 {Workshops in Computing},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  pages =        {111--126},
  publisher =	 S-V,
  address =	 {Ayr, Scotland, September 12--14}
}

@InProceedings{LoHa96,
  author =       {Loidl, H-W. and Hammond, K.},
  title =        {{A Sized Time System for a Parallel Functional Language}},
  year =         {1996},
  booktitle =	 {Glasgow Workshop on Functional Programming},
  address =	 {Ullapool, Scotland, July 8--10},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/sized.ps.gz},
  ALTurl =          {http://www.dcs.glasgow.ac.uk/fp/workshops/fpw96/Loidl.ps.gz},
  abstractURL =  {http://www.dcs.glasgow.ac.uk/fp/workshops/fpw96/Proceedings96.html},
  abstract =     "This paper describes an inference system, whose purpose is to determine the
cost of evaluating expressions in a strict purely functional
language. Upper bounds can be derived for both computation cost and the
size of data structures. We outline a static analysis based on this
inference system for inferring size and cost information. The analysis is a
synthesis of the sized types of Hughes et al., and the polymorphic time
system of Dornic et al., which was extended to static dependent costs by
Reistad and Gifford.

Our main interest in cost information is for scheduling tasks in the
parallel execution of functional languages. Using the GranSim parallel
simulator, we show that the information provided by our analysis is
sufficient to characterise relative task granularities for a simple
functional program. This information can be used in the runtime-system of
the Glasgow Parallel Haskell compiler to improve dynamic program
performance."
}

@InProceedings{Loid97a,
  author = 	 {Loidl, H-W.},
  title = 	 {{LinSolv: a Case Study in Strategic Parallelism}},
  year =         1997,
  booktitle =	 {Glasgow Workshop on Functional Programming},
  address =	 {Ullapool, Scotland, September 15--17},
  OPTnote =         {To appear},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/linsolv.ps.gz},
  ALTurl =          {http://www.dcs.gla.ac.uk/\~{}hwloidl/publications/LinSolv.ps.gz},
  annote =	 {ToDo: add URLs and abstract},
}

@InProceedings{HLT*97,
  author = 	 {Hall, C.V. and Loidl, H-W. and Trinder, P.W. and Hammond,
                  K. and O'Donnell, J.T.},
  title = 	 {{Refining a Parallel Algorithm For Calculating Bowings}},
  year =         1997,
  booktitle =	 {Glasgow Workshop on Functional Programming},
  address =	 {Ullapool, Scotland, September 15--17},
  note =         {To appear},
  annote =	 {ToDo: add URLs and abstract},
}

@InProceedings{HLP95,
  author =       {Hammond, K. and Loidl, H-W. and Partridge, A.},
  title =        {{Visualising Granularity in Parallel Programs: A
                   Graphical Winnowing System for Haskell}},
  year =         {1995},
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  pages =        {208--221},
  address =	 {Denver, Colorado, April 10--12},
  url =          {http://www.dcs.st-and.ac.uk/\~{}kh/papers/hpfc95/hpfc95.html},
  abstract =     "To take advantage of distributed-memory parallel machines it is essential
to have good control of task granularity. This paper describes a fairly
accurate parallel simulator for Haskell, based on the Glasgow compiler, and
complementary tools for visualising task granularities. Together these
tools allow us to study the effects of various annotations on task
granularity on a variety of simulated parallel architectures. They also
provide a more precise tool for the study of parallel execution than has
previously been available for Haskell programs.

These tools have already confirmed that thread migration is essential in
parallel systems, demonstrated a close correlation between thread execution
times and total heap allocations, and shown that fetching data
synchronously normally gives better overall performance than asynchronous
fetching, if data is fetched on demand."
} 


% InProceedings{LoHa96,
%   author =       {Loidl, H-W. and Hammond, K.},
%   title =        {{On the Granularity of Divide-and-Conquer Parallelism}},
%   year =         {1995},
%   series =	 {Workshops in Computing},
%   booktitle =	 {Glasgow Workshop on Functional Programming},
%   publisher =	 S-V,
%   address =	 {Ullapool, Scotland, July 10 -- 12}
%}

%  publisher =	 {Lawrence Livermore National Laboratory},

@Manual{Gran,
  title = 	 {GranSim User's Guide},
  key =		 {GranSim},
  author =	 {Hans-Wolfgang Loidl},
  organization = {Department of Computing Science},
  address =	 {University of Glasgow},
  edition =	 {0.03},
  year =	 1996,
  month =	 {July},
  annote =	 {Part of GHC documentation in 0.29+}
}

@TechReport{LHP94,
  author = 	 {Loidl, H-W. and Hammond, K. and Partridge, A.},
  title  = 	 {{Solving Systems of Linear Equations Functionally: a Case
		  Study in Parallelisation}},
  institution =  {Department of Computing Science}, 
  address =      {University of Glasgow},
  year =	 {1995},
}

%LNCS 1268 (1997) Implementation of Functional Languages
%8th International Workshop
%Bad Godesberg, Germany, September 1996
%Selected Papers

@InProceedings{LoHa96a,
  author = 	 {Loidl, H-W. and Hammond, K.},
  title = 	 {{Making a Packet: Cost-Effective Communication for a
                   Parallel Graph Reducer}},
  booktitle = 	 {{IFL'96 ---  International Workshop on the Implementation of Functional Languages}},
  year =	 1996,
  address =	 {Bad Godesberg, Germany},
  month =	 {September},
  series =       LNCS,
  volume =       {1268},
  pages =        {184--199},
  publisher =    S-V,
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/packet.ps.gz},
  ALTurl =       {http://www.dcs.gla.ac.uk/\~{}hwloidl/publications/IFL96.ps.gz},
  abstract =     "This paper studies critical runtime-system issues encountered when
packing data for transmission in a lazy, parallel graph reduction system. 
In particular, we aim to answer two questions:
 - How much graph should go into a packet?
 - How aggressively should a processor look for work after requesting remote data?

In order to answer the first question, we compare various
packing schemes, of which one extreme packs just the node
that is demanded (``incremental fetching''), and the other packs all
the graph that is reachable from that node (``bulk fetching'').  The
second question is addressed by considering various mechanisms for
latency hiding during communication, ranging from fully synchronous
communication with no attempt to mask latency, to full thread
migration during asynchronous communication.

In order to make our results as general as possible, we have used the
GranSim simulator to study a wide variety of parallel machine
configurations.  Based on these measurements we propose concrete
improvements for parallel graph reducers such as the GUM
implementation of Glasgow Parallel Haskell.
"
}

@InProceedings{THL*96,
  author = 	 {Trinder, P.W. and Hammond, K. and Loidl, H-W. and 
                  {Peyton Jones}, S.L. and Wu, J.},
  title = 	 {{A Case Study of Data-intensive Programs in Parallel
                   Haskell}}, 
  booktitle =	 {Glasgow Workshop on Functional Programming},
  year =         {1996},
  series =	 {Workshops in Computing},
  publisher =	 S-V,
  address =	 {Ullapool, Scotland, July 8--10}
}

@Article{THLP98,
  author = 	 {Trinder, P.W. and Hammond, K. and Loidl, H-W. and {Peyton
                  Jones}, S.L.}, 
  title  = 	 {{Algorithm $+$ Strategy $=$ Parallelism}},
  journal =      JFP,
  volume =       {8},
  number =       {1},
  pages =        {23--60},
  year =	 {1998},
  month =	 {January},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/strategies.ps.gz},
  ALTurl =          {http://www.dcs.glasgow.ac.uk/\~{}hwloidl/publications/strategies.ps.gz},
  ALTurl =       {http://www.cee.hw.ac.uk/~dsg/gph/papers/ps/strategies.ps.gz},
  abstract =     "
The process of  writing large parallel programs is  complicated by the need
to specify both the parallel  behaviour of the   program and the  algorithm
that is to be used to compute its  result. This paper introduces evaluation
strategies,  lazy   higher-order functions     that control  the   parallel
evaluation of non-strict functional languages. Using evaluation strategies,
it  is  possible to achieve  a  clean   separation between algorithmic  and
behavioural code. The  result is   enhanced  clarity and shorter   parallel
programs.

Evaluation strategies are a very general concept: this paper shows how they
can be used to  model a wide range  of commonly used programming paradigms,
including  divide-and-conquer, pipeline   parallelism,    producer/consumer
parallelism, and data-oriented   parallelism.  Because they are   based  on
unrestricted  higher-order  functions,  they   can also capture   irregular
parallel structures.

Evaluation strategies  are not   just  of theoretical interest:   they have
evolved out of our experience in parallelising several large-scale parallel
applications, where they have  proved invaluable in  helping to manage  the
complexities of parallel behaviour.   These applications are  described  in
detail here. The largest application we have studied  to date, Lolita, is a
60,000 line  natural language parser. Initial  results  show that for these
programs  we can achieve  acceptable  parallel performance, while incurring
minimal overhead for using evaluation strategies."
}

@InProceedings{THLP96,
  author = 	 {Trinder, P.W and Hammond, K. and Loidl, H-W. and {Peyton
                  Jones}, S.L.},
  title = 	 {{Algorithm $+$ Strategy $=$ Parallelism}},
  booktitle = 	 {{IFL'96 --- International Workshop on the Parallel
		   Implementation of Functional Languages}},
  year =	 1996,
  address =	 {Bonn/Bad-Godesberg, Germany},
  month =	 {September},
  annote =	 {First published strategy paper; not in proceedings}
}

@InProceedings{THL*94,
  author = 	 {Trinder, P. and Hammond, K. and Loidl, H-W. and {Mattson Jr.},
		  J. and Partridge, A. and {Peyton Jones}, S.L.},
  title = 	 {{GRAPHing the Future}},
  booktitle =    {{IFL'94 --- International Workshop on the Implementation of
                   Functional Languages}}, 
  year =	 {1994},
  address =	 {University of East Anglia, Norwich, U.K., September 7--9},
  url =          {ftp://ftp.dcs.gla.ac.uk/pub/glasgow-fp/authors/Hans\_Loidl/publications/gum-ifl94.ps.gz},
  abstract =     "At Glasgow our research into parallel functional programming has been
moving away from our novel architecture, GRIP towards the provision of
a general parallel runtime environment.  We call this GRAPH (Graph
Reduction for an Assortment of Parallel Hardware).

This paper describes the design of a new memory and load management
model for GRAPH, which is intended to match shared- and
distributed-memory machines better and to provide a framework for our
research into parallel functional databases and granularity control.
This model is currently under implementation.  No performance
results can therefore be provided at the present time."
}

@InProceedings{LoHa94,
  author = 	 {Loidl, H-W. and Hammond, K.},
  title = 	 {{GRAPH for PVM: Graph Reduction for Distributed Hardware}},
  booktitle =    {{IFL'94 --- International Workshop on the Implementation of
                   Functional Languages}},
  year =	 {1994},
  address =	 {University of East Anglia, Norwich, U.K., September 7--9},
  url =          {http://www.dcs.gla.ac.uk/\~{}hwloidl/publications/IFL94.ps.gz},
  abstract =     "We describe a  version of the GRAPH  system (Graph Reduction for
  an   Assortment  of Parallel  Hardware)    designed to  execute  parallel
  functional   programs on  a range  of   distributed-memory MIMD  machines
  running  the  PVM communications harness.    GRAPH was developed from the
  runtime system for the novel  GRIP multiprocessor.  Although this  system
  has  proved highly successful, being  a novel architecture  it is hard to
  compare results with  other  architectures or implementations.  The  CPUs
  used in the GRIP design are also rather old.

  The  principal extensions  from  GRAPH  for  GRIP  to GRAPH  for PVM  are
  intended  to handle   high  latencies  more efficiently,  by   exploiting
  asynchronous  communication, multi-threaded  scheduling, and by  grouping
  packets into larger entities where possible.   The main innovation is the
  development of  new, sophisticated {\em  packet flushing} and {\em packet
    fetching} algorithms whose purpose is to allow graphs to be exported to
  and fetched from  global memory  without inter-processor synchronisation.
  As communication and    reduction  can be interleaved,    a  new form   of
  synchronising communication and reduction is necessary.

  We also give a short outlook on the design  of a new portable distributed
  graph reduction     system, GRAPH for  UMM,  that   adepts the  ideas and
  techniques originally  developed  for the  GRIP system  to  a distributed
  memory environment.

  GRAPH for PVM has been implemented  on a network  of SUN workstations and
  it  is  currently being   tested and   debugged.   Although no  extensive
  performance  measurements have  been  performed  yet, the available  data
  shows a significant  decrease in the overall   communication and thus  an
  improvement in the overall performance of the system."
}

@InProceedings{LoTr97,
  author = 	 {Loidl, H-W. and Trinder, P.W.},
  title = 	 {{Engineering Large Parallel Functional Programs}},
  booktitle =    {{IFL'97 --- International Workshop on the Implementation of
                   Functional Languages}},
  year =	 1997,
  address =	 {University of St.\ Andrews, Scotland, September 10--12},
  pages =        {178--197},
  series =       LNCS,
  volume =       {1467},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/eng.ps.gz},
  ALTurl =          {http://www.dcs.gla.ac.uk/\~{}hwloidl/publications/IFL97.ps.gz},
  abstract =     "The design and implementation of useful programming languages, whether
  sequential or parallel, should be driven by large, realistic
  applications.  In constructing several medium- and large-scale programs
  in Glasgow Parallel Haskell, GPH, a parallel extension of Haskell,
  the group at Glasgow has investigated several important engineering
  issues:
  \begin{itemize}
  \item {\it Real Application Parallelism.} The programs achieve good
    wall-clock speedups and acceptable scale-up on both a shared-memory and
    a distributed memory machine. The programs typify a number of
    application areas and use a number of different parallel paradigms,
    e.g. pipelining or divide-and-conquer, often combining several
    paradigms in a single program.
  \item {\it Language Issues.} Although the largely implicit parallelism in
    GPH is a satisfactory programming model in general the base
    constructs for introducing and controlling parallelism tend to
    obfuscate the semantics of large programs.  As a result we developed
    evaluation strategies, a more abstract, and systematic mechanism
    for introducing and controlling parallelism. 
  \item {\it Engineering Environment.} The development and performance
    tuning of these programs emphasised the importance of an integrated
    engineering environment. In the process we have refined components of
    this environment like the simulator, the runtime system, and the
    profiling tools.
\end{itemize}"
}

@InProceedings{LMT*97,
  author = 	 {Loidl, H-W. and Morgan, R. and Trinder, P.W. and Poria,
                  S. and Cooper, C. and {Peyton Jones}, S.L. and
                  Garigliano, R.},
  title = 	 {{Parallelising a Large Functional Program; Or: Keeping LOLITA Busy}},
  booktitle =    {{IFL'97 --- International Workshop on the Implementation of
                   Functional Languages}},
  year =	 1997,
  address =	 {University of St.\ Andrews, Scotland, September 10--12},
  pages =        {199--214},
  series =       LNCS,
  volume =       {1467},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/lolita.ps.gz},
  ALTurl =          {http://www.dcs.gla.ac.uk/\~{}hwloidl/publications/Lolita.ps.gz},
  abstract =     "In this paper we report on the ongoing parallelisation of Lolita,
a natural language engineering system. Although Lolita currently exhibits only modest
parallelism, we believe that it is the largest parallel functional program
ever, comprising more than 47,000 lines of Haskell. Lolita has the
following interesting features common to real world applications of lazy languages:
\begin{itemize}
\item the code was not specifically designed for parallelism; 
\item laziness is essential for efficiency in Lolita; 
\item Lolita interfaces to data structures outside the Haskell heap,
  using a foreign language interface; 
\item Lolita was not written by those most closely involved in the
  parallelisation.
\end{itemize}

Our expectations in parallelising the program were to achieve moderate
speedups with small changes in the code.  To date speedups of up to 2.4
have been achieved for Lolita running under a realistic simulation of our
4~processor shared-memory target machine. Most notably, the parallelism is
achieved with a very small number of changes to, and without requiring an
understanding of most of the application.  On the Sun SPARCserver target
machine wall-clock speedup is currently limited by physical memory
availability."
}

%% was: IFL97
%  booktitle =    {{IFL'97 --- International Workshop on the Implementation of
%                   Functional Languages}},
%  year =	 1997,
%  address =	 {University of St.\ Andrews, Scotland, September 10--12},
%  note =         {Draft proceedings},
%  annote =	 {ToDo: add URLs and abstract},
@InProceedings{HLT97,
  author = 	 {Hammond, K. and Loidl, H-W. and Trinder, P.W.},
  title = 	 {{Parallel Cost Centre Profiling}},
  year =         1997,
  booktitle =	 {Glasgow Workshop on Functional Programming},
  address =	 {Ullapool, Scotland, September 15--17},
  OPTnote =         {Submitted for publication},
  pages =        {51--72}, 
  annote =	 {ToDo: add URLs and abstract},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/grancc.ps.gz},
  ALTurl =          {http://www.dcs.gla.ac.uk/fp/workshops/fpw97/HammondLoidlTrinder.ps}
}

@TechReport{Loid95,
  author = 	 {Loidl, H-W.},
  title = 	 {{First Year Research Report}},
  institution =  {Department of Computing Science},
  year = 	 {1995},
  month =        {June},
  address =	 {University of Glasgow}
}

@PhdThesis{Loid97,
  author = 	 {Loidl, H-W.},
  title = 	 {Granularity in Large-Scale Parallel Functional Programming},
  school = 	 {Department of Computing Science},
  month =        {March},
  year = 	 1998,
  address =	 {University of Glasgow},
  annote =	 {Submitted: December 1997; accept with minor changes:
                  February 1998},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/loidl-thesis.ps.gz},
  ALTurl =          {http://www.dcs.gla.ac.uk/\~{}hwloidl/publications/PhD.ps.gz},
  abstract =     "This thesis demonstrates how to reduce the runtime of large non-strict
  functional programs using parallel evaluation. The parallelisation of
  several  programs  shows the importance of granularity,
  i.e. the computation costs of program expressions. The aspect of
  granularity is studied both on a practical level, by presenting and
  measuring runtime granularity improvement mechanisms, and at a more 
  formal level, by devising a static granularity analysis.

  By parallelising several large functional programs this thesis
  demonstrates for the first time the advantages of combining lazy and
  parallel evaluation on a large scale: laziness aids modularity, while
  parallelism reduces runtime.  One of the parallel programs is the
  Lolita system
  which, with more than
  47,000 lines of code, is the largest existing parallel non-strict functional program.
  A new mechanism for parallel programming, evaluation strategies, to which
  this thesis contributes, is shown to be useful in this parallelisation.
  Evaluation strategies simplify parallel programming by separating
  algorithmic code from code specifying dynamic behaviour. For large programs 
  the abstraction provided by functions 
  is maintained by using a data-oriented style of
  parallelism, which defines parallelism over intermediate data
  structures rather than inside the functions.

  A highly parameterised simulator, GranSim, has been constructed
  collaboratively and is discussed in detail in this thesis. GranSim is a
  tool for architecture-independent parallelisation and  a testbed
  for implementing runtime-system features of the parallel graph reduction
  model. By providing an idealised as well as an accurate model of the
  underlying parallel machine, GranSim has proven to be an essential part
  of an integrated parallel software engineering environment. Several
  parallel runtime-system features, such as granularity improvement
  mechanisms, have been tested via GranSim.  It is publicly available and
  in active use at several universities worldwide.

  In order to provide granularity information
  this thesis presents an inference-based static granularity analysis.
  This analysis combines two existing analyses, one for cost and one 
  for size information. It determines an upper bound for
  the computation costs of evaluating an expression in a simple strict
  higher-order language. By exposing recurrences during cost reconstruction
  and using a library of recurrences and their closed forms,
  it is  possible to infer the  costs  for some recursive functions. 
  The possible
  performance improvements are assessed by measuring the parallel
  performance of a hand-analysed and annotated program."
}

%$%node My own publications at RISC, Glaswegian publications, My own publications at Glasgow, Our Own Stuff
%$%subsection My own publications at RISC

% More publications in par-own.bib (Circular programs, PACLIB, etc)

@mastersthesis{Loid92,
  author =       {Loidl, H-W.},
  title =        {{A Parallelizing Compiler for the Functional Programming
                   Language EVE}}, 
  month =        {May},
  year =         {1992},
  school =       {RISC-Linz},
  address =      {Johannes Kepler University, Linz, Austria},
  descr =        {plfun-eve},
}
%note = {Also: Technical Report 92-30, RISC-Linz, Johannes Kepler University,
%              Linz, Austria, May 1992. Also: Technical Report 93-16, ACPC
%              Technical Report Series, Austrian Center for Parallel
%	      Computation, July 1993.},

@TechReport{Loid93b,
  author = 	 {Loidl, Hans Wolfgang},
  title = 	 {{Solving a System of Linear Equations by Using a Modular
		  Method}}, 
  number = 	 {93-69},
  institution =  {RISC-Linz},
  month =        {December},
  year = 	 {1993},
  address =	 {Johannes Kepler University, Linz, Austria},
  descr =        {paca},
}

@InProceedings{LiLo93,
  author = 	 {Limongelli, C. and Loidl, H-W.},
  title = 	 {{Rational Number Arithmetic by Parallel P-adic Algorithms}},
  OPTeditor =	 {Volkert, J.},
  volume =	 734,
  series =	 LNCS,
  pages =	 {72--86},
  booktitle =	 {ACPC'93 --- Parallel Computation --- Second International ACPC
		  Conference}, 
  year =	 1993,
  OPTorganization = {Austrian Center for Parallel Computation (ACPC)},
  publisher =	 S-V,
  address =	 {Gmunden, Austria, October 4--6},
  ALTurl =          {http://www.dcs.gla.ac.uk/\~{}hwloidl/publications/p-adic.ps.gz},
  descr =        {paca},
  OPTalso =	 {Also: Technical Report 93-41, RISC-Linz, Johannes Kepler
		  University, Linz, Austria, July 1993.},
}

@inproceedings{HoLo94,
  author =       {Hong, H. and Loidl, H-W.},
  title =        {{Parallel Computation of Modular Multivariate Polynomial
                   Resultants on a Shared Memory Machine}},
  OPTeditor =	 {Buchberger, B. and Volkert, J.},
  volume =	 {854},
  series =	 {Lecture Notes in Computer Science},
  pages =	 {325--336},
  booktitle =    {CONPAR'94 --- Conference on Parallel and Vector Processing}, 
  year =         {1994},
  address =      {Linz, Austria, September 6--8},
  url =          {http://www.dcs.gla.ac.uk/\~{}hwloidl/publications/resultant.ps.gz},
  OPTalso =         {Also: Technical Report 94-19, RISC-Linz, Johannes Kepler
		  University, Linz, Austria, 1994.}, 
}

@techreport{HSN*92,
  author =       {Hong, H. and Schreiner, W. and Neubacher, A. and Siegl,
		    K. and Loidl, H-W. and Jebelean, T. and Zettler, P.},
  title =        {{PACLIB User Manual}},
  institution =  {RISC-Linz},
  address =      {Johannes Kepler University, Linz, Austria},
  month =        {May},
  year =         {1992},
  number =       {92-32},
  OPTnote =         {Also: Technical Report ACPC/TR 92-9, ACPC Technical Report Series, Austrian Center for Parallel Computation, July 1992},
}

%$%node Glaswegian publications,  , My own publications at RISC, Our Own Stuff
%$%subsection Glaswegian publications

%$%cindex GUM
%$%cindex GRIP

@InProceedings{Hamm94,
  author = 	 {Hammond, K.}, 
  title = 	 {{Parallel Functional Programming: An Introduction}},
  OPTeditor =	 {H. Hong},
  volume =	 {5},
  series =	 {Lecture Notes Series on Computing},
  pages =	 {181--193},
  booktitle =    {{PASCO'94 ---  International Symposium on Parallel Symbolic
		  Computation}}, 
  year =	 {1994},
  organization = {RISC-Linz},
  publisher =	 {World Scientific},
  address =	 {Hagenberg/Linz, Austria, September 26--28},
  url =          {http://www.dcs.st-and.ac.uk/\~{}kh/papers/pasco94/pasco94.html},
  ALTurl =       {ftp://ftp.dcs.glasgow.ac.uk/pub/glasgow-fp/papers/parallel-introduction.ps.Z},
  abstract =     "This paper introduces the general area of parallel functional programming,
surveying the current state of research and suggesting areas which could
profitably be explored in the future. No new results are presented. The
paper contains 97 references selected from the 500 or so publications in
this field."
}

@InProceedings{HMP*95,
  author = 	 {Hammond, K. and {Mattson Jr.}, J.S. and Partridge, A.S
                  and {Peyton Jones}, S.L. and Trinder, P.},
  title = 	 {{GUM: a Portable Parallel Implementation of Haskell}},
  OPTeditor = 	 {Johnsson, T.},
  booktitle =    {{IFL'95 ---  International Workshop on the Parallel
		  Implementation of Functional Languages}},
  year =	 {1995},
  address =	 {Bastad, Sweden},
  month =	 {September},
  url =          {ftp://ftp.dcs.gla.ac.uk/pub/glasgow-fp/authors/Simon\_Peyton\_Jones/gum-ifl95.ps.gz},
  abstract =     "GUM is a portable, parallel implementation of the Haskell functional
language which has been publicly released with version 0.26 of the Glasgow
Haskell Compiler (GHC). GUM is message-based, and portability is
facilitated by using the PVM communications-harness available on most
multi-processors, including shared-memory and distributed-memory
machines. For example GUM is available by FTP for a Sun SPARCserver
multiprocessor and for a networks of Sun SPARC workstations.

High message-latency in distributed machines is ameliorated by sending
messages asynchronously, and by sending large packets of related data in
each message. Initial performance figures demonstrate absolute speedups
relative to the best sequential compiler technology. To improve the
performance of a parallel Haskell program GUM provides tools for monitoring
and visualising the behaviour of threads and of PEs during execution."
}

%$%InProceedings{HLT97,
%  author = 	 {Hammond, K. and Loidl, H-W. and Trinder, P.W.},
%  title = 	 {{Parallel Cost-Centre Profiling}},
%  booktitle = 	 {IFL'97 --- International Workshop on the Implementation of
%                  Functional Languages},
%  year = 	 {1997},
%  address = 	 {September 10--12, St.\ Andrews, Scotland},
%  annote = 	 {draft proceedings for now}
%}

@InProceedings{THM*96,
  author = 	 {Trinder, P. and Hammond, K. and {Mattson Jr.}, J.S. and
                  Partridge, A.S and {Peyton Jones}, S.L.},
  title = 	 {{GUM: a Portable Parallel Implementation of Haskell}},
  booktitle =    {{PLDI'96 --- Programming Languages Design and
                   Implementation}}, 
  year =	 {1996},
  address =	 {Philadelphia, PA, USA},
  pages =        {79--88},
  month =	 {May},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/gum.ps.gz},
  ALTurl =       {{ftp://ftp.dcs.glasgow.ac.uk/pub/glasgow-fp/authors/Philip\_Trinder/gumFinal.ps.Z}},
  abstract =     "GUM is a portable, parallel implementation of the Haskell functional
language. Despite sustained research interest in parallel functional
programming, GUM is one of the first such systems to be made publicly
available. GUM is message-based, and portability is facilitated by using
the PVM communications harness that is available on many
multi-processors. As a result, GUM is available for both shared-memory (Sun
SPARCserver multiprocessors) and distributed-memory (networks of
workstations) architectures. The high message-latency of distributed
machines is ameliorated by sending messages asynchronously, and by sending
large packets of related data in each message. Initial performance figures
demonstrate absolute speedups relative to the best sequential compiler
technology. To improve the performance of a parallel Haskell program GUM
provides tools for monitoring and visualising the behaviour of threads and
of processors during execution."
}

% Not really by `us' but Kei is family anyways

@InProceedings{Davi96,
  author = 	 {Davis, K.},
  title = 	 {{MPP Parallel Haskell}},
  booktitle = 	 {{IFL'96 ---  International Workshop on the Implementation of Functional Languages}},
  year =	 1996,
  address =	 {Bad Godesberg, Germany},
  month =	 {September},
  pages =	 {49--54},
  note =	 {Draft Proceedings},
  annote =	 {draft proceedings only}
}

%$%InProceedings{LoTr97,
%  author = 	 {Loidl, H-W. and Trinder, P.},
%  title = 	 {{Engineering Large Parallel Functional Programs}},
%  booktitle = 	 {IFL'97 --- International Workshop on the Implementation of
%                  Functional Languages},
%  year = 	 {1997},
%  address = 	 {September 10--12, St.\ Andrews, Scotland},
%  annote = 	 {draft proceedings for now}
%}

%$%InProceedings{LMT*97,
%  author = 	 {Loidl, H-W. and Morgan, R. and Trinder, P. and
%                  Poria, S. and Cooper, C. and {Peyton Jones}, S. and
%                  Garigliano, R.}, 
%  title = 	 {{Parallelising a Large Functional Program; or Keeping
%                   Lolita Busy}},
%  booktitle = 	 {IFL'97 --- International Workshop on the Implementation of
%                  Functional Languages},
%  year = 	 {1997},
%  address = 	 {September 10--12, St.\ Andrews, Scotland},
%  annote = 	 {draft proceedings for now}
%}

@InProceedings{THL*97,
  author = 	 {Trinder, P. and Hammond, K. and Loidl, H-W. and  {Peyton Jones}, S.L. and Wu, J.},
  title = 	 {{Go-faster Haskell; Or: Data-intensive Programming in Parallel Haskell}},
  year =         1997,
  booktitle =	 {Glasgow Workshop on Functional Programming},
  address =	 {Ullapool, Scotland, September 15--17},
  annote =	 {Draft Proceedings only}
}

@InProceedings{THL*98,
  author = 	 {Trinder, P. and Hammond, K. and Loidl, H-W. and  {Peyton Jones}, S.L. and Wu, J.},
  title = 	 {{Go-faster Haskell; Or: Data-intensive Programming in Parallel Haskell}},
  year =         1998,
  booktitle =	 {{ICFP'98 --- International Conference on Functional Programming}},
  address =	 {Baltimore, MD, USA, September 27--29},
  note =	 {Submitted for publication},
}


@InProceedings{TBD*98a,
  author = 	 {Trinder, P.W. and {Barry Jr.}, E. and Davis, M.K. and Hammond, K. and Junaidu, S.B. and  Klusik, U. and Loidl, H-W. and {Peyton Jones}, S.L.},
  title = 	 {{Low level Architecture-independence of Glasgow Parallel Haskell (GpH)}},
  year =         1998,
  booktitle =	 {Glasgow Workshop on Functional Programming},
  address =	 {Pitlochry, Scotland},
  month =        {September},
  annote =	 {Draft Proceedings}
}

@InProceedings{HaskSkel,
  author =   {Hammond, K. and {Rebon Portillo}, A. J.},
  title =    {{HaskSkel: Algorithmic Skeletons for Haskell}},
  booktitle =  {{IFL'99 --- Intl.\ Workshop on the Implementation of Functional Languages}},
  year =     1999,
  series =   {LNCS},
  volume =   {1868},
  pages =    {181--198},
  address =  {Lochem, The Netherlands},
  month =    sep,
  publisher =    {Springer-Verlag},
  annote =   {To appear},
  url =          {http://www-fp.dcs.st-and.ac.uk/publications/1999/haskskel.ps.gz},
}

@InProceedings{ClPe86,
  author = 	 {Clack, C. and {Peyton Jones}, S.L.},
  title = 	 {{The Four-Stroke Reduction Engine}},
  booktitle =    {{LFP'86 --- Conference on Lisp and Functional Programming}},
  year =	 {1986},
  publisher =	 {ACM Press},
  month =	 {August},
  pages = 	 {220--232},
  annote = 	 {first reference to spark model (due to \cite{AuJo89})}
}

@inproceedings{PCSH87,
  author =  {{Peyton Jones}, S.L. and Clack, C. and Salkild, J. and Hardie, M.},
  title =   {{GRIP --- a High-Performance Architecture for Parallel Graph
		  Reduction}}, 
  booktitle = {{FPCA'87 ---  Conference on Functional Programming Languages
		  and Computer Architecture}}, 
  address = {Portland, Oregon, September 14--16},
  year = {1987},
  pages = {98--112},
  series = {Lecture Notes in Computer Science},
  volume = {274},
  publisher = S-V,
  owner = {pcl},
  annote = {Detailed description of GRIP: HW, SW}, 		  
}

@TechReport{Hamm92,
  author = 	 {Hammond, K.},
  title = 	 {{The Spineless Tagless G-machine --- NOT!}},
  institution =  {Department of Computing Science},
  year = 	 1992,
  address =	 {University of Glasgow},
  note =	 {Unpublished document},
  annote =	 {how to improve preformance with a semi-tagging scheme}
}

@Article{Peyt89,
  author =   {{Peyton Jones}, S.L.},
  title =    {{Parallel Implementations of Functional Programming Languages}},
  journal =  {The Computer Journal},
  volume =   {32},
  number =   {2},
  pages =    {175--186},
  month =    {April},
  year =     {1989},
  owner =    {pcl},
  descr =    {plfun},
  annote =   {Survey paper; talks about GRIP as case study},
  abstract = "One of the most attractive features of functional
       programming languages is their suitability for
       programming parallel computers. This paper is devoted
       to discussion of such a claim. Firstly, parallel
       functional programming is discussed from the
       programmer's point of view. Secondly, since most
       parallel functional language implementations are based
       on the concept of graph reduction, the issues raised by
       graph reduction are discussed. Finally, the paper
       concludes with a case study of a particular parallel
       graph reduction machine and a survey of other parallel
       architectures."
}

@InProceedings{PCS89,
  author = 	 {{Peyton Jones}, S.L. and Clack, C. and Salkild, J.},
  title = 	 {{High-Performance Parallel Graph Reduction}},
  number =	 {365},
  series =	 {Lecture Notes in Computer Science},
  pages =	 {193--206},
  booktitle =	 {PARLE'89 --- Parallel Architectures and Languages Europe},
  year =	 {1989},
  publisher =	 S-V,
  notes =        {2 main topics: control growth of par, improve locality},
}

@InProceedings{Peyt96,
  author = 	 {{Peyton Jones}, S.L.},
  title = 	 {{Compiling Haskell by Program Transformation: a Report
     from the Trenches}},
  booktitle = 	{{ESOP'96 --- European Symposium on Programming}},
  publisher = 	S-V,
  OPTeditor = 	 {Nielson, H.R.},
  volume = 	 {1058},
  series = 	 LNCS,
  year = 	 {1996},
  address = 	 {Link\"oping, Sweden, April 22--24},
  pages = 	 {18--44},
  OPTnote = 	 {ISBN 3-540-61055-3},
  annote = 	 {discusses optimisation by transformation approach in GHC},
  url =          {http://www.dcs.gla.ac.uk/fp/authors/Simon\_Peyton\_Jones/comp-by-trans.ps.gz},
  ALTurl = {http://www.dcs.gla.ac.uk/fp/authors/Simon\_Peyton\_Jones/comp-by-trans.ps.gz},
  abstract = "Many compilers do some of their work by means of correctness-
     preserving, and hopefully performanceimproving, program transforma-
     tions. The Glasgow Haskell Compiler (GHC) takes this idea of {"}compi-
     lation by transformation{"} as its war-cry, trying to express as much as
     possible of the compilation process in the form of program transforma-
     tions.

     This paper reports on our practical experience of the transformational
     approach to compilation, in the context of a substantial compiler."
}

@InProceedings{PHH*93,
  author = 	 {{Peyton Jones}, S.L. and Hall, C.V. and Hammond, K. and
                  Partain, W.D. and Wadler, P.L.},
  title = 	 {{The Glasgow Haskell Compiler: a Technical Overview}},
  booktitle = 	 {Joint Framework for Information Technology Technical Conference},
  year =	 1993,
  address =	 {Keele, U.K.},
  month =	 {March},
  pages =	 {249--257},
  annote =	 {std ref for GHC overview},
  url =          {http://www.dcs.gla.ac.uk/fp/papers/grasp-jfit.ps.Z},
  abstract =     "We give an overview of the Glasgow Haskell compiler, focusing especially on
ways  in which we have been  able to exploit  the rich theory of functional
languages to give very practical improvements in the compiler. The compiler
is portable, modular, generates good code, and is freely available."
}

@InProceedings{PGF96,
  author = 	 {{Peyton Jones}, S.L. and Gordon, A. and Finne, S.},
  title = 	 {{Concurrent Haskell}},
  booktitle   =	 {{POPL'96 --- Symposium on Principles of Programming
		      Languages}},
  month       =	 {January},
  year        =	 {1996},
  address     =	 {St Petersburg, Florida},
  publisher   =	 {ACM},
  OPTeditor      =	 {{Steele Jr.}, G.L.},
  pages       =	 {295--308},
  annote = 	 {std ref for concurrent Haskell},
  url =          {http://www.dcs.gla.ac.uk/fp/authors/Simon\_Peyton\_Jones/concurrent-haskell.ps.gz},  
  abstract = "Some applications are most easily expressed  in a programming language that
supports concurrency, notably   interactive  and distributed  systems.   We
propose extensions to the purely-functional language  Haskell that allow it
to express  explicitly  concurrent  applications;  we call  the   resulting
language Concurrent   Haskell.  The resulting system   appears  to  be both
expressive and efficient,   and we  give  a number  of examples  of  useful
abstractions that  can be built from our   primitives. We have  developed a
freely-available implementation of Concurrent Haskell, and are now using it
as a substrate for a graphical user interface toolkit.
"}

@InProceedings{LaPe94,
  author = 	 {Launchbury, J. and {Peyton Jones}, S.L.},
  title = 	 {{Lazy Functional State Threads}},
  booktitle =    {{PLDI'94 --- Programming Languages Design and
                   Implementation}}, 
  year =	 1994,
  month =        aug,
  address =	 {Orlando, Florida},
  url =          {http://www.cse.ogi.edu/\~{}jl/Papers/stateThreads.ps},
  abstract =     "Some algorithms make critical internal use of updatable state, even though
their external specification is purely functional. Based on earlier work on
monads, we present a way of securely encapsulating stateful computations that
manipulate multiple, named, mutable objects, in the context of a non-strict,
purely-functional language. The security of the encapsulation is assured by
the type system, using parametricity. Intriguingly, this parametricity
requires the provision of a (single) constant with rank-2 polymorphic type."
}

@Article{LaPe95,
  author = 	 {Launchbury, J. and {Peyton Jones}, S.L.},
  title = 	 {{State in Haskell}},
  journal = 	 LSC,
  year = 	 1995,
  volume =	 8,
  number =	 4,
  month =	 dec,
  pages =	 {293--342},
  url =          {http://research.microsoft.com/Users/simonpj/Papers/state-lasc.ps.gz}
}


@Article{Wadl94,
  author = 	 {Wadler, P.},
  title = 	 {{Monads and Composable Continuations}},
  journal = 	 {Lisp and Symbolic Computation},
  year = 	 1994,
  volume =	 7,
  number =	 1,
  month =	 {January},
  pages =	 {39--56},
  url =          {http://cm.bell-labs.com/cm/cs/who/wadler/papers/composable/composable.ps.gz},
  abstract =     "Moggi's use of monads to factor semantics is used to model the composable
continuations of Danvy and Filinski. This yields some insights into the type
systems proposed by Murthy and by Danvy and Filinski. Interestingly, modelling
some aspects of composable continuations requires a structure that is almost,
but not quite, a monad."
}

@InProceedings{PeWa93,
  author = 	 {{Peyton Jones}, S.L. and Wadler, P.},
  title = 	 {{Imperative Functional Programming}},
  booktitle =    {POPL'93 --- Symposium on Principles of Programming Languages},
  year =	 1993,
  publisher =	 {ACM Press},
  address =	 {Charleston, South Carolina},
  month =	 {January},
  pages =        {71--84},
  url =          {http://cm.bell-labs.com/cm/cs/who/wadler/papers/imperative/imperative.ps.gz},
  abstract =     "We present a new model, based on monads, for performing input/output in a
non-strict, purely functional language. It is composable, extensible,
efficient, requires no extensions to the type system, and extends smoothly to
incorporate mixed-language working and in-place array updates."
}


@Article{Wadl92a,
  author = 	 {Wadler, P.},
  title = 	 {{Comprehending Monads}},
  journal = 	 {Mathematical Structures in Computer Science},
  year = 	 1992,
  note =	 {Special issue of selected papers from 6'th Conference on Lisp and Functional Programming},
  url =          {http://cm.bell-labs.com/cm/cs/who/wadler/papers/monads/monads.ps.gz},
  abstract =     "Category theorists invented monads in the 1960's to concisely express certain
aspects of universal algebra. Functional programmers invented list
comprehensions in the 1970's to concisely express certain programs involving
lists. This paper shows how list comprehensions may be generalised to an
arbitrary monad, and how the resulting programming feature can concisely
express in a pure functional language some programs that manipulate state,
handle exceptions, parse text, or invoke continuations. A new solution to the
old problem of destructive array update is also presented. No knowledge of
category theory is assumed."
}


@InProceedings{Wadl92,
  author = 	 {Wadler, P.},
  title = 	 {{The Essence of Functional Programming}},
  booktitle = 	 {POPL'92 --- Symposium on Principles of Programming Languages},
  year =	 1992,
  address =      {Albuquerque, New Mexico},
  month =	 {January},
  year =	 1992,
  publisher =	 {ACM Press},
  url =          {http://cm.bell-labs.com/cm/cs/who/wadler/papers/essence/essence.ps.gz},
  abstract =     "This paper explores the use monads to structure functional programs. No prior
knowledge of monads or category theory is required.

Monads increase the ease with which programs may be modified. They can mimic
the effect of impure features such as exceptions, state, and continuations;
and also provide effects not easily achieved with such features. The types of
a program reflect which effects occur.

The first section is an extended example of the use of monads. A simple
interpreter is modified to support various extra features: error messages,
state, output, and non-deterministic choice. The second section describes the
relation between monads and continuation-passing style. The third section
sketches how monads are used in a compiler for Haskell that is written in
Haskell."
}


@InProceedings{KiWa92,
  author = 	 {King, D. and Wadler, P.},
  title = 	 {{Combining Monads}},
  booktitle = 	 {Glasgow Workshop on Functional Programming},
  series =	 {Workshops in Computing Series},
  year =	 1992,
  publisher =	 S-V,
  address =	 {Ayr, July},
  url =          {http://cm.bell-labs.com/cm/cs/who/wadler/papers/monadscomb/monadscomb.ps.gz},
  abstract =     "Monads provide a way of structuring functional programs. Most real
applications require a combination of primitive monads. Here we describe how
some monads may be combined with others to yield a combined monad."
}

@InProceedings{HaPe92,
  author =	{Hammond, K. and {Peyton Jones}, S.L.},
  title =	{Profiling {S}cheduling {S}trategies on the {GRIP}
		  {M}ultiprocessor}, 
  OPTeditor =       {Kuchen, H. and Loogen, R.},
  booktitle =    {{IFL'92 --- International Workshop on the Parallel
		  Implementation of Functional Languages}},
  pages =        {73--98},
  year =	 {1992},
  address =	 {RWTH Aachen, Germany},
  month =	 {September},
  url =          {ftp://ftp.dcs.gla.ac.uk/pub/glasgow-fp/papers/grip-scheduling.ps.gz},
  abstract =     "It is widely claimed that functional languages are particularly suitable
for programming parallel computers. A claimed advantage is that the
programmer is not burdened with details of task creation, placement,
scheduling, and synchronisation, these decisions being taken by the system
instead. Leaving aside the question of whether a pure functional language
is expressive enough to encompass all the parallel algorithms we might wish
to program, there remains the question of how effectively the compiler and
run-time system map the program onto a real parallel system, a task usually
carried out mostly by the programmer. This is the question we address in
this paper.

We first introduce the system architecture of GRIP, a shared-memory
parallel machine supporting an implementation of the functional language
Haskell. GRIP executes functional programs in parallel using compiled
supercombinator graph reduction, a form of declarative rule system.

We then describe several strategies for run-time resource control which we
have tried, presenting comprehensive measurements of their
effectiveness. We are particularly concerned with strategies controlling
task creation, in order to improve task granularity and minimise
communication overheads. This is, so far as we know, one of the first
attempts to make a systematic study of task-control strategies in a
high-performance parallel functional-language system. GRIP's high absolute
performance render these results credible for real applications."
}

% was: GRIP
@inproceedings{HaPe90,
   author = {Hammond, K. and {Peyton Jones}, S.L.},
   title = {{Some Early Experiments on the GRIP Parallel Reducer}},
   booktitle = {{IFL'90 --- International Workshop on the Parallel
                 Implementation of Functional Languages}},
   address = 	 {Nijmegen, The Netherlands},
   month = {June},
   key = {HP90},
   pages = {51--72},
   year = {1990},
   url = {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/early-grip.ps.gz}
}

@InProceedings{Hamm93,
  author = 	 {Hammond, K.},
  title = 	 {{Getting a GRIP}},
  booktitle =    {{IFL'93 --- International Workshop on the Parallel
                   Implementation of Functional Languages}},
  year =	 {1993},
  month =        {September},
  address = 	 {Nijmegen, The Netherlands},
  url =          {{ftp://ftp.dcs.gla.ac.uk/pub/glasgow-fp/authors/Kevin\_Hammond/Getting\_a\_GRIP.ps.Z}},
  abstract =     "This paper describes a portable implementation of the Graph Reduction in
Parallel (GRIP) software, built on the Parallel Virtual Machine (PVM)
process control system.  Using this system, it is easy for anyone to ``get
a GRIP'' without needing to invest in special-purpose hardware.  An
important contribution of this paper is its detailed description of GRIP's
Intelligent Memory Unit (IMU) software."
}

@InProceedings{HMP94,
  author = 	 {Hammond, K. and {Mattson Jr.}, J.S. and {Peyton Jones}, S.L.},
  title = 	 {{Automatic Spark Strategies and Granularity for a
		  Parallel Functional Language Reducer}},
  OPTeditor =	 {Buchberger, B. and Volkert, J.},
  volume =       {854},
  series =	 {Lecture Notes in Computer Science},
  pages =        {521--532},
  booktitle =    {{CONPAR'94 --- Conference on Algorithms and Hardware for
		  Parallel Processing}},
  year =	 {1994},
  address =	 {Linz, Austria, September 6--8},
  url =          {ftp://ftp.dcs.glasgow.ac.uk/pub/glasgow-fp/papers/spark-strategies-and-granularity.ps.Z},
  abstract =     "This paper considers the issue of dynamic task control in the context of a
parallel Haskell implementation on the GRIP multiprocessor. For the first
time, we report the effect of our task control strategies on task
granularity, as measured in terms of dynamic heap allocations. This gives a
concrete means of measuring the effectiveness of these strategies other
than wall-clock timings, which are notoriously uninformative."
}

@InProceedings{AHPT93,
  author = 	 {Akerholt, G. and Hammond, K. and {Peyton Jones}, S.L.
	          and Trinder, P.},
  title = 	 {{Processing Transactions on GRIP}},
  booktitle =    {{PARLE'93 --- Parallel Languages and Architectures
                 Europe}},
  address =      {Munich, Germany, June 14--18},
  year =         1993,
  series =	 LNCS,
  publisher =	 S-V,
  pages =	 {634--647},
  url =          {ftp://ftp.dcs.gla.ac.uk/pub/glasgow-fp/papers/grip-transactions.ps.Z},
  abstract =     "The GRIP architecture allows efficient execution of functional
       programs on a multi-processor built from standard hardware components.
       State-of-the-art compilation techniques are combined with
       sophisticated runtime resource-control to give good parallel
       performance.  This paper reports the results of running GRIP on an
       application which is apparently unsuited to the basic functional
       model: a database transaction manager incorporating updates as well as
       lookup transactions.  The results obtained show good relative speedups
       for GRIP, with real performance advantages over the same application
       executing on sequential machines."
}

@InBook{Hamm95a,
  author = 	 {Hammond, K.},
  title = 	 {{Applications of Functional Programming}},
  chapter = 	 {{Implementation on a Parallel Machine}},
  publisher = 	 {UCL Press},
  year = 	 1995,
  OPTeditor =	 {Runciman, C. and Wakeling, D.},
  pages =	 {177--190},
  annote =	 {Talks about results of ray tracer on GRIP}
}

@Book{HaMi98,
  editor =	 {Hammond, K. and Michaelson, G.},
  title = 	 {{Research Directions in Parallel Functional Programming}},
  publisher = 	 S-V,
  month =        oct,
  year = 	 {1999},
  OPTnote = 	 {In Preparation},
}


@Book{SkTa94,
  ALTeditor = 	 {Skillicorn, D.  and Talia, D.},
  title = 	 {{Programming Languages for Parallel Processing}},
  publisher = 	 {IEEE Computer Society Press},
  year = 	 {1994},
  annote = 	 {Survey of parallel languages in general, covering different paradigms as well as different parallel architectures},
  abstract =     "The book  contains a  set of high-quality  papers describing  various para-
digms which have been defined and implemented to support different mo- dels
of parallelism. They represent  a balance of practical approaches currently
used  by many  practitioners  (e.g.,  C*, Occam,  PVM,  HPF, Linda,  Sisal,
ABCL/1) and  research proposals which will  be fruitful in  the near future
(e.g.,  Orca,  Concurrent   Aggregates,  PCN,  Actors,  Mentat,  Concurrent
Constraint languages, CC++, P3L, BSP language).

This  book  presents  and  discusses  programming  languages  for  parallel
processing architectures. The aim of the book is to give an overview of the
most important  parallel programming languages designed in  the last decade
and  to introduce  issues  and concepts  related  to the  deve- lopment  of
parallel software.  The book covers both parallel  languages currently used
to develop parallel  applications in many areas from  numerical to symbolic
computing,  and new  parallel programming  languages that  will be  used to
program parallel computers in the next ten years.

The  text first  gives an  overview of  parallel programming  paradigms and
discusses   the   major   properties   of  several   parallel   programming
languages.  Papers  describing  parallel  programming  languages  are  then
collected into six sections, classified  according to the paradigm they use
to express parallelism: languages based on the shared-memory model (Section
2), lan- guages based on the distributed-memory model (Section 3), parallel
object-  oriented languages  (Section 4),  parallel  functional programming
languages (Section 5), and concurrent logic languages (Section 6). Finally,
a collection of innovative  approaches to parallel programming is presented
in Section  7. An introduction is  provided for each  section. An extensive
bibliography is also provided."
}
@InProceedings{KHT98,
  author = 	 {King, D.J. and Hall, J.G. and Trinder, P.W.},
  title = 	 {{A Strategic Profiler for Glasgow Parallel Haskell}},
  booktitle = 	 {IFL'98 --- International Workshop on the Implementation of
                  Functional Languages},
  year = 	 1998,
  series =	 LNCS,
  volume =       1595,
  address = 	 {September 9--11, University College London, UK},
  publisher =	 S-V,
  pages =        {90--104},
  note = 	 {To appear in Springer Verlag LNCS},
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/gransp.ps.gz}
}


@InProceedings{HBTK98,
  author = 	 {Hall, J.G. and  {Baker-Finch}, C. and Trinder, P.W. and King, D.J.},
  title = 	 {{Towards an Operational Semantics for a Parallel Non-Strict Functional Language}},
  booktitle = 	 {IFL'98 --- International Workshop on the Implementation of
                  Functional Languages},
  year = 	 1998,
  series =	 LNCS,
  volume =       1595,
  pages =        {55-72},
  address = 	 {September 9--11, University College London, UK},
  publisher =	 S-V,
  url =          {http://www.cee.hw.ac.uk/\~{}dsg/gph/papers/ps/semantics.ps.gz}
}

@InProceedings{BKT00,
  author = 	 {{Baker-Finch}, C. and King, D.J. and Trinder, P.W.},
  title = 	 {{An Operational Semantics for Parallel Lazy Evaluation}},
  booktitle =    {{ICFP 2000 --- International Conference on Functional Programming}},
  year =         {2000},
  month =        sep,
  note =	 {To appear},
  abstractURL =  {http://www.cee.hw.ac.uk/~dsg/gph/papers/abstracts/icfp00.html},
  url =          {http://www.cee.hw.ac.uk/~dsg/gph/papers/icfp00.ps.gz},
  abstract =     "We present an operational semantics for \emph{parallel lazy
evaluation} that accurately models the
parallel behaviour the non-strict parallel functional language \GpH.
Parallelism is modelled synchronously, that is, single reductions are
carried out separately, then combined before proceeding onto the next
set of reductions.  Consequently the semantics has two levels, with
single-step transition rules at one level and combining rules at the
other.  Parallel threads are modelled by labelled bindings and to the
best of our knowledge this is the first semantics that models thread
states.  A set of labelled bindings corresponds to a heap and is used
to model sharing.

The semantics is set at a higher level of abstraction than an abstract
machine and is therefore more manageable for proofs about programs
rather than implementations.  At the same time,
it is low level enough to allow us to reason about programs in terms
of \emph{parallelism} (i.e., the number of processors used), as well
as \emph{work} and \emph{run-time} with different numbers of
processors.

The framework used by the semantics is flexible in that
the model can easily be adapted to express other evaluation models
such as sequential call-by-need and fully-speculative evaluation,
non-deterministic choice and others.",
}

@InProceedings{Trin99,
  author = 	 {Trinder, P.W.},
  title = 	 {{Motivation for Glasgow distributed Haskell, a non-strict Functional Language}},
  booktitle =    {{PDSIA'99 --- Parallel and Distributed Computing for Symbolic and Irregular Applications}},
  OPTeditor =    {Ito, T. and Yuasa, T.},
  year =         {1999},
  address =      {Sendai, Japan, July 5--7},
  publisher =	 {World Scientific},
  abstractURL =  {http://www.cee.hw.ac.uk/~dsg/gph/papers/abstracts/pdsia00.html},
  url =          {http://www.cee.hw.ac.uk/~dsg/gph/papers/pdsia00.ps.gz},
  abstract =     "Non-strict functional languages offer potential
benefits for constructing distributed systems: namely a highly-dynamic
model of distribution, a relatively high degree of distribution
transparency, and the potential to abstract over distribution-control
primitives.  We describe our motivation for implementing such a
language, a variant of Haskell, and evaluating it.  The implementation
is a fusion of existing Glasgow Haskell Compiler technologies. The
evaluation will be based on experiences implementing a distributed
interactive simulation, and comparing it with a Java version.",
}


% sort of; did work on GUM

@PhdThesis{Part91,
  author = 	 {Partridge, A.},
  title = 	 {{Speculative Evaluation in Parallel Implementations of
                   Lazy Functional Languages}},
  school = 	 {University of Tasmania},
  year = 	 1991
}

%$% speculative parallelism

@PhdThesis{Matt93,
  author = 	 {{Mattson Jr.}, J.S.},
  title = 	 {{An Effective Speculative Evaluation Technique for Parallel
                   Supercombinator Graph Reduction}},
  school = 	 {Department of Computer Science and Engineering},
  year = 	 {1993},
  address =	 {University of California, San Diego}
}

@TechReport{Osborne,
  author = 	 {Osborne, R.B.},
  title = 	 {{Speculative Computation in Multilisp}},
  institution =  {DEC Cambridge Research Lab},
  year = 	 1990,
  number =	 {CRL 90/1},
  month =	 mar
}

@Article{ParProlog,
  author = 	 {Gupta, G. and Pontelli, E. and Ali, K.A.M. and Carlsson, M. andHermenegildo, M.},
  title = 	 {{Parallel Execution of Prolog Programs: A Survey}},
  journal = 	 TOPLAS,
  year = 	 2001,
  volume =	 23,
  number =	 4,
  pages =	 {472--602},
  month =	 jul
}


% PhdThesis{Matt93,
%   author = 	 {{Mattson Jr}, J.S.},
%   title = 	 {{An Effective Technique for Parallel Supercombinator
%                    Graph Reduction}},
%   school = 	 {University of California, San Diego},
%   year = 	 1993
% }


@InProceedings{Hamm99,
  author = 	 {Hammond, K.},
  title = 	 {{Evaluation Strategies for Clustering and Locality Control}},
  booktitle = 	 {SFP99 --- Scottish Functional Programming Workshop, University of Stirling, Aug 29 -- Sep 1},
  year =	 1999,
  annote =	 {Draft Proceedings}
}


@InProceedings{Trin99,
  author = 	 {Trinder, P.W.},
  title = 	 {{The Design of {\sc GdH}}},
  booktitle = 	 {SFP99 --- Scottish Functional Programming Workshop, University of Stirling, Aug 29 -- Sep 1},
  year =	 1999,
  annote =	 {Draft Proceedings}
}


@InProceedings{PPT*01,
  author = 	 {Pointon, R.F. and Priebe, S.M. and Loidl, H-W. and Loogen, R. and Trinder, P.W.},
  title = 	 {{Functional vs Object-Oriented Distributed Languages}},
  booktitle =	 {{EUROCAST01 --- Computer Aided Systems Theory}},
  pages =	 {642--656},
  year =	 2001,
  volume =	 2178,
  series =	 LNCS,
  address =	 {Las Palmas de Gran Canaria, Spain},
  month =	 feb,
  publisher =	 S-V
}

@InProceedings{sfp00-ft,
  author = 	 {Trinder, P.W. and Pointon, R.F. and Loidl, H-W.},
  title = 	 {{Towards Runtime System Level Fault Tolerance for a Distributed Functional Language}},
  booktitle = 	 {{SFP'00 --- Scottish Functional Programming Workshop}},
  series = 	 {{Trends in Functional Programming}},
  volume =       2,
  pages =        {103--113},
  year =	 2000,
  address =	 {University of St Andrews, Scotland},
  month =	 jul,
  publisher =	 {Intellect},
  abstract =     "Functional languages potentially offer benefits for distributed fault
tolerance: many computations are pure, and hence have no side-effects
to be reversed during error recovery; moreover functional languages
have a high-level runtime system (RTS) where computations and data are
readily manipulated. We propose a new RTS level of fault tolerance for
distributed functional languages, and outline a design for its
implementation for the GdH language. The design distinguishes
between pure and impure computations: impure computations must be
recovered using conventional exception-based techniques, but the RTS
attempts implicit recovery of pure computations."
}

@InProceedings{GdH,
author ={Pointon, R.F. and Trinder, P.W. and Loidl, H-W.},
title = {{The Design and Implementation of Glasgow distributed Haskell}}, 
booktitle= {{IFL'00 --- International Workshop on the Implementation of Functional Languages}},
address={Aachen, Germany}, 
month = sep,
year= {2000},
annote = {std Gdh ref},
pages = {101--116},
series = {LNCS},
volume = 2011,
} 


% #############################################################################
%$%node References To Check, Index, Our Own Stuff, Top
%$%section References To Check
% #############################################################################

%$%menu
%* Older stuff::		
%* New stuff::			
%$%end menu

%$%node Older stuff, New stuff, References To Check, References To Check
%$%subsection Older stuff

% Is there a better std. reference for Conc Clean?
@InProceedings{PENS91,
  author = 	 {Plasmeijr, M.J. and {van Eekelen}, M.C.J.D. and NÅˆcker,
                  E. and Smesters, J.E.W.},
  title = 	 {{The Concurrent Clean System --- Functional Programming
                   on the MacIntosh}},
  booktitle =    {{International Conference of the Apple European University
                   Consortium}}, 
  year =	 {1991},
  pages =        {14--24},
  address =	 {Paris},
  annote =       {A short description of the Clean implementation on the Mac.},
  url =          {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/plar91-CleanMac.ps.gz},
  abstractURL =  {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/plar91-CleanMac.abs},
  abstract =     "Functional programming languages are general purpose, high-level languages based
on the mathematical notion of functions. A functional program consists of a set
of (possibly recursive) function definitions. The execution of a program consists
of the evaluation of an indicated function application. Programs written in a
functional language are generally very compact and also very elegant. This is
mainly due to the availability of pattern matching, guards and higher-order
functions. Modern functional languages use lazy evaluation which means that
expressions are only evaluated when their values are actually needed in a
calculation. This makes it possible to define infinite data structures. A
programmer never has to worry about memory management or pointers. There is no
assignment statement. As a consequence, there are no side-effects possible such
that one can reason about functional programs using traditional mathematical
proof techniques like induction and symbolic substitution. The expressive power
is the same as with ordinary languages such as C. Another property of a
functional program is that the order in which functions are evaluated cannot
change the outcome of a computation. This makes functional programs also very
suited for parallel execution. For all these reasons an increasing number of
universities use functional languages in introductory programming courses.
Functional languages are also very suited for rapid prototyping. A disadvantage
of functional languages was that programs ran very, very slow and that they
consumed a large amount of memory. At several universities much attention has
been paid to improve the compilation techniques which has led to good compilers
for several languages (Hope (Burstall et al. 1980), Lml (Johnson (1984), Haskell
(Hudak et al. (1990)). However, good compilers for personal computers such as the
Macintosh were not available until now."
}

%   o NÅˆcker, E.G.J.M.H., J.E.W. Smetsers, M.C.J.D. van Eekelen and
%     M.J. Plasmeijer (1991), Concurrent Clean, In Proc. of Parallel
%     Architectures and Languages Europe (PARLE '91) , Eindhoven, the
%     Netherlands, Aarts, Leeuwen and Rem Eds., Springer-Verlag, LNCS 505,
%     pp. 202-219.

%     Gives an overview of the language Concurrent Clean (version 0.7) as well
%     as of its implementation. (abstract)


@InProceedings{NSE*91,
  author = 	 {NÅˆcker, E.G.J.M.H. and Smetsers,  J.E.W. and {van
                  Eekelen}, M.C.J.D.  and  Plasmeijer, M.J.},
  title = 	 {{Concurrent Clean}},
  booktitle = 	 {{PARLE'91 --- Parallel Architectures and Languages Europe}},
  number =	 505,
  series =	 LNCS,
  year =	 1991,
  publisher =	 S-V,
  address =	 {Veldhoven, The Netherlands},
  month =	 {June},
  pages =	 {202--219},
  annote =	 {Gives an overview of the language Concurrent Clean (version 0.7) as well
     as of its implementation.}, 
  url =          {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/noce91-concurrentclean.ps.gz},
  abstractURL =  {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/noce91-concurrentclean.abs},
  abstract =     "Concurrent Clean is an experimental, lazy, higher-order parallel functional
programming language based on term graph rewriting. An important difference
with other languages is that in Clean graphs are manipulated and not terms.
This can be used by the programmer to control communication and sharing of
computation. Cyclic structures can be defined. Concurrent Clean furthermore
allows to control the (parallel) order of evaluation to make efficient
evaluation possible. With help of sequential annotations the default lazy
evaluation can be locally changed into eager evaluation. The language
enables the definition of partially strict data structures which make a
whole new class of algorithms feasible in a functional language. A powerful
and fast strictness analyser is incorporated in the system. The quality of
the code generated by the Clean compiler has been greatly improved such that
it is one of the best code generators for a lazy functional language. Two
very powerful parallel annotations enable the programmer to define
concurrent functional programs with arbitrary process topologies. Concurrent
Clean is set up in such a way that the efficiency achieved for the
sequential case can largely be maintained for a parallel implementation on
loosely coupled parallel machine architectures."
}

%   o Eekelen, M.C.J.D. van, M.J. Plasmeijer and J.E.W. Smetsers (1991),
%     Parallel Graph Rewriting on Loosely Coupled Machine Architectures, In
%     Proc. of Conditional and Typed Rewriting Systems (CTRS '90) , Montreal
%     CA, Kaplan and Okada Eds., Springer-Verlag, LNCS 516, pp. 354-369.

%     Explains parallel Graph Rewriting and process communication via Lazy
%     Copying. (abstract)


@InProceedings{EPS91,
  author = 	 {{van Eekelen}, M.C.J.D. and Plasmeijer,  M.J. and
                  Smetsers, J.E.W.},
  title = 	 {{Parallel Graph Rewriting on Loosely Coupled Machine
                  Architectures}},
  booktitle = 	 {CTRS'90 --- Conditional and Typed Rewriting Systems},
  volume =	 516,
  series =	 LNCS,
  year =	 1991,
  publisher =	 S-V,
  address =	 {Montreal, Canada},
  pages =	 {354--369},
  annote =	 {Explains parallel Graph Rewriting and process
                  communication via Lazy Copying. },
  url =          {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/eekm91-parallel.ps.gz},
  abstractURL =  {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/eekm91-parallel.abs},
  abstract =     "Graph rewriting models are very suited to serve as the basic computational
model for functional languages and their implementation. Graphs are used to
share computations which is needed to make efficient implementations of
functional languages on sequential hardware possible. When graphs are
rewritten (reduced) on parallel loosely coupled machine architectures,
subgraphs have to be copied from one processor to another such that sharing
is lost. In this paper we introduce the notion of lazy copying. With lazy
copying it is possible to duplicate a graph without duplicating work. Lazy
copying can be combined with simple annotations which control the order of
reduction. In principle, only interleaved execution of the individual
reduction steps is possible. However, a condition is deduced under which
parallel execution is allowed. When only certain combinations of lazy
copying and annotations are used it is guarantied that this so-called
non-interference condition is fulfilled. Abbreviations for these
combinations are introduced. Now complex process behaviours, such as process
communication on a loosely coupled parallel machine architecture, can be
modelled. This also includes a special case: modelling multiprocessing on a
single processor. Arbitrary process topologies can be created. Synchronous
and asynchronous process communication can be modelled. The implementation
of the language Concurrent Clean, which is based on the proposed graph
rewriting model, has shown that complicated parallel algorithms which can go
far beyond divide-and-conquer like applications can be expressed."
}

%   o Eekelen, M.C.J.D van and M.J. Plasmeijer (1993), Process Annotations and
%     Process Types, In Term Graph Rewriting - Theory and Practice , Sleep,
%     Plasmeijer and Eekelen Eds., John Wiley & Sons, pp. 347-362.

%     The concurrent behaviour of a program is defined by means of annotations
%     with an associated type system. (abstract)


@InCollection{EePl93,
  author = 	 {{van Eekelen}, M.C.J.D and Plasmeijer, M.J.},
  title = 	 {{Process Annotations and Process Types}},
  booktitle = 	 {{Term Graph Rewriting --- Theory and Practice}},
  publisher =	 JWS,
  year =	 1993,
  editor =	 {Sleep, M.R. and {van Eekelen}, M.C.J.D and Plasmeijer, M.J.},
  pages =	 {347--362},
  annote =	 {The concurrent behaviour of a program is defined by means
                  of annotations with an associated type system.},
  url =          {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/eekm93-Processes\&Types.ps.gz},
  abstractURL =  {ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/eekm93-Processes\&Types.abs},
  abstract =     "In a concurrent functional language processes are functions that are executed
concurrently. Using special annotations based on lazy copying arbitrary
dependencies between these functions can be used to specify arbitrary networks
of processes. The communication and synchronization between the processes is
realized using the lazy evaluation principle without any additional
communication primitives. Communication takes place when a process demands a
value that is being calculated by another process. A type system is proposed that
enables the programmer to specify higher order functions as process skeletons for
frequently occurring process structures."
}

@TechReport{ENPS89,
  author = 	 {{van Eekelen}, M.C.J.D. and NÅˆcker, E. and Plasmeijr,
                  M.J. and Smesters, J.E.W.}, 
  title = 	 {{Concurrent Clean}},
  institution =  {Department of Informatics},
  year = 	 {1989},
  number =	 {89-18},
  address =	 {University of Nijmegen},
  month =	 {October}
}

@TechReport{Schr89,
  author = 	 {{Wichers Schreur}, R.J.M.}, 
  title = 	 {{The Concurrent Clean System --- Users's Manual}},
  institution =  {Department of Informatics},
  year = 	 {1989},
  number =	 {89-21},
  address =	 {University of Nijmegen},
  month =	 {November}
}

@InProceedings{Acht91,
         author =       {Achten, P.},
         title =        {Annotations for Load Distribution},
         booktitle =    {{IFL'91 --- International Workshop on the Parallel
                        Implementation of Functional Languages}},
         address =      {Southampton, UK, June 5--7},
         year =         "1991",
         editor =       "Glaser, H. and Hartel, P.",
         publisher =    "Technical Report CSTR 91-07, University of
                        Southampton",
         OPTpages =        "",
}


@InProceedings{NPS91,
  author =       {NÅˆcker, E. and  Plasmeijer, R. and Smetsers, S.},
  title =        {{The Parallel ABC Machine}},
  booktitle =    {{IFL'91 --- International Workshop on the Parallel
                 Implementation of Functional Languages}},
  address =      {Southampton, UK, June 5--7},
  year =         {1991},
  OPTeditor =       {Glaser, H. and Hartel, P.},
  publisher =    {Technical Report CSTR 91-07, University of
                 Southampton},
  pages =        {351--381},
  abstract =     "In this paper we will introduce the parallel ABC
                 machine. The parallel ABC machine is an abstract
                 parallel machine used for creating and describing
                 implementations of Concurrent Clean, a parallel
                 functional language based on the principles of term
                 graph rewriting. Basically, the parallel ABC machine
                 represents a loosely coupled processor architecture,
                 but abstracts from specific implementation issues. We
                 will show how such a machine can be defined, and how
                 code for it can be generated. As for the sequential ABC
                 machine, the description will be given in a functional
                 framework.",
  keywords =     "Term Graph Rewriting, Abstract Machine",
}

@InProceedings{Gron92,
  author =       {{van Groningen}, J.H.G.},
  title =        {{Some Implementation Aspects of Concurrent Clean on
                 Distributed Memory Architectures}},
  booktitle =    {{IFL'92 ---  International Workshop on the Parallel
                 Implementation of Functional Languages}},
  address =      {{Aachen, Germany, September 28--30}},
  year =         {1992},
  opteditor =       "Herbert Kuchen and Rita Loogen",
  abstract =     "We have implemented a code generator and a runtime
                 system that can be used to simulate parallel execution
                 of Concurrent Clean programs on a single Macintosh
                 computer. This code generator generates machine code
                 and is an extension of the (sequential) ABC code
                 generator for the MC680x0 and SPARC processors. This
                 implementation is discussed briefly. The largest part
                 of the paper describes two aspects of the
                 implementation in detail. The first one is how to let a
                 process wait until a node is overwritten with its head
                 normal form. Several possible solutions are described.
                 The other one is a description of an efficient graph
                 copying algorithm. This graph copying algorithm can be
                 used to send graphs to remote processors using a more
                 compact representation. Finally, some results are
                 presented.",
  keywords =     "Parallel Graph Reduction",
}


%$%node New stuff,  , Older stuff, References To Check
%$%subsection New stuff

@InProceedings{Dietz88,
  author =       "Henry G. Dietz",
  title =        "Finding Large-Grain Parallelism in Loops with Serial
                 Control Dependencies",
  year =         "1988",
  month =        aug,
  volume =       "II, Software",
  booktitle =    "Proceedings of the 1988 International Conference on
                 Parallel Processing",
  publisher =    "Penn State",
  pages =        "114--121",
  address =      "University Park, Penn",
  keywords =     "Compilers, control precomputation, transformation,
                 closure loop, synchronization, iteration,
                 restructuring",
  annote =       "Paper should have summarized comparison to DOALL and
                 FORALL loops. Needs to speak more about loop exception
                 handling or branching. Proposes a way to {"}straighten
                 out{"} loops.",
}

@InProceedings{GoGa88,
  author = 	 {Goldman, R. and Gabriel, R.P.},
  title = 	 {{Qlisp: Experience and New Directions}},
  pages =	 {111--123},
  booktitle =    {{SIGPLAN PPEALS 1988}},
  year =	 {1988},
  organization = {ACM},
  address =	 {New Haven, CT},
  annote =	 {annotations to control when to create parallel tasks}
}

@Article{PLW96,
  author = 	 {Palis, M.A. and Liou, J.C. and Wei, D.S.L.},
  title = 	 {{Task Clustering and Scheduling for Distributed-Memory 
                   Parallel Architectures}},
  journal = 	 {IEEE Transactions on Parallel and Distributed Systems},
  year = 	 1996,
  volume =	 7,
  number =	 1,
  pages =	 {46--55},
  annote =	 {increasing granularity; language independent (works on DAGs)},
  abstract =     "This paper addresses the problem of scheduling parallel programs 
    represented as directed acyclic task graphs for execution on 
    distributed memory parallel architectures. Because of the high 
    communication overhead in existing parallel machines, a crucial step 
    in scheduling is task clustering, the process of coalescing fine 
    grain tasks into single coarser ones so that the overall execution 
    time is minimized. The task clustering problem is NP-hard, even when 
    the number of processors is unbounded and task duplication is 
    allowed. A simple greedy algorithm is presented for this problem 
    which, for a task graph with arbitrary granularity, produces a 
    schedule whose makespan is at most twice optimal. Indeed, the quality
    of the schedule improves as the granularity of the task graph becomes
    larger. For example, if the granularity is at least 1/2, the makespan
    of the schedule is at most 5/3 times optimal. For a task graph with n
    tasks and e inter-task communication constraints, the algorithm runs 
    in O(n(n lg n + e)) time, which is n times faster than the currently 
    best known algorithm for this problem. Similar algorithms are 
    developed that produce: (1) optimal schedules for coarse grain 
    graphs; (2) 2-optimal schedules for trees with no task duplication; 
    and (3) optimal schedules for coarse grain trees with no task 
    duplication.",
}

@Article{RiDi96,
  author = 	 {Rinard, M.C. and Diniz, P.C.},
  title = 	 {{Commutativity Analysis: A New Analysis Framework for
                   Parallelizing Compilers}}, 
  journal = 	 {{ACM Sigplan Notices}},
  year = 	 1996,
  volume =	 31,
  number =	 5,
  pages =	 {54--67},
  annote =	 {detects independent computations i.e. only interesting
                  for imperative prgs},
  keywords =     {parallelizing compilers, parallel computation, commutativity
analysis},
  url =          {http://www.cs.ucsb.edu/TRs/techreports/TRCS96-08.ps},
  abstractURL =  {http://www.cs.ucsb.edu/TRs/TRCS96-08.html},
  abstract =     "
    This paper presents a new analysis technique, commutativity analysis,
    for automatically parallelizing computations that manipulate dynamic,
    pointer-based data structures. Commutativity analysis views the 
    computation as composed of operations on objects. It then analyzes 
    the program at this granularity to discover when operations commute 
    (i.e. generate the same final result regardless of the order in which
    they execute). If all of the operations required to perform a given 
    computation commute, the compiler can automatically generate parallel
    code.
    We have implemented a prototype compilation system that uses 
    commutativity analysis as its primary analysis framework. We have 
    used this system to automatically parallelize two complete scientific
    computations: the Barnes-Hut N-body solver and the Water code. This 
    paper presents performance results for the generated parallel code 
    running on the Stanford DASH machine. These results provide 
    encouraging evidence that commutativity analysis can serve as the 
    basis for a successful parallelizing compiler."
}

@PhdThesis{Horn84,
  author =       {Hornig, D.A.},
  title =        {{Automatic Partitioning and Scheduling on a Network of
                 Personal Computers}},
  school =       {Department of Computer Science},
  address =      {Carnegie Mellon University},
  year =         {1984},
  month =        {November},
  abstract =     "This Ph.D thesis describes the development of a
                 language Stardust in which indications are given of the
                 running time of each function. The run-time evironment
                 then schedules the functions based on the costs of
                 message passing and load balancing. There is some
                 discussion of granularity. The language contains no
                 explicit partitioning.",
}

@InProceedings{McCo95,
  author = 	 {McColl, W.F.},
  title = 	 {{Scalable Computing}},
  booktitle = 	 {{Computer Science Today: Recent Trends and Developments}},
  volume =	 100,
  series =	 LNCS,
  year =	 1995,
  publisher =	 {Springer-Verlag},
  pages =	 {46--61},
  annote =	 {std reference for BSP?}
}


@Article{McCo96,
  author = 	 {McColl, W.F.},
  title = 	 {{Scalability, Portability and Predictability: The BSP Approach to Parallel Programming}},
  journal = 	 {Future Generation Computer Systems},
  year = 	 1996,
  volume =	 12,
  number =	 4,
  pages =	 {265--272}
}

@Article{Vali90,
  author = 	 {Valiant, L.G.},
  title = 	 {{A Bridging Model for Parallel Computation}},
  journal = 	 CACM,
  year = 	 1990,
  volume =	 33,
  number =	 8,
  month =	 {August},
  annote =	 {seems to be first reference to BSP}
}

@Book{RoHa03,
  author =	 {Peter van Roy, Seif Haridi},
  title = 	 {{Concepts, Techniques, and Models of Computer Programming}},
  publisher = 	 {MIT Press},
  year = 	 2003
}

@InProceedings{SMT*95,
  OPTauthor = 	 {Smirni, E. and Merlo, A. and Tessera, D. and Haring, G. and Kotsis, G.},
  author = 	 {Smirni, E. and Rosti, E.},
  title = 	 {{Modeling Speedup of SPMD Applications on the Intel Paragon: a Case Study}},
  booktitle = 	 {{HPCN'95 --- High Performance Computing and Networking}},
  OPTeditor =	 {Hertzberger, B. and Serazzi, G.},
  volume =	 919,
  series =	 LNCS,
  year =	 1995,
  publisher =	 S-V,
  address =	 {Milan, Italy},
  month =	 {May},
  pages =	 {94--101},
  annote =	 {std reference to SPMD (from strategies paper)}
}

@Article{BuSi00,
  author = 	 {Burton, F.W. and Simpson, D.J.},
  title = 	 {{Memory requirements for parallel programs}},
  journal = 	 {Parallel Computing},
  year = 	 2000,
  volume =	 26,
  number =	 {13--14},
  pages =	 {1739--1763},
  month =	 dec,
  keywords =     {Memory management; Scheduling; Parallel programming; Memory bounds; Shared memory; Functional programming},
  abstract =     "Parallel execution is normally used to decrease the amount of time required
to  run a program.  However, the  parallel execution  may require  far more
space  than that  required  by  the sequential  execution.  Worse yet,  the
parallel space requirement may be  very much more difficult to predict than
the  sequential  space  requirement  because  there  are  more  factors  to
consider.  These  include  essentially  nondeterministic factors  that  can
influence  scheduling,  which  in  turn may  dramatically  influence  space
requirements. We  survey some scheduling  algorithms that attempt  to place
bounds on the  amount of time and space used  during parallel execution. We
also outline a direction for  future research. This direction takes us into
the area  of functional  programming, where the  declarative nature  of the
languages can help  the programmer to produce correct  parallel programs, a
feat  that  can  be  difficult  with procedural  languages.  Currently  the
high-level nature  of functional  languages can make  it difficult  for the
programmer to understand  the operational behavior of the  program. We look
at  some of  the  problems  in this  area,  with the  goal  of achieving  a
programming environment that supports correct, efficient parallel programs."
}

%$%cindex GAMMA
G. Chiola and G. Ciaccio, GAMMA: a Low-cost Network of Workstations Based on Active Messages, 
      in proceedings of PDP'97 (5th EUROMICRO workshop on Parallel and Distributed Processing), London, UK,
      January 1997.


@InProceedings{ChCi97,
  author = 	 {Chiola, G. and Ciaccio, G.},
  title = 	 {{GAMMA: a Low-cost Network of Workstations Based on Active Messages}},
  booktitle = 	 {{PDP'97 --- EUROMICRO Workshop on Parallel and Distributed Processing}},
  year =	 1997,
  address =	 {London, U.K.},
  month =	 {January},
  url =          {ftp://ftp.disi.unige.it/pub/project/GAMMA/pdp97.ps.gz},
  abstract =     "Networks  Of Workstations  (NOW) are  an emerging  architecture  capable of
supporting  parallel  processing  with significantly  low  cost/performance
ratio.   At  the  moment   the  implementation   of  standard   high  level
communication  mechanisms in  a NOW  does not  provide such  a satisfactory
cost/performance ratio,  as modern  communication hardware would  allow. We
show how a standard, Unix-like operating system kernel can be extended with
efficient and  performant low-level  communication primitives based  on the
Active Message communication  paradigm. Higher level standard communication
libraries,  like  MPI, should  be  implemented  on  top of  such  efficient
low-level mechanisms. We provide  some preliminary results obtained from an
experimental prototype called GAMMA (Genoa Active Message MAchine) which is
a  NOW whose  nodes run  the operating  system Linux  2.0 enhanced  with an
Active Message communication layer."
}

%$%cindex Orca

@article{BLBB98,
  Author    = {Bal, H. and Langendoen, K. and Bhoedjang, R. and Breg, F.},
  Address   = {Vrije Univ Amsterdam, Dept Math \& Comp Sci, Amsterdam,
              Netherlands},
  Title     = {{Experience with Parallel Symbolic Applications in Orca}},
  Journal   = {Journal Programming Languages},
  Year      = {1998},
  Volume    = {6},
  Pages     = {37-61},
  Abstract  = "Orca is a simple, portable, imperative parallel programming
              language, based on a form of distributed shared memory called
              shared data-objects. This paper discusses the suitability of
              Orca for parallel symbolic programming. Orca was not designed
              specifically for this application area, and it lacks several
              features supported by specific languages for symbolic
              parallel computing, such as futures, automatic load balancing
              and automatic garbage collection. On the other hand, Orca
              does give high-level support for sharing global state. Also,
              its implementation automatically distributes shared data
              (stored in shared objects). In addition, Orca programs are
              portable, because the language abstracts from the underlying
              hardware and operating systems. Efficient Orca
              implementations exist on a variety of parallel systems. We
              first give a comparison between Orca and two other models:
              imperative message-passing systems and functional languages.
              We do so by looking at several key issues in parallel
              programming and by studying how each of the three paradigms
              deals with these issues. Next, we describe our experiences
              with writing parallel symbolic applications in Orca. We study
              the performance of each application on two platforms: the SP-2 
              and a collection of SPARC processors connected by an
              ethernet. This work indicates that Orca is quite suitable for
              writing efficient and portable programs for symbolic
              applications."
}

@Article{BBJ98,
  author = 	 {{Ben Hassen}, S. and Bal, H.E. and Jacobs, C.},
  title = 	 {{A Task and Data Parallel Programming Language based on Shared Objects}},
  journal = 	 TOPLAS,
  year = 	 {1998},
  pages =        {1131--1170},
  volume = 	 {20},
  number = 	 {6},
  Tmonth = 	 nov,
  annote = 	 {Discusses both language and implementation; gives some performance measurements},
  keywords =     {D.1.3 [Programming T echniques ]: Concurrent Programming --- Distributed programming; Parallel programming; D.3.2 [Programming Languages]: Language Classifications --- Concurrent, distributed, and parallel languages; D.3.4 [Programming Languages]: Processors --- Compilers; Run-time environments. Data parallelism, task parallelism, shared objects},
  url =          {ftp://ftp.cs.vu.nl/pub/amoeba/orca_papers/toplas98.ps.Z},
  abstract =     "Many  programming  languages  support   either  task  parallelism  or  data
parallelism  but few  languages  provide a  uniform  framework for  writing
applications that need both types  of parallelism. We present a programming
language and system that integrates  task and data parallelism using shared
objects.  Shared  objects  may  be  stored  on  one  processor  or  may  be
replicated.  Objects may  also be  partitioned and  distributed  on several
processors. Task parallelism is  achieved by forking processes remotely and
have them communicate and  synchronize through objects. Data parallelism is
achieved by executing operations on partitioned objects in parallel.

Writing task and data parallel applications with shared objects has several
advantages. Programmers use the objects as  if they were stored in a memory
common to  all processors. On  distributed memory machines, if  objects are
remote, replicated, or partitioned, the system takes care of many low-level
details such as data transfers and consistency semantics.

In this  paper, we show how to  write task and data  parallel programs with
our shared ob ject model. We also describe a portable implementation of the
model.  To  assess  the  performance   of  the  system,  we  wrote  several
applications  that use task  and data  parallelism and  executed them  on a
collection of Pentium  Pros connected by Myrinet. The  performance of these
applications is also discussed in this paper."
}


%$%cindex Jade

@Article{RiLa98,
  author = 	 {Rinard, M.C. and Lam, M.S.},
  title = 	 {{The Design, Implementation, and Evaluation of Jade}},
  journal = 	 TOPLAS,
  year = 	 1998,
  volume =	 20,
  number =	 3,
  pages = 	 {483--545},
  month = 	 {May},
  keywords =     {D.1.3 [Programming Techniques]: Concurrent Programming --- parallel programming; D.3.2 [Programming Languages]: Language Constructs and Features --- concurrent programming structures. Parallel computing, parallel programming languages},
  url =          {http://www.cag.lcs.mit.edu/~rinard/paper/toplas98.ps},
  abstract =     "Jade is  a portable, implicitly  parallel language designed  for exploiting
task-level concurrency. Jade programmers start  with a program written in a
standard serial,  imperative language, then use Jade  constructs to declare
how parts  of the  program access data.  The Jade implementation  uses this
data access  information to automatically  extract the concurrency  and map
the application onto the machine  at hand. The resulting parallel execution
preserves the semantics of the original serial program. We have implemented
Jade as an extension to C, and Jade implementations exist for shared-memory
multiprocessors,  homogeneous message-passing  machines,  and heterogeneous
networks of workstations.  In this article we discuss  the design goals and
decisions that determined the final form of Jade and present an overview of
the  Jade implementation.  We also  present  our experience  using Jade  to
implement several complete scientific  and engineering applications. We use
this experience to  evaluate how the different Jade  language features were
used  in practice and  how well  Jade as  a whole  supports the  process of
developing parallel applications. We find that the basic idea of preserving
the serial  semantics simplifies the program development  process, and that
the   concept  of   using   data  access   specifications   to  guide   the
parallelization  offers   significant  advantages  over   more  traditional
control-based  approaches.  We also  find  that  the  Jade data  model  can
interact poorly with  concurrency patterns that write disjoint  pieces of a
single aggregate data  structure, although this problem arises  in only one
of the applications."
}


@PhdThesis{Rina94,
  author = 	 {},
  title = 	 {},
  school = 	 {},
  year = 	 {},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

% these are some post-PhD references; ToCheck!


@Article{CFG01,
  author = 	 {Paolo Ciancarini and Daniela Fogli and Mauro Gaspari},
  title = 	 {A declarative coordination language},
  journal = 	 {Computer Languages},
  year = 	 2001,
  volume =	 {2--4},
  pages =	 {125-163},
  publisher =    {Elsevier Science Ltd},
  annote =	 {Uses Gamma model for coordination in a logic lang},
  abstract =     "We  describe Gammalog, a  logic language  whose semantics  is based  on the
chemical  metaphor.  The  language   combines  the  ability  of  describing
coordination by  transformation rules on  a shared-dataspace, as  in Gamma,
with the execution model of logic programming. The main feature of Gammalog
is that the  declarative reading of programs is not  restricted to the pure
logic  language but  it  also includes  the  coordination mechanisms.  This
feature makes Gammalog a promising alternative to other coordination models
which can be embedded in logic programming. We present the language syntax,
its formal semantics, and a  prototype implementation based on Gˆdel, which
inherits  its strongly  typed framework.  As an  example of  the expressive
power of  the language we provide  the specification of  a simple operating
system"
}

@article{CGKL98,
  Author    = {Chakravarty, M.M.T. and Guo, Y. and Kohler, M. and Lock,
              H.C.R.},
  Address   = {TU Berlin, Fachbereich Informatik, D-10587 Berlin, Germany
              Univ London Imperial Coll Sci Technol \& Med, Dept Comp,
              London Sw7 2Bz, England Th Karlsruhe, Fak Informat, D-76128
              Karlsruhe, Germany},
  Title     = {{Goffin: Higher-Order Functions Meet Concurrent Constraints}},
  Journal   = {Science Computer Programming},
  Year      = {1998},
  Volume    = {30},
  Pages     = {157-199},
  Abstract  = {We introduce a higher-order constraint-based language for
              structured and declarative parallel programming. The
              language, called GOFFIN, systematically integrates
              constraints and user-defined functions within a uniform
              setting of concurrent programming. From the perspective of
              parallel programming methodology, the constraint parr of
              GOFFIN provides a co-ordination language for the functional
              part, which takes on the role of the computation language.
              This conceptual distinction allows the structured formulation
              of parallel algorithms. GOFFIN is an extension of the purely
              functional language Haskell. The functional kernel is
              embedded in a layer based on concurrent constraints, Logical
              variables are bound by constraints which impose relations
              over expressions that may contain user-defined functions.
              Referential transparency is preserved by restricting the
              creation of logical variables to the constraint part and by
              suspending the reduction of functional expressions that
              depend on the value of an unbound logical variable. Hence,
              constraints are the means to organize the concurrent
              reduction of functional expressions. Moreover, constraint
              abstractions, i.e., functions over constraints, allow the
              definition of parameterized co-ordination forms. In
              correspondence with the higher-order nature of the functional
              part, abstractions in the constraint logic part are based on
              higher-order logic, leading to concise and modular
              specifications of behaviour. We introduce and explain GOFFIN
              together with its underlying programming methodology, and
              present a declarative as well as an operational semantics for
              the language. To formalize the semantics, we identify the
              essential core constructs of the language and characterize
              their declarative meaning by associating them with formulae
              of Church's simple theory of types. We also present a
              reduction system that captures the concurrent operational
              semantics of the core constructs. In the course of this
              paper, the soundness of this reduction system with respect to
              the declarative semantics is established.}
}

@article{Lins97,
  Author    = {Lins, R.D.},
  Address   = {Univ Fed Pernambuco, Dept Informat, Recife, Pe, Brazil},
  Title     = {Functional Programming and Parallel Processing (Invited
              Paper)},
  Journal   = {Lecture Notes Computer Science},
  Year      = {1997},
  Volume    = {1215},
  Pages     = {429-457},
  Abstract  = {Functional languages belong to a neat and very high-level
              programming paradigm. A functional program is a set of
              function definitions. The lambda-Calculus, a theory of
              functions under recursion, offers a solid theoretic
              background to functional programming. In 1978, John Backus
              pointed at the functional programming as a natural candidate
              to solve ''the software crisis''. One of the many promises of
              functional programming was the possibility of extracting
              parallelism. This paper analyses the evolution of the
              functional programming paradigm under this outlook.}
}

%$%cindex Signal Processing

@PhdThesis{Reek95,
  author = 	 {Reekie, H.J.},
  title = 	 {{Realtime Signal Processing: Dataflow, Visual and Functional Programming}},
  school = 	 {School of Electrical Engineering},
  year = 	 1995,
  address =	 {University of Technology at Sydney}
}

@InProceedings{Denn95,
  author = 	 {Dennis, J.},
  title = 	 {{Static Mapping of Functional Programs: an Example in Signal Processing}},
  booktitle =	 {HPFC'95 --- High Performance Functional Computing},
  address =	 {Denver, Colorado, April 10--12},
  year =         1995,
  pages =	 {149--163}
}

@PhdThesis{Hill94,
  author = 	 {Hill, J.M.D.},
  title = 	 {{Data-Parallel Lazy Functional Programming}},
  school = 	 {Department of Computer Science},
  year = 	 1994,
  address =	 {Queen Mary and Westfield College, University of London}
}

@Book{PoMa87,
  author =	 {Pountain, D. and May, D.},
  title = 	 {{A Tutorial Introduction to Occam Programming}},
  publisher = 	 {McGraw-Hill, New York},
  year = 	 1987
}

@InProceedings{Beowulf,
  author = 	 {Ridge, D. and Becker, D. and Merkey, P. and Sterling, T.},
  title = 	 {{Beowulf: Harnessing the Power of Parallelism in a Pile-of-pcs}},
  booktitle =	 {{IEEE Aerospace Conference}},
  pages =	 {79--91},
  year =	 1997,
  volume =	 2,
  publisher =	 {IEEE},
  annote =	 {std Beowulf reference},
  documentURL = {http://www.beowulf.org/papers/AA97/aa97.ps},
  abstract = "
The rapid increase in  performance of mass market commodity microprocessors
and   significant  disparity   in  pricing   between  PCs   and  scientific
workstations  has   provided  an  opportunity  for   substantial  gains  in
performance to  cost by harnessing  PC technology in parallel  ensembles to
provide    high   end   capability    for   scientific    and   engineering
applications. The  Beowulf project  is a NASA  initiative sponsored  by the
HPCC program  to explore  the potential of  Pile-of-PCs and to  develop the
necessary methodologies  to apply these  low cost system  configurations to
NASA computational requirements in  the Earth and space sciences. Recently,
a 16 processor  Beowulf costing less than \$50,000  sustained 1.25 Gigaflops
on a  gravitational N-body simulation of  10 million particles  with a Tree
code   algorithm   using   standard   commodity   hardware   and   software
components.  This  paper   describes  the  technologies  and  methodologies
employed to achieve this  breakthrough. Both opportunities afforded by this
approach  and  the challenges  confronting  its  application to  real-world
problems are discussed in the framework of hardware and software systems as
well  as the  results  from benchmarking  experiments.  Finally, near  term
technology  trends and  future directions  of the  Pile-of-PCs  concept are
considered."
}

%$%appendix

%$%node Index,  , References To Check, Top
%$%section Index

%$%index
%* Active Messages::  %$cindex\s-+Active Messages
%* Alfalfa::  %$cindex\s-+Alfalfa
%* CAM::  %$cindex\s-+CAM
%* Clean::  %$cindex\s-+Clean
%* Dactl::  %$cindex\s-+Dactl
%* Dataflow::  %$cindex\s-+Dataflow
%* Dutch Parallel Machine::  %$cindex\s-+Dutch Parallel Machine
%* EDEN::  %$cindex\s-+EDEN
%* EQUALS::  %$cindex\s-+EQUALS
%* Flagship::  %$cindex\s-+Flagship
%* G-machine::  %$cindex\s-+G-machine
%* GAML::  %$cindex\s-+GAML
%* GRIP::  %$cindex\s-+GRIP
%* GUM::  %$cindex\s-+GUM
%* Gamma::  %$cindex\s-+Gamma
%* Ginger::  %$cindex\s-+Ginger
%* Graph Reduction::  %$cindex\s-+Graph Reduction
%* HDG::  %$cindex\s-+HDG
%* Id::  %$cindex\s-+Id
%* LAGER::  %$cindex\s-+LAGER
%* Manchester Dataflow Machine::  %$cindex\s-+Manchester Dataflow Machine
%* Monsoon::  %$cindex\s-+Monsoon
%* NESL::  %$cindex\s-+NESL
%* PAM::  %$cindex\s-+PAM
%* PASTEL::  %$cindex\s-+PASTEL
%* SISAL::  %$cindex\s-+SISAL
%* STGM::  %$cindex\s-+STGM
%* Sigma-1::  %$cindex\s-+Sigma-1
%* Skeletons::  %$cindex\s-+Skeletons
%* Spineless G-machine::  %$cindex\s-+Spineless G-machine
%* StarT::  %$cindex\s-+StarT
%* TAM::  %$cindex\s-+TAM
%* TIM::  %$cindex\s-+TIM
%* TTDA::  %$cindex\s-+TTDA
%* Tuki::  %$cindex\s-+Tuki
%* ZAPP::  %$cindex\s-+ZAPP
%* lambda-S::  %$cindex\s-+lambda-S
%* load bounding::  %$cindex\s-+load bounding
%* pH::  %$cindex\s-+pH
%* pHluid::  %$cindex\s-+pHluid
%* pired::  %$cindex\s-+pired
%* throttling::  %$cindex\s-+throttling
%* weighted reference counting::  %$cindex\s-+weighted reference counting
%$%end index


